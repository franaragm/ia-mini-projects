# **ðŸ§  CONTEXTO GLOBAL DEL REPOSITORIO â€” Mini-Proyectos LangChain Lab**

Este repositorio contiene una colecciÃ³n de **mini-proyectos progresivos**, cada uno enfocado en un componente distinto del ecosistema LLM: prompts, output parsing, RAG, embeddings, scrapers, chains, routers y agentes.

El servidor estÃ¡ construido sobre **FastAPI**, con un diseÃ±o modular donde cada mini-proyecto es independiente, pero todo comparte la misma infraestructura base.

---

# ðŸ—ï¸ **ARQUITECTURA GENERAL DEL REPOSITORIO**

```
mini-projects-langchain/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ CONTEXT_REPO.md            â† ESTE ARCHIVO (contexto global)
â”œâ”€â”€ config_base.py             â† Config global compartida
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”‚
â”œâ”€â”€ app/                       â† Servidor FastAPI + utilidades globales
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ routes.py
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ llm_client.py      â† Cliente LLM (OpenRouter)
â”‚       â”œâ”€â”€ utils.py
â”‚       â””â”€â”€ ...
â”‚
â””â”€â”€ projects/                  â† Mini-proyectos aislados
    â”œâ”€â”€ A1_chat_structured/
    â”œâ”€â”€ A2_output_parser/
    â”œâ”€â”€ A3_rag_basic/
    â”œâ”€â”€ A3_rag_basic_v2/
    â”œâ”€â”€ A4_rag_advanced/
    â”œâ”€â”€ A4_rag_advanced_v2/
    â”œâ”€â”€ A5_chains_routers/
    â”œâ”€â”€ A5_memory/
    â””â”€â”€ â€¦
```

**requeriments.txt actual**

```
fastapi
uvicorn[standard]

# NÃºcleo de LangChain (PromptTemplate, OutputParser, etc.)
langchain-core
langchain
langchain-text-splitters
langchain-openai

# Integraciones generales y utilidades (sin modelos OpenAI)
langchain-community

# Cliente OpenAI compatible con OpenRouter
openai

# Para usar modelos de embeddings locales (MiniLM, etc.)
sentence-transformers

# Vector DB local para RAG
chromadb

# Scraping web
requests
beautifulsoup4

# Utilidades comunes
python-dotenv
pydantic
httpx

# ValidaciÃ³n de correos (elimina warning de FastAPI/Pydantic)
email-validator
```

---

# ðŸ”§ **DESCRIPCIÃ“N DE LA CAPA BASE DEL SERVIDOR**

## **`app/main.py`**

Configura FastAPI:

* URLs de documentaciÃ³n (`/docs`) sÃ³lo en entorno `dev`
* Metadatos (nombre, versiÃ³n, contacto)
* Incluye el router principal

## **`app/routes.py`**

Define:

* Endpoints globales (`/health`, `/test-llm`)
* Registro automÃ¡tico de todos los mini-proyectos:

  ```python
  router.include_router(a1_router)
  router.include_router(a2_router)
  ...
  ```

## **`app/services/llm_client.py`**

Proporciona **dos clientes diferentes**:

### 1. `llm(prompt: str)`

Cliente simplificado para obtener **solo texto** (sin LangChain).
Ideal para proyectos bÃ¡sicos o utilidades.

### 2. `llm_chain()`

Devuelve `ChatOpenAI` para integrarse en:

* LCEL
* RouterChain
* Sequential chains
* Agents
* Tools con funciones reales

Ambos clientes usan OpenRouter (modelos GPT-OSS, Nemotron, etc.).

---

# âš™ï¸ **CONFIGURACIÃ“N GLOBAL â€” `config_base.py`**

Define:

* Rutas absolutas del repo
* Ruta centralizada de bases vectoriales
* Modelo de embeddings global (`all-MiniLM-L6-v2`)
* LLM por defecto y fallback
* Paths de `projects/` y `app/`

Todos los mini-proyectos importan desde aquÃ­ para evitar duplicaciÃ³n:

```python
from config_base import CHROMA_PATH, DEFAULT_EMBEDDING_MODEL
```

---

# ðŸ§© **PATRÃ“N DE DISEÃ‘O DE CADA MINI-PROYECTO**

Cada mini-proyecto se estructura con la misma arquitectura:

```
A4_rag_advanced_v2/
â”‚
â”œâ”€â”€ router.py               â† Endpoint FastAPI
â”œâ”€â”€ config.py               â† Config especÃ­fica del mini proyecto
â”œâ”€â”€ schemas.py              â† Pydantic (Request/Response)
â”œâ”€â”€ prompts.py              â† PromptTemplates usados
â”œâ”€â”€ rag.py                  â† LÃ³gica principal (RAG, embeddings, compresiÃ³n)
â”œâ”€â”€ chroma_client.py        â† InicializaciÃ³n de colecciÃ³n Chroma
â”œâ”€â”€ loader.py               â† Carga de documentos locales
â”œâ”€â”€ scraper.py              â† Web scraping (opcional)
â”œâ”€â”€ utils.py                â† Hash, formateo de fuentes, etc.
â””â”€â”€ data/                   â† Documentos locales a indexar
```

### **Archivos frecuentes:**

### âœ” `router.py`

Define:

* Rutas del mini-proyecto
* IndexaciÃ³n automÃ¡tica al arrancar
* ConversiÃ³n entre modelos Pydantic y outputs del pipeline

### âœ” `schemas.py`

EstÃ¡ndar comÃºn para todos los mini-proyectos:

```python
class QueryRequest(BaseModel):
    question: str

class SourceDocument(BaseModel):
    source: str
    score: float

class QueryResponse(BaseModel):
    answer: str
    sources: list[SourceDocument]
```

### âœ” `config.py`

Config local del proyecto:

* Nombre de colecciÃ³n
* URLs a scrapear
* ParÃ¡metros de embeddings
* Ruta de almacenamiento

### âœ” `prompts.py`

PromptTemplates para:

* Clasificador de intenciÃ³n
* Prompt general
* Prompt de RAG
* Prompt de resumen
* Prompt de cÃ³digo
* Prompt matemÃ¡tico

### âœ” `chroma_client.py`

Inicializa la colecciÃ³n con:

```python
client = chromadb.PersistentClient(path=CHROMA_PATH)
collection = client.get_or_create_collection(COLLECTION_NAME)
```

### âœ” `loader.py`

Carga documentos desde `/data`, los divide con `RecursiveCharacterTextSplitter`.

### âœ” `scraper.py`

Extrae HTML de URLs, limpia scripts/style, devuelve `Document`.

### âœ” `utils.py`

Funciones auxiliares como:

* `hash_text()`
* `is_chunk_indexed()`
* `format_sources()`

---

# ðŸ“š **DESCRIPCIÃ“N RESUMIDA DE LOS MINI-PROYECTOS**

### **A1 â€“ Chat estructurado**

Primeros prompts, respuestas controladas.

### **A2 â€“ Output Parser**

ValidaciÃ³n estricta con Pydantic / JSON.

### **A3 â€“ RAG BÃ¡sico**

Cargar documentos â†’ fragmentarlos â†’ indexar â†’ recuperar contexto.

### **A3 V2 â€“ RAG BÃ¡sico Mejorado**

Separadores custom + limpieza + mejor chunking.

### **A4 â€“ RAG Avanzado**

Fuentes + puntajes + anti-alucinaciÃ³n rÃ­gida.

### **A4 V2 â€“ RAG con Web Scraping**

* Carga local + scrapeo web
* CompresiÃ³n contextual
* Puntajes normalizados

### **A5 â€“ Chains & Routers**

Arquitectura completa:

```
Pregunta â†’ Clasificador â†’ Router â†’ (General / Summary / Code / Math / RAG)
```

con LCEL (`|` operator) y `RunnableLambda`.

### **A5 â€“ Memory**

Buffers, resumen incremental, memoria contextual.

---

# ðŸ§  **DIRECTRICES PARA CREAR UN NUEVO MINI-PROYECTO**

Cada nuevo mini-proyecto debe seguir este template:

```
projects/A6_nombre/
â”‚
â”œâ”€â”€ router.py
â”œâ”€â”€ config.py
â”œâ”€â”€ prompts.py
â”œâ”€â”€ logic.py o rag.py o agent.py (segÃºn el caso)
â”œâ”€â”€ schemas.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ chroma_client.py (si usa vectores)
â”œâ”€â”€ loader.py / scraper.py (si aplica)
â””â”€â”€ data/
```

### **Reglas de arquitectura:**

1. **JamÃ¡s duplicar llm_client**: usar siempre el cliente global.
2. **Nunca crear su propio chromadb** â†’ usar `CHROMA_PATH`.
3. **Un mini-proyecto = un endpoint FastAPI bien aislado**.
4. **Prompts siempre en `prompts.py`.**
5. **Schemas siempre en `schemas.py`.**
6. **Toda lÃ³gica en un archivo separado (ej: `rag.py`, `agent.py`, `chain.py`).**
7. **Mantener mÃ¡xima modularidad.**
8. **Si usa embeddings â†’ usar modelo global salvo que tengas buena razÃ³n.**

---

# ðŸ”¥ **DIRECTRICES PARA CÃ“MO RESPONDEN LOS LLM EN ESTE REPO**

### âœ” ClasificaciÃ³n de intenciÃ³n (A5)

El LLM debe:

* Responder solo: `general`, `rag`, `summary`, `code`, `math`
* No agregar nada mÃ¡s
* En caso de duda â†’ `general`

### âœ” RAG

Reglas:

* No puede inventar nada
* Solo usa el contexto recuperado
* Si no hay suficiente contexto:

```
"Sin suficiente informaciÃ³n en la documentaciÃ³n para responder."
```

* SÃ­ puede sintetizar (no solo copiar)
* No puede rellenar huecos con conocimiento general

### âœ” Code

Solo cÃ³digo correcto y explicaciones cuando proceda.

### âœ” Summary

Resumen objetivo basado en texto proporcionado por el usuario.

### âœ” General

ConversaciÃ³n natural.

---

# ðŸ§± **ESTÃNDAR DE PATRONES PARA CHAINS (A5)**

### Ejemplo tÃ­pico:

```python
chain = (
    preprocess_text
    | classifier_chain
    | router
    | selected_chain
    | RunnableLambda(lambda x: {"answer": x, "chain_used": "..."} )
)
```

### Reglas:

* Usar LCEL (`|`)
* No usar clases antiguas de LangChain (ej: `LLMChain`, `RouterChain`, `TransformChain`)
* Preferir `RunnableLambda` y `ChatPromptTemplate`

---

# ðŸ§© **RAG Pipeline estÃ¡ndar del repo**

1. Cargar documentos (`loader.py`)
2. Dividir documentos (`split_documents`)
3. Limpiar y hash para evitar duplicados
4. Crear embedding con `SentenceTransformers`
5. Persistir en ChromaDB
6. Recuperar contexto
7. (Opcional) CompresiÃ³n contextual
8. Ejecutar prompt RAG

---

# ðŸ§ª **Pruebas y desarrollo**

* Ejecutar servidor:

```
uvicorn app.main:app --reload --port 8000
```

* Usar `/docs` para probar endpoints en dev.

---

# (ExtensiÃ³n del contexto â€” **app/** completo)**

Estos archivos contenidos en app sirven para reutilizar en los miniproyectos , nop volver hacer estas funcionalidades de app/ en los mini proyectos.

**Fuente del repositorio:** `https://github.com/franaragm/ia-mini-projects`. ([GitHub][1])

---

## ðŸ“ `app/main.py`

**Ruta:** `app/main.py`
**FunciÃ³n:** Punto de entrada del servidor FastAPI. Configura la aplicaciÃ³n, la documentaciÃ³n condicional (dev/prod), los metadatos y registra el router principal.

**CÃ³digo (exacto, sin modificar):**

```python
from fastapi import FastAPI
from .routes import router
from .services.utils import get_env

ENV = get_env("ENV", "dev")  # dev | prod

docs_url = "/docs" if ENV == "dev" else None
redoc_url = "/redoc" if ENV == "dev" else None
openapi_url = "/openapi.json" if ENV == "dev" else None

app = FastAPI(
    title="LangChain Lab - AI Server",
    description="""
    Servidor de experimentaciÃ³n con modelos de IA, RAG y agentes.

    Este backend expone APIs para explorar:
    - RecuperaciÃ³n aumentada con generaciÃ³n (RAG)
    - Llamadas a modelos LLM
    - Herramientas generativas
    - Proyectos modulares de IA

    """,
    version="1.0.0",
    summary="Backend laboratorio para proyectos de IA",
    contact={
        "name": "Francisco AragÃ³n",
        "email": "franaragonmesa@gmail.com",
    },
    license_info={
        "name": "MIT License",
        "url": "https://opensource.org/licenses/MIT",
    },
    docs_url=docs_url,
    redoc_url=redoc_url,
    openapi_url=openapi_url,
)

app.include_router(router)
```

---

## ðŸ“ `app/routes.py`

**Ruta:** `app/routes.py`
**FunciÃ³n:** Router principal que registra endpoints globales (`/health`, `/test-llm`) y **incluye** los routers de cada mini-proyecto (A1..A5...). Ãštil para mantener la separaciÃ³n modular entre proyectos.

**CÃ³digo (exacto, sin modificar):**

```python
from fastapi import APIRouter
from .services.llm_client import llm
from projects.A1_chat_structured.router import router as a1_router
from projects.A2_output_parser.router import router as a2_router
from projects.A3_rag_basic.router import router as a3_router
from projects.A3_rag_basic_v2.router import router as a3v2_router
from projects.A4_rag_advanced.router import router as a4_router
from projects.A4_rag_advanced_v2.router import router as a4v2_router
from projects.A5_chains_routers.router import router as a5_router


router = APIRouter()

@router.get("/health")
def health():
    return {"status": "ok"}

@router.get("/test-llm")
async def test_llm():
    answer = await llm("Dime una frase corta divertida como un astronauta para confirmar conexiÃ³n.")
    return {"response": answer}

# Rutas de los mini-proyectos
router.include_router(a1_router)
router.include_router(a2_router)
router.include_router(a3_router)
router.include_router(a3v2_router)
router.include_router(a4_router)
router.include_router(a4v2_router)
router.include_router(a5_router)
```

---

## ðŸ“ `app/services/llm_client.py`

**Ruta:** `app/services/llm_client.py`
**FunciÃ³n:** Cliente universal para LLMs vÃ­a **OpenRouter**. Provee:

* `llm(prompt: str, model: str | None = None) -> str`: cliente async minimalista (devuelve texto).
* `llm_chain(model: str | None = None, temperature: float = 0.0) -> ChatOpenAI`: wrapper para LangChain (devuelve `ChatOpenAI` compatible con LCEL / chains).

**CÃ³digo (exacto, sin modificar):**

```python
from openai import AsyncOpenAI
from langchain_openai import ChatOpenAI
from .utils import get_env
from config_base import DEFAULT_LLM_MODEL, FALLBACK_LLM_MODEL

OPENROUTER_API_KEY = get_env("OPENROUTER_API_KEY")
OPENROUTER_BASE_URL = get_env("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")

# Cliente OpenRouter compatible con AsyncOpenAI
client = AsyncOpenAI(
    api_key=OPENROUTER_API_KEY,
    base_url=OPENROUTER_BASE_URL,
)

# ============================================================
# 1) Cliente simple (para proyectos casuales o endpoints bÃ¡sicos)
# ============================================================

# Cliente minimalista para prompts directos sin LangChain. Devuelve solo texto. Ideal para endpoints simples.
async def llm(prompt: str, model: str | None = None) -> str:
    model_to_use = model or DEFAULT_LLM_MODEL
    params = {
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 400,
        "temperature": 0.7,
        "top_p": 0.9,
    }
    try:
        response = await client.chat.completions.create(model=model_to_use, **params)

    except Exception:
        # Reintento limpio usando fallback
        response = await client.chat.completions.create(model=FALLBACK_LLM_MODEL, **params)

    return response.choices[0].message.content

# ============================================================
# 2) Cliente especial para Chains LangChain
# ============================================================

# Devuelve un objeto ChatOpenAI configurado para OpenRouter. Compatible con LLMChain, RouterChain, MultiPromptChain, agentes, etc.
def llm_chain(model: str | None = None, temperature: float = 0.0,) -> ChatOpenAI:
    model_to_use = model or DEFAULT_LLM_MODEL

    llm_params = {
        "api_key": OPENROUTER_API_KEY,
        "base_url": OPENROUTER_BASE_URL,
        "temperature": temperature,
    }

    try:
        return ChatOpenAI(model=model_to_use, **llm_params)
    except Exception:
        return ChatOpenAI(model=FALLBACK_LLM_MODEL, **llm_params)
```

---

## ðŸ“ `app/services/utils.py`

**Ruta:** `app/services/utils.py`
**FunciÃ³n:** Utilidades comunes del servidor; carga variables de entorno y helpers. (Nota: en tu repo existe versiÃ³n `get_env` en este archivo.)

**CÃ³digo (exacto, sin modificar):**

```python
import os
from dotenv import load_dotenv

load_dotenv()  # Carga .env automÃ¡ticamente

def get_env(name: str, default=None):
    value = os.getenv(name, default)
    if value is None:
        raise ValueError(f"âŒ Variable de entorno no encontrada: {name}")
    return value
```

---

## ðŸ“ `config_base.py`

**Ruta:** `config_base.py` (archivo de configuraciÃ³n global)
**FunciÃ³n:** Define rutas y parÃ¡metros globales usados por todos los mini-proyectos (paths, modelos, colecciones persistentes).

**CÃ³digo (exacto, sin modificar):**

```python
import os

# ==========================================================
# ConfiguraciÃ³n base global compartida entre todos los proyectos.
# Define rutas y parÃ¡metros comunes para LLM, RAG y almacenamiento.
# ==========================================================

# === Rutas base ===

# Ruta absoluta a la raÃ­z del repositorio
ROOT_DIR = os.path.abspath(os.path.dirname(__file__))

# Carpeta global compartida de bases vectoriales persistentes (ChromaDB)
CHROMA_PATH = os.path.join(ROOT_DIR, "chroma_db")

# Carpeta de proyectos
PROJECTS_PATH = os.path.join(ROOT_DIR, "projects")

# Carpeta de aplicaciÃ³n comÃºn (FastAPI, servicios, utilidades)
APP_PATH = os.path.join(ROOT_DIR, "app")


# === ConfiguraciÃ³n tÃ©cnica compartida ===

# Modelo de embeddings por defecto (SentenceTransformers)
DEFAULT_EMBEDDING_MODEL = "all-MiniLM-L6-v2"

# Modelo LLM default y modelo LLM fallback (para OpenRouter / OpenAI compatible)
DEFAULT_LLM_MODEL = "openai/gpt-oss-20b:free"
FALLBACK_LLM_MODEL = "nvidia/nemotron-nano-12b-v2-vl:free"
```

---



# (ExtensiÃ³n del contexto â€” ejemplos de archivos incluidos en algunos mini proyectos)**

Estos ejemplos de cÃ³digo son solo a modo de ejemplo de como se han estructurado funcionalidades en algunos mini proyectos y a modo de estilo a seguir:

repositorio [1]: https://github.com/franaragm/ia-mini-projects "GitHub - franaragm/ia-mini-projects: LangChain Lab #OpenRouter #FastAPI"

---

## âœ… **1. `schemas.py` â€” Modelos Pydantic (Ejemplo real, sin modificar)**

### **Para quÃ© sirve**

Este archivo define **las estructuras de entrada y salida de tu API** usando Pydantic.
Todos los mini-proyectos siguen este patrÃ³n:

* **QueryRequest** â†’ lo que envÃ­a el usuario al endpoint
* **SourceDocument** â†’ cada documento recuperado por el RAG (con similitud, fuente, etc.)
* **QueryResponse** â†’ respuesta final que devuelve el endpoint

### **CÃ³mo se usa**

El router lo importa y lo utiliza como `response_model`.
FastAPI convierte automÃ¡ticamente tus valores Python en el formato de las clases.

---

### **A MODO DE EJEMPLO**

```python
from pydantic import BaseModel, Field

class QueryRequest(BaseModel):
    question: str = Field(..., example="QuÃ© significa escrapear una pÃ¡gina web?")

class SourceDocument(BaseModel):
    source: str = Field(..., description="Ruta o nombre del documento de origen")
    score: float = Field(..., description="Similitud o relevancia del documento recuperado")

class QueryResponse(BaseModel):
    answer: str = Field(..., description="Respuesta generada por el modelo")
    sources: list[SourceDocument] = Field(..., description="Documentos usados para generar la respuesta")
```

---

## âœ… **2. `config.py` â€” ConfiguraciÃ³n del mini-proyecto (Ejemplo real)**

### **Para quÃ© sirve**

Define parÃ¡metros globales:

* **COLLECTION_NAME** â†’ colecciÃ³n de ChromaDB
* **EMBEDDING_MODEL** â†’ modelo SentenceTransformer
* **CHROMA_PATH** â†’ carpeta donde se guardan los vectores
* **URLS_TO_SCRAPE** â†’ URLs que se scrapearÃ¡n automÃ¡ticamente

### **CÃ³mo se usa**

El archivo `rag.py` y `chroma_client.py` importan estos valores.

---

### **A MODO DE EJEMPLO**

```python
from config_base import CHROMA_PATH, DEFAULT_EMBEDDING_MODEL

COLLECTION_NAME = "a4_docs_v2"
EMBEDDING_MODEL = DEFAULT_EMBEDDING_MODEL
CHROMA_PATH = CHROMA_PATH
URLS_TO_SCRAPE = [
    "https://es.wikipedia.org/wiki/Web_scraping",
    "https://es.wikipedia.org/wiki/Base_de_datos_de_vectores",
    # AÃ±ade mÃ¡s URLs segÃºn sea necesario
]
```

---

## âœ… **3. `prompts.py` â€” Prompts del mini-proyecto**

### **Para quÃ© sirve**

Define **TODOS los prompts del pipeline**:

* **classifier_prompt** â†’ clasifica intenciÃ³n del usuario
* **general_prompt, code_prompt, summary_prompt, math_prompt**
* **rag_prompt** â†’ prompt estricto para RAG

### **CÃ³mo se usa**

El router principal importa los prompts para combinarlos con chains LCEL.

---

### **A MODO DE EJEMPLO**

```python
from langchain_core.prompts import PromptTemplate

classifier_prompt = PromptTemplate.from_template("""
Clasifica la siguiente pregunta en una de las siguientes categorÃ­as:

- general â†’ preguntas abiertas, explicaciones, creatividad, conversaciÃ³n normal.
- rag â†’ preguntas que requieren usar informaciÃ³n REAL proveniente de documentos, base de conocimiento o datos externos. Palabras clave: "basado en documentaciÃ³n", "segÃºn el textos almacenados", "extrae de informaciÃ³n almacenada", "basado en el material", "quÃ© indican los datos almacenados".
- summary â†’ cuando se pide RESUMIR un texto proporcionado por el usuario. Solo se clasifica como summary si el usuario claramente proporciona texto para ser resumido.
- code â†’ preguntas relacionadas con programaciÃ³n, errores, generaciÃ³n de funciones, fragmentos de cÃ³digo, debugging.
- math â†’ problemas matemÃ¡ticos, cÃ¡lculos, expresiones numÃ©ricas, ecuaciones o razonamiento matemÃ¡tico.

Reglas importantes:
1. Si la pregunta pide resumir contenido proporcionado por el usuario â†’ summary.
2. Si la pregunta pide extraer, buscar o consultar informaciÃ³n de un documento, PDF, manual o contexto â†’ rag.
3. Si no estÃ¡ claro que es summary o rag â†’ clasificar como general.
4. Responde SOLO con una palabra EXACTA de esta lista:
general, rag, summary, code, math

Pregunta: "{input}"

Tu respuesta:
""")


general_prompt = PromptTemplate.from_template("""
Responde de forma clara y precisa, evitando informaciÃ³n inventada o no verificada, si no tienes la respuesta di "No lo sÃ©":
{input}
""")

code_prompt = PromptTemplate.from_template("""
Eres un asistente experto en Python. Ayuda al usuario con su cÃ³digo:
{input}
""")

summary_prompt = PromptTemplate.from_template("""
Resume el siguiente contenido de manera clara:
{input}
""")

math_prompt = PromptTemplate.from_template("""
Resuelve el siguiente ejercicio paso a paso, pero devuelve solo el resultado final:
{input}
""")

rag_prompt = PromptTemplate.from_template("""
Eres un asistente especializado en RAG. Debes responder **Ãºnicamente** con la informaciÃ³n que aparezca en el siguiente contexto. 
No inventes datos, no agregues conocimiento externo y no uses informaciÃ³n general que no estÃ© contenida explÃ­citamente en el contexto.

Si el contexto no contiene informaciÃ³n suficiente para responder la pregunta, responde exactamente:
"Sin suficiente informaciÃ³n en la documentaciÃ³n para responder."

Produce una respuesta:
- Clara, concisa y directa.
- Basada solo en detalles presentes en el contexto.
- Sintetizando y explicando, no copiando el contexto textual sin procesarlo.
- Sin aÃ±adir interpretaciones no justificadas por el contenido.

Contexto recuperado:
{context}

Pregunta:
{input}

Respuesta:
""")


```

---

## âœ… **4. `rag.py` â€” Pipeline RAG completo (Ejemplo real)**

### **Para quÃ© sirve**

Este es **el corazÃ³n del mini-proyecto RAG**.

Incluye:

âœ” Carga de documentos
âœ” Web scraping
âœ” Chunking
âœ” Evita duplicados
âœ” Inserta en ChromaDB
âœ” RecuperaciÃ³n semÃ¡ntica
âœ” CompresiÃ³n contextual
âœ” EnvÃ­o al modelo â†’ respuesta final

### **CÃ³mo se usa**

El router llama:

```python
await answer_query(question)
```

Y el pipeline hace todo.

---

### **A MODO DE EJEMPLO**

```python
from sentence_transformers import SentenceTransformer
from app.services.llm_client import llm
from .config import COLLECTION_NAME, EMBEDDING_MODEL
from .chroma_client import collection
from .loader import load_documents, split_documents
from .prompts import rag_prompt
from .utils import hash_text, is_chunk_indexed, format_sources
from .scraper import scrape_webpage

# Modelo de embeddings del proyecto
model = SentenceTransformer(EMBEDDING_MODEL)

# ==========================================================
# ConstrucciÃ³n del Ã­ndice (local + web) (siempre se reconstruye si hay nuevos documentos)
# ==========================================================

# Crea embeddings y guarda documentos nuevos en la colecciÃ³n persistente.
def build_vectorstore(urls: list[str] = None):
    print("ðŸ“Œ Iniciando indexado...")
    
    # list[Document] con todos los chunks sin procesar
    raw_chunks = []
    
    # Documentos locales desde /data/
    raw_chunks.extend(split_documents(load_documents()))
    
    # Documentos scrapeados desde URLs (si se proporcionan)
    if urls:
        for url in urls:
            print(f"Scrapeando y procesando: {url}")
            scraped_docs = scrape_webpage(url)
            raw_chunks.extend(split_documents(scraped_docs))
    
    if not raw_chunks:
        print("No se encontraron documentos para procesar.")
        return collection
    
    # Lista de nuevos chunks a indexar
    new_chunks = []

    # Recorrer raw_chunks que es un list[Document] con documentos fragmentados y verificar duplicados, 
    # cada elemento Document de la lista contiene las propiedades page_content y metadata
    for chunk in raw_chunks:
        chunk_text = chunk.page_content.strip() # Limpiar espacios en blanco alrededor y verificar texto vacÃ­o
        if not chunk_text:
            continue

        chunk_id = hash_text(chunk_text)
        if not is_chunk_indexed(chunk_id):
            new_chunks.append({
                "id": chunk_id,
                "text": chunk_text,
                "metadata": {"source": chunk.metadata.get("source", "desconocido")}
            })

    # Si hay nuevos chunks, calcular embeddings y aÃ±adirlos
    if new_chunks:
        print(f"Generando {len(new_chunks)} nuevos embeddings...")
        
        chunks_text = [item["text"] for item in new_chunks]
        ids = [item["id"] for item in new_chunks]
        metadatas = [item["metadata"] for item in new_chunks]
        vectors = model.encode(chunks_text).tolist()
        
        collection.add(
            ids=ids,
            documents=chunks_text,
            embeddings=vectors,
            metadatas=metadatas
        )
        print(f"{len(new_chunks)} fragmentos aÃ±adidos a '{COLLECTION_NAME}'.")
    else:
        print("No se encontraron nuevos fragmentos para indexar (colecciÃ³n ya actualizada).")

    return collection

# ==============================================
# CompresiÃ³n contextual
# ==============================================

# Usa el LLM para comprimir mÃºltiples documentos en un solo contexto.
async def compress_context(docs: list[str]) -> str:
    joined = "\n\n".join(docs)

    prompt = f"""
        Reduce y resume el siguiente texto manteniendo solo la informaciÃ³n esencial para contestar preguntas:

        {joined}
    """

    compressed = await llm(prompt)
    return compressed

# ==========================================================
# RecuperaciÃ³n de contexto
# ==========================================================

# Recupera contexto relevante desde la colecciÃ³n Chroma.
def retrieve_context(question: str, n_results: int = 3) -> (tuple[list[str], list[dict]]):
    # Convertir pregunta â†’ embedding
    query_vec = model.encode([question]).tolist()[0]

    # Consultar la colecciÃ³n en ChromaDB
    results = collection.query(
        query_embeddings=[query_vec],
        n_results=n_results
    )

    # Extraer documentos y metadatos (si no existen, usar listas vacÃ­as), distancias para posibles futuros usos
    retrieved_docs = results.get("documents", [[]])[0]
    metadatas = results.get("metadatas", [[]])[0]
    distances = results.get("distances", [[]])[0]
    
    # Formatear las fuentes para la respuesta
    sources = format_sources(metadatas, distances)

    return retrieved_docs, sources


# ==========================================================
# Llamada al modelo LLM
# ==========================================================

# Genera una respuesta del LLM usando el contexto relevante.
async def _answer_with_llm(context: str, question: str) -> str:
    prompt = rag_prompt.format(context=context, question=question)
    response = await llm(prompt)
    return response.strip()


# ==========================================================
# Pipeline principal RAG
# ==========================================================

# Ejecuta el pipeline RAG completo: recuperaciÃ³n, compresiÃ³n y respuesta.
async def answer_query(question: str):
    docs, sources = retrieve_context(question)
    
    # compresiÃ³n contextual
    context_compressed = await compress_context(docs)
    
    answer = await _answer_with_llm(context_compressed, question)
    return {"answer": answer, "sources": sources}

```

---

## âœ… **5. `loader.py` â€” Cargar y fragmentar documentos (Ejemplo real)**

### **Para quÃ© sirve**

Carga todos los TXT y MD desde la carpeta `data/` y los divide en chunks.

### **CÃ³mo se usa**

`rag.py` lo llama dentro de `build_vectorstore()`.

---

### **A MODO DE EJEMPLO**

```python
import os
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Ruta al directorio de datos
DATA_PATH = os.path.join(os.path.dirname(__file__), "data")

# Carga todos los documentos TXT y MD desde /data/
def load_documents():
    docs = []
    for file in os.listdir(DATA_PATH):
        if file.endswith((".txt", ".md")):
            loader = TextLoader(os.path.join(DATA_PATH, file), encoding="utf-8") # Crear cargador de texto
            docs.extend(loader.load()) # Cargar y agregar documentos
    return docs

# Divide documentos en chunks con solapamiento
def split_documents(documents, chunk_size=600, chunk_overlap=100):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ".", "!", "?", " "]
    )
    return splitter.split_documents(documents)

```

---

## âœ… **6. `chroma_client.py` â€” Cliente persistente ChromaDB**

### **Para quÃ© sirve**

Crea un cliente persistente en disco y una colecciÃ³n.

### **CÃ³mo se usa**

`rag.py` lo importa como:

```python
from .chroma_client import collection
```

---

### **A MODO DE EJEMPLO**

```python
import chromadb
from .config import COLLECTION_NAME, CHROMA_PATH

client = chromadb.PersistentClient(path=CHROMA_PATH)
collection = client.get_or_create_collection(COLLECTION_NAME)

```

---

## âœ… **7. `router.py` â€” Endpoint FastAPI del mini-proyecto RAG**

### **Para quÃ© sirve**

Define:

* ruta `/query`
* esquema de entrada y salida
* indexado automÃ¡tico en background

### **CÃ³mo se usa**

FastAPI lo monta en `main.py`.

---

### **A MODO DE EJEMPLO**

```python
import threading
from fastapi import APIRouter
from .config import URLS_TO_SCRAPE
from .schemas import QueryRequest, QueryResponse, SourceDocument
from .rag import build_vectorstore, answer_query

router = APIRouter(prefix="/a4v2", tags=["A4 - RAG Avanzado con web scraping, compresiÃ³n contextual y fuentes puntuadas"])

# Lanzamos indexado en segundo plano al importar el router
def _auto_build_index():
    try:
        print("Construyendo Ã­ndice RAG avanzado en background...")
        build_vectorstore(URLS_TO_SCRAPE)
        print("Ãndice RAG avanzado listo.")
    except Exception as e:
        print(f"[RAG] Error durante el indexado automÃ¡tico: {e}")

threading.Thread(target=_auto_build_index, daemon=True).start()


@router.post(
    "/query",
    summary="RAG Avanzado con web scraping, compresiÃ³n contextual y fuentes puntuadas",
    description="""
    Implementar un pipeline **RAG completo** usando **LangChain**, con:
    - Carga automÃ¡tica de documentos locales.
    - Web scraping de URLs especificadas.
    - CompresiÃ³n contextual.
    - Chunking inteligente.
    - IndexaciÃ³n persistente con **ChromaDB**.
    - RecuperaciÃ³n semÃ¡ntica.
    - GeneraciÃ³n de respuestas con contexto real.
    """,
    response_description="Respuesta generada y documentos fuente",
    response_model=QueryResponse
)
async def query_rag(req: QueryRequest):
    result = await answer_query(req.question)
    # Formatear las fuentes segÃºn el esquema Pydantic
    formatted_sources = [SourceDocument(**src) for src in result["sources"]]
    return QueryResponse(
        answer=result["answer"],
        sources=formatted_sources,
    )

```

---

## âœ… **8. `scraper.py` â€” Web Scraper HTML**

### **Para quÃ© sirve**

Hace scraping:

* limpia scripts/style
* extrae texto visible
* lo devuelve como `Document`

### **CÃ³mo se usa**

`rag.py` lo usa dentro de `build_vectorstore()` cuando hay URLs.

---

### **A MODO DE EJEMPLO**

```python
import requests
from bs4 import BeautifulSoup
from langchain_core.documents import Document

# FunciÃ³n para scrapear una pÃ¡gina web y devolver su contenido como lista de Document
def scrape_webpage(url: str) -> list[Document]:
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/123.0.0.0 Safari/537.36"
        )
    }
    
    resp = requests.get(url, headers=headers, timeout=10)
    resp.raise_for_status()

    # Parseamos el contenido HTML, soup contiene el Ã¡rbol DOM completo
    soup = BeautifulSoup(resp.text, "html.parser")

    # Extraemos texto visible, obtenemos los tags no deseados y los eliminamos de soup
    # tag representa cada etiqueta no deseada encontrada y es un apuntador a la misma en el Ã¡rbol DOM que contiene soup
    # tag no es un objeto independiente, por lo que al eliminarlo de soup, se elimina del Ã¡rbol DOM original
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose() # Elimina etiquetas no deseadas

    # Obtenemos el texto limpio del Ã¡rbol DOM modificado
    # retorna texto del dom sin tags HTML y separado por saltos de lÃ­nea
    text = soup.get_text(separator="\n")
    # retorna una lista de lÃ­neas limpias sin espacios en blanco ni lÃ­neas vacÃ­as
    clean_lines = [line.strip() for line in text.splitlines() if line.strip()]
    # unimos las lÃ­neas limpias en un solo string separado por saltos de lÃ­nea
    clean_text = "\n".join(clean_lines)

    # Retornamos el contenido como una lista con un solo Document
    return [Document(page_content=clean_text, metadata={"source": url})]

```

---

## âœ… **9. `utils.py` â€” Utilidades reusables**

### **Para quÃ© sirve**

Funciones clave del pipeline:

* **hash_text** â†’ evita duplicados
* **is_chunk_indexed** â†’ consulta Chroma
* **format_sources** â†’ genera puntuaciÃ³n de similitud

### **CÃ³mo se usa**

`rag.py` importa todas estas utilidades.

---

### **A MODO DE EJEMPLO**

```python
import hashlib
from .chroma_client import collection

# Genera un hash Ãºnico para un texto (para identificar chunks).
def hash_text(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()

# Comprueba si un chunk ya estÃ¡ almacenado en la colecciÃ³n.
def is_chunk_indexed(chunk_id: str) -> bool:
    try:
        existing = collection.get(ids=[chunk_id])
        return bool(existing and existing["ids"])
    except Exception:
        return False
    
# Convierte metadatos y distancias en una lista de fuentes formateadas.
def format_sources(metadatas: list, distances: list) -> list:
    formatted = []
    for meta, dist in zip(metadatas, distances):
        # Convertir distancia a similitud (asumiendo distancia >= 0)
        similarity = 1 / (1 + dist)
        formatted.append({
            "source": meta.get("source", "desconocido"),
            "score": round(similarity, 4) # Redondear a 4 decimales
        })
    return formatted

```

---
