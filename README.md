# **Mini Proyectos Laboratorio: LangChain ‚Äî Servidor IA en Python**

Este repositorio contiene una serie de **mini-proyectos progresivos** dise√±ados para aprender a construir sistemas de **IA aplicados a software real**, usando herramientas modernas del ecosistema LLM:

| Tecnolog√≠a               | Para qu√© se usa                                                    |
| ------------------------ | ------------------------------------------------------------------ |
| **Python**               | Lenguaje principal del servidor IA                                 |
| **FastAPI**              | Crear endpoints HTTP que devuelven JSON                            |
| **LangChain**            | Orquestar modelos de lenguaje, prompts, chains, RAG y agentes      |
| **OpenRouter**           | Acceder a modelos avanzados (Mistral, Gemini, LLaMA, Claude, etc.) |
| **Embeddings**           | Representar texto como vectores para b√∫squeda sem√°ntica            |
| **ChromaDB**             | Base vectorial persistente para sistemas RAG                       |
| **SentenceTransformers** | Generar embeddings locales                                         |

---

## üéØ Objetivos del repositorio

El prop√≥sito de estos mini-proyectos es aprender a construir **aplicaciones AI reales**, avanzando desde prompts simples hasta arquitecturas completas con RAG, agentes y routers.

Aprender√°s a:

### üß© Control del modelo

* Controlar y estructurar la salida de un modelo de lenguaje
* Validar y tipar respuestas (**OutputParser**)
* Encadenar pasos de razonamiento y transformar texto

### üìö RAG (Retrieval Augmented Generation)

* Crear pipelines RAG b√°sicos y avanzados
* Cargar, dividir y vectorizar documentos
* Construir y persistir bases de datos vectoriales
* Recuperar informaci√≥n con precision (top-k, puntajes, compresi√≥n contextual)
* Integrar web scraping en el pipeline RAG

### üß† Agentes y Herramientas

* Dar herramientas reales a un LLM
* Ejecutar funciones autom√°ticamente desde la IA
* Conectar la IA con APIs externas
* Gestionar memoria y contexto entre llamadas

### üßµ Chains (A5)

* Crear **cadenas secuenciales** para combinar modelos
* Crear cadenas de **transformaci√≥n**
* Crear **Router Chains** para enrutar din√°micamente
* Combinar **RAG + Chains + Clasificaci√≥n de intenci√≥n**
* Construir pipelines complejos estilo:

  ```
  Pregunta ‚Üí Clasificador ‚Üí Transformadores ‚Üí RAG ‚Üí LLM ‚Üí Respuesta final
  ```

---

## üèóÔ∏è Estructura del repositorio

```
mini-projects-langchain/
‚îÇ
‚îú‚îÄ README.md
‚îú‚îÄ config_base.py        # Configuraci√≥n global compartida
‚îú‚îÄ .env.example
‚îú‚îÄ requirements.txt
‚îÇ
‚îú‚îÄ app/
‚îÇ   ‚îú‚îÄ main.py
‚îÇ   ‚îú‚îÄ routes.py
‚îÇ   ‚îî‚îÄ services/
‚îÇ       ‚îú‚îÄ llm_client.py
‚îÇ       ‚îî‚îÄ utils.py
‚îÇ
‚îî‚îÄ projects/
    ‚îú‚îÄ A1_chat_structured/
    ‚îú‚îÄ A2_output_parser/
    ‚îú‚îÄ A3_rag_basic/
    ‚îú‚îÄ A3_rag_basic_v2/
    ‚îú‚îÄ A4_rag_advanced/
    ‚îú‚îÄ A4_rag_advanced_v2/
    ‚îú‚îÄ A5_chains_and_routers/
    ‚îú‚îÄ A5_tools_basic/
    ‚îî‚îÄ A6_tools_external_api/

```

### Sobre el directorio `app/`

`/app` contiene **todos los componentes base compartidos**:

* Inicializaci√≥n de **FastAPI**
* Enrutador general del servidor
* Cliente universal para LLM v√≠a **OpenRouter**
* Archivos de configuraci√≥n global
* Utilidades para cargar variables de entorno

Cada mini-proyecto solo agrega una nueva ruta o endpoint mediante:

```python
router.include_router(aX_router)
```

---

## üß† Lista de Mini Proyectos Completos

### **A1 ‚Äî Chat estructurado**

* Prompt fijo
* Respuesta controlada

### **A2 ‚Äî Output Parser**

* Validaci√≥n estricta
* Tipado de salida
* Conversi√≥n a JSON robusto

### **A3 ‚Äî RAG B√°sico**

* Cargar documentos
* Fragmentarlos
* Crear embeddings
* Recuperar contexto

### **A3 V2 ‚Äî RAG B√°sico Mejorado**

* Limpieza mejorada
* Separadores custom
* Mejor chunking

### **A4 ‚Äî RAG Avanzado**

* Evidencias
* Fuentes con puntaje
* Control anti-alucinaciones

### **A4 V2 ‚Äî RAG con Web Scraping + Compresi√≥n Contextual**

* Scrapeo de p√°ginas web
* Mezclar documentos locales y online
* Compresi√≥n del contexto
* RAG de m√∫ltiples etapas

---

## üÜï **A5 ‚Äî Chains & Routers (Cadenas avanzadas)**

Este mini-proyecto es una expansi√≥n importante. Incluye:

### ‚úîÔ∏è Clasificador de intenci√≥n (Intent Classifier)

Determina si la pregunta es:

* general
* sobre c√≥digo
* RAG
* resumen
* matem√°tica

### ‚úîÔ∏è Cadenas espec√≠ficas (General, Code, Summary, Math)

Cada cadena es un **LLMChain**.

### ‚úîÔ∏è RAGChain integrada

Integraci√≥n directa con ChromaDB para consultas sem√°nticas dentro de una chain.

### ‚úîÔ∏è Router din√°mico

L√≥gica para enrutar la petici√≥n hacia la chain correcta:

```
Input ‚Üí ClassifierChain ‚Üí [GeneralChain | CodeChain | SummaryChain | MathChain | RAGChain]
```

### ‚úîÔ∏è Cadena secuencial avanzada

Permite construir aplicaciones complejas combinando pasos:

```
‚Üí Normalizar texto
‚Üí Clasificar intenci√≥n
‚Üí Seleccionar cadena
‚Üí Ejecutar pipeline RAG si aplica
‚Üí Generar respuesta final
```

Este mini-proyecto introduce **la arquitectura que usan aplicaciones reales** (Copilot, ChatGPT Tools, agentes planificadores, etc.).

---

## ‚öôÔ∏è Instalaci√≥n del entorno

### 1) Crear entorno virtual

```bash
python -m venv .venv
source .venv/bin/activate      # Mac / Linux
.venv\Scripts\activate         # Windows
```

### 2) Instalar dependencias

```bash
pip install -r requirements.txt
```

### 3) Configurar variables de entorno

```bash
cp .env.example .env
```

Edita tu `.env`:

```
OPENROUTER_API_KEY=TU_API_KEY
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
ENV=dev
```

Obtener API key:
[https://openrouter.ai/keys](https://openrouter.ai/keys)

---

## ‚ñ∂Ô∏è Ejecutar el servidor

```bash
uvicorn app.main:app --reload --port 8000
```

Rutas de prueba:

```
GET /health
GET /test-llm
```

---


## üõ†Ô∏è **config_base.py (configuraci√≥n global del repositorio)**

Este archivo centraliza la configuraci√≥n compartida entre todos los mini-proyectos.

Se encuentra en:

```
/config_base.py
```

### ‚úîÔ∏è ¬øPor qu√© existe este archivo?

Evita repetici√≥n de l√≥gica en cada mini-proyecto:

* Define rutas absolutas comunes
* Establece los modelos LLM por defecto
* Define el modelo de embeddings est√°ndar
* Mantiene la configuraci√≥n de almacenamiento del RAG centralizada

As√≠ cualquier mini-proyecto puede simplemente importar:

```python
from config_base import CHROMA_PATH, DEFAULT_LLM_MODEL
```

---

## üìÑ **Contenido completo de `config_base.py`**

```python
import os 

# ==========================================================
# Configuraci√≥n base global compartida entre todos los proyectos.
# Define rutas y par√°metros comunes para LLM, RAG y almacenamiento.
# ==========================================================

# === Rutas base ===

# Ruta absoluta a la ra√≠z del repositorio
ROOT_DIR = os.path.abspath(os.path.dirname(__file__))

# Carpeta global compartida de bases vectoriales persistentes (ChromaDB)
CHROMA_PATH = os.path.join(ROOT_DIR, "chroma_db")

# Carpeta de proyectos
PROJECTS_PATH = os.path.join(ROOT_DIR, "projects")

# Carpeta de aplicaci√≥n com√∫n (FastAPI, servicios, utilidades)
APP_PATH = os.path.join(ROOT_DIR, "app")


# === Configuraci√≥n t√©cnica compartida ===

# Modelo de embeddings por defecto (SentenceTransformers)
DEFAULT_EMBEDDING_MODEL = "all-MiniLM-L6-v2"

# Modelo LLM default y modelo LLM fallback (para OpenRouter / OpenAI compatible)
DEFAULT_LLM_MODEL = "openai/gpt-oss-20b:free"
FALLBACK_LLM_MODEL = "nvidia/nemotron-nano-12b-v2-vl:free"
```

## üöÄ ¬øQu√© sigue?

Puedes continuar con:

* **A6: Tools con APIs externas reales**
* **A7: Memory, historiales y buffers**
* **A8: Agents con m√∫ltiples herramientas**
* **A9: Multi-step planning (ReAct / MRKL)**
* **A10: RAG h√≠brido (web + local + embeddings mixtos)**

---

## ‚ö†Ô∏è Consideraciones para macOS Intel (Python 3.11)

En este equipo espec√≠fico con macOS 14 Intel, se requieren algunos pasos adicionales para evitar conflictos de dependencias:

---

### 1Ô∏è‚É£ Instalar pyenv y configurar Python 3.11

1. Instalamos `pyenv`:

```bash
brew install pyenv
```

2. Instalamos Python 3.11 y lo configuramos como **versi√≥n global** del equipo:

```bash
pyenv install 3.11.8
pyenv global 3.11.8
```

3. Verificamos que se use la versi√≥n correcta:

```bash
python3 -V       # Debe mostrar Python 3.11.8
which python3    # Debe apuntar a ~/.pyenv/versions/3.11.8/bin/python3
```

---

### 2Ô∏è‚É£ Configurar el shell (`zsh`) para pyenv

Para que pyenv funcione correctamente en todas las sesiones, a√±adimos estas l√≠neas a **`~/.zshrc`** o **`~/.zprofile`**:

```bash
export PYENV_ROOT="$HOME/.pyenv"
export PATH="$PYENV_ROOT/bin:$PATH"
eval "$(pyenv init - zsh)"
```

Luego recargamos la configuraci√≥n:

```bash
source ~/.zshrc
```

> Esto asegura que `python3` y `pip` apunten a la versi√≥n de pyenv, no al Python del sistema.

---

### 3Ô∏è‚É£ Crear y activar entorno virtual

1. Creamos un entorno virtual dentro del proyecto:

```bash
python -m venv .venv
```

2. Activamos el entorno:

```bash
source .venv/bin/activate   # macOS / Linux
```

3. Verificamos que `python` y `pip` apunten al entorno virtual:

```bash
which python   # Debe apuntar a .venv/bin/python
which pip      # Debe apuntar a .venv/bin/pip
python -V      # Debe mostrar Python 3.11.8
```

---

### 4Ô∏è‚É£ Instalar pip para la versi√≥n de pyenv

Si el entorno no tiene pip:

```bash
python -m ensurepip
python -m pip install --upgrade pip
```

> Ahora pip est√° correctamente asociado a Python 3.11 del entorno virtual.

---

### 5Ô∏è‚É£ Forzar NumPy < 2

Para compatibilidad con paquetes compilados (PyTorch, sentence-transformers):

```bash
pip uninstall numpy -y
pip install "numpy<2"
```

---

### 6Ô∏è‚É£ Instalar dependencias sin usar cach√©

```bash
pip install --no-cache-dir -r requirements.txt
```

> Esto evita que se instalen versiones antiguas o incompatibles de los paquetes.

---

### 7Ô∏è‚É£ Verificar instalaci√≥n

```bash
python -c "import numpy; print(numpy.__version__)"
python -c "import torch; print(torch.__version__)"
python -c "from sentence_transformers import SentenceTransformer; print('ST OK')"
```

Todo debe funcionar sin errores.

---

### 8Ô∏è‚É£ Arrancar el servidor

Con el entorno activado:

```bash
uvicorn app.main:app --reload --port 8000
```

> Ahora el servidor funciona correctamente, sin errores de NumPy o PyTorch en este equipo.

---

