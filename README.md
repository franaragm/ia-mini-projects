# **Mini Proyectos Laboratorio: LangChain â€” Servidor IA en Python**

Este repositorio contiene una serie de **mini-proyectos progresivos** diseÃ±ados para aprender a construir sistemas de **IA aplicados a software real**, usando herramientas modernas del ecosistema LLM:

| TecnologÃ­a               | Para quÃ© se usa                                                    |
| ------------------------ | ------------------------------------------------------------------ |
| **Python**               | Lenguaje principal del servidor IA                                 |
| **FastAPI**              | Crear endpoints HTTP que devuelven JSON                            |
| **LangChain**            | Orquestar modelos de lenguaje, prompts, chains, RAG y agentes      |
| **OpenRouter**           | Acceder a modelos avanzados (Mistral, Gemini, LLaMA, Claude, etc.) |
| **Embeddings**           | Representar texto como vectores para bÃºsqueda semÃ¡ntica            |
| **ChromaDB**             | Base vectorial persistente para sistemas RAG                       |
| **SentenceTransformers** | Generar embeddings locales                                         |

---

## ğŸ¯ Objetivos del repositorio

El propÃ³sito de estos mini-proyectos es aprender a construir **aplicaciones AI reales**, avanzando desde prompts simples hasta arquitecturas completas con RAG, agentes y routers.

AprenderÃ¡s a:

### ğŸ§© Control del modelo

* Controlar y estructurar la salida de un modelo de lenguaje
* Validar y tipar respuestas (**OutputParser**)
* Encadenar pasos de razonamiento y transformar texto

### ğŸ“š RAG (Retrieval Augmented Generation)

* Crear pipelines RAG bÃ¡sicos y avanzados
* Cargar, dividir y vectorizar documentos
* Construir y persistir bases de datos vectoriales
* Recuperar informaciÃ³n con precision (top-k, puntajes, compresiÃ³n contextual)
* Integrar web scraping en el pipeline RAG

### ğŸ§  Agentes y Herramientas

* Dar herramientas reales a un LLM
* Ejecutar funciones automÃ¡ticamente desde la IA
* Conectar la IA con APIs externas
* Gestionar memoria y contexto entre llamadas

### ğŸ§µ Chains (A5)

* Crear **cadenas secuenciales** para combinar modelos
* Crear cadenas de **transformaciÃ³n**
* Crear **Router Chains** para enrutar dinÃ¡micamente
* Combinar **RAG + Chains + ClasificaciÃ³n de intenciÃ³n**
* Construir pipelines complejos estilo:

  ```
  Pregunta â†’ Clasificador â†’ Transformadores â†’ RAG â†’ LLM â†’ Respuesta final
  ```

---

## ğŸ—ï¸ Estructura del repositorio

```
mini-projects-langchain/
â”‚
â”œâ”€ README.md
â”œâ”€ config_base.py        # ConfiguraciÃ³n global compartida
â”œâ”€ .env.example
â”œâ”€ requirements.txt
â”‚
â”œâ”€ app/
â”‚   â”œâ”€ main.py
â”‚   â”œâ”€ routes.py
â”‚   â””â”€ services/
â”‚       â”œâ”€ llm_client.py
â”‚       â””â”€ utils.py
â”‚
â””â”€ projects/
    â”œâ”€ A1_chat_structured/
    â”œâ”€ A2_output_parser/
    â”œâ”€ A3_rag_basic/
    â”œâ”€ A3_rag_basic_v2/
    â”œâ”€ A4_rag_advanced/
    â”œâ”€ A4_rag_advanced_v2/
    â”œâ”€ A5_chains_and_routers/
    â”œâ”€ A5_tools_basic/
    â””â”€ A6_tools_external_api/

```

### Sobre el directorio `app/`

`/app` contiene **todos los componentes base compartidos**:

* InicializaciÃ³n de **FastAPI**
* Enrutador general del servidor
* Cliente universal para LLM vÃ­a **OpenRouter**
* Archivos de configuraciÃ³n global
* Utilidades para cargar variables de entorno

Cada mini-proyecto solo agrega una nueva ruta o endpoint mediante:

```python
router.include_router(aX_router)
```

---

## ğŸ§  Lista de Mini Proyectos Completos

### **A1 â€” Chat estructurado**

* Prompt fijo
* Respuesta controlada

### **A2 â€” Output Parser**

* ValidaciÃ³n estricta
* Tipado de salida
* ConversiÃ³n a JSON robusto

### **A3 â€” RAG BÃ¡sico**

* Cargar documentos
* Fragmentarlos
* Crear embeddings
* Recuperar contexto

### **A3 V2 â€” RAG BÃ¡sico Mejorado**

* Limpieza mejorada
* Separadores custom
* Mejor chunking

### **A4 â€” RAG Avanzado**

* Evidencias
* Fuentes con puntaje
* Control anti-alucinaciones

### **A4 V2 â€” RAG con Web Scraping + CompresiÃ³n Contextual**

* Scrapeo de pÃ¡ginas web
* Mezclar documentos locales y online
* CompresiÃ³n del contexto
* RAG de mÃºltiples etapas

---

## ğŸ†• **A5 â€” Chains & Routers (Cadenas avanzadas)**

Este mini-proyecto es una expansiÃ³n importante. Incluye:

### âœ”ï¸ Clasificador de intenciÃ³n (Intent Classifier)

Determina si la pregunta es:

* general
* sobre cÃ³digo
* RAG
* resumen
* matemÃ¡tica

### âœ”ï¸ Cadenas especÃ­ficas (General, Code, Summary, Math)

Cada cadena es un **LLMChain**.

### âœ”ï¸ RAGChain integrada

IntegraciÃ³n directa con ChromaDB para consultas semÃ¡nticas dentro de una chain.

### âœ”ï¸ Router dinÃ¡mico

LÃ³gica para enrutar la peticiÃ³n hacia la chain correcta:

```
Input â†’ ClassifierChain â†’ [GeneralChain | CodeChain | SummaryChain | MathChain | RAGChain]
```

### âœ”ï¸ Cadena secuencial avanzada

Permite construir aplicaciones complejas combinando pasos:

```
â†’ Normalizar texto
â†’ Clasificar intenciÃ³n
â†’ Seleccionar cadena
â†’ Ejecutar pipeline RAG si aplica
â†’ Generar respuesta final
```

Este mini-proyecto introduce **la arquitectura que usan aplicaciones reales** (Copilot, ChatGPT Tools, agentes planificadores, etc.).

---

## âš™ï¸ InstalaciÃ³n del entorno

### 1) Crear entorno virtual

```bash
python -m venv .venv
source .venv/bin/activate      # Mac / Linux
.venv\Scripts\activate         # Windows
```

### 2) Instalar dependencias

```bash
pip install -r requirements.txt
```

### 3) Configurar variables de entorno

```bash
cp .env.example .env
```

Edita tu `.env`:

```
OPENROUTER_API_KEY=TU_API_KEY
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
ENV=dev
```

Obtener API key:
[https://openrouter.ai/keys](https://openrouter.ai/keys)

---

## â–¶ï¸ Ejecutar el servidor

```bash
uvicorn app.main:app --reload --port 8000
```

Rutas de prueba:

```
GET /health
GET /test-llm
```

---


## ğŸ› ï¸ **config_base.py (configuraciÃ³n global del repositorio)**

Este archivo centraliza la configuraciÃ³n compartida entre todos los mini-proyectos.

Se encuentra en:

```
/config_base.py
```

### âœ”ï¸ Â¿Por quÃ© existe este archivo?

Evita repeticiÃ³n de lÃ³gica en cada mini-proyecto:

* Define rutas absolutas comunes
* Establece los modelos LLM por defecto
* Define el modelo de embeddings estÃ¡ndar
* Mantiene la configuraciÃ³n de almacenamiento del RAG centralizada

AsÃ­ cualquier mini-proyecto puede simplemente importar:

```python
from config_base import CHROMA_PATH, DEFAULT_LLM_MODEL
```

---

## ğŸ“„ **Contenido completo de `config_base.py`**

```python
import os 

# ==========================================================
# ConfiguraciÃ³n base global compartida entre todos los proyectos.
# Define rutas y parÃ¡metros comunes para LLM, RAG y almacenamiento.
# ==========================================================

# === Rutas base ===

# Ruta absoluta a la raÃ­z del repositorio
ROOT_DIR = os.path.abspath(os.path.dirname(__file__))

# Carpeta global compartida de bases vectoriales persistentes (ChromaDB)
CHROMA_PATH = os.path.join(ROOT_DIR, "chroma_db")

# Carpeta de proyectos
PROJECTS_PATH = os.path.join(ROOT_DIR, "projects")

# Carpeta de aplicaciÃ³n comÃºn (FastAPI, servicios, utilidades)
APP_PATH = os.path.join(ROOT_DIR, "app")


# === ConfiguraciÃ³n tÃ©cnica compartida ===

# Modelo de embeddings por defecto (SentenceTransformers)
DEFAULT_EMBEDDING_MODEL = "all-MiniLM-L6-v2"

# Modelo LLM default y modelo LLM fallback (para OpenRouter / OpenAI compatible)
DEFAULT_LLM_MODEL = "openai/gpt-oss-20b:free"
FALLBACK_LLM_MODEL = "nvidia/nemotron-nano-12b-v2-vl:free"
```

## ğŸš€ Â¿QuÃ© sigue?

Puedes continuar con:

* **A6: Tools con APIs externas reales**
* **A7: Memory, historiales y buffers**
* **A8: Agents con mÃºltiples herramientas**
* **A9: Multi-step planning (ReAct / MRKL)**
* **A10: RAG hÃ­brido (web + local + embeddings mixtos)**

---

## âš ï¸ Consideraciones para macOS Intel (Python 3.11)

En este equipo especÃ­fico con macOS 14 Intel, se requieren algunos pasos adicionales para evitar conflictos de dependencias:

---

### 1ï¸âƒ£ Instalar pyenv y configurar Python 3.11

1. Instalamos `pyenv`:

```bash
brew install pyenv
```

2. Instalamos Python 3.11 y lo configuramos como **versiÃ³n global** del equipo:

```bash
pyenv install 3.11.8
pyenv global 3.11.8
```

3. Verificamos que se use la versiÃ³n correcta:

```bash
python3 -V       # Debe mostrar Python 3.11.8
which python3    # Debe apuntar a ~/.pyenv/versions/3.11.8/bin/python3
```

---

### 2ï¸âƒ£ Configurar el shell (`zsh`) para pyenv

Para que pyenv funcione correctamente en todas las sesiones, aÃ±adimos estas lÃ­neas a **`~/.zshrc`** o **`~/.zprofile`**:

```bash
export PYENV_ROOT="$HOME/.pyenv"
export PATH="$PYENV_ROOT/bin:$PATH"
eval "$(pyenv init - zsh)"
```

Luego recargamos la configuraciÃ³n:

```bash
source ~/.zshrc
```

> Esto asegura que `python3` y `pip` apunten a la versiÃ³n de pyenv, no al Python del sistema.

---

### 3ï¸âƒ£ Crear y activar entorno virtual

1. Creamos un entorno virtual dentro del proyecto:

```bash
python -m venv .venv
```

2. Activamos el entorno:

```bash
source .venv/bin/activate   # macOS / Linux
```

3. Verificamos que `python` y `pip` apunten al entorno virtual:

```bash
which python   # Debe apuntar a .venv/bin/python
which pip      # Debe apuntar a .venv/bin/pip
python -V      # Debe mostrar Python 3.11.8
```

---

### 4ï¸âƒ£ Instalar pip para la versiÃ³n de pyenv

Si el entorno no tiene pip:

```bash
python -m ensurepip
python -m pip install --upgrade pip
```

> Ahora pip estÃ¡ correctamente asociado a Python 3.11 del entorno virtual.

---

### 5ï¸âƒ£ Forzar NumPy < 2

Para compatibilidad con paquetes compilados (PyTorch, sentence-transformers):

```bash
pip uninstall numpy -y
pip install "numpy<2"
```

---

### 6ï¸âƒ£ Instalar dependencias sin usar cachÃ©

```bash
pip install --no-cache-dir -r requirements.txt
```

> Esto evita que se instalen versiones antiguas o incompatibles de los paquetes.

---

### 7ï¸âƒ£ Verificar instalaciÃ³n

```bash
python -c "import numpy; print(numpy.__version__)"
python -c "import torch; print(torch.__version__)"
python -c "from sentence_transformers import SentenceTransformer; print('ST OK')"
```

Todo debe funcionar sin errores.

---

### 8ï¸âƒ£ Arrancar el servidor

Con el entorno activado:

```bash
uvicorn app.main:app --reload --port 8000
```

> Ahora el servidor funciona correctamente, sin errores de NumPy o PyTorch en este equipo.

---

Si quieres, puedo hacer una **versiÃ³n resumida â€œnota rÃ¡pida del equipoâ€**, de unas 10-12 lÃ­neas, perfecta para poner **al final del README** y que quede claro para cualquier persona que use este Mac Intel.

Â¿Quieres que haga eso?

