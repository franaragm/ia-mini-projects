# ğŸš€ **A5 â€“ LangChain LCEL: Chains, Runnables y Router Inteligente**

Este mÃ³dulo implementa un pipeline profesional basado en **LangChain Expression Language (LCEL)** para:

* Crear *chains declarativas y componibles*
* Enrutar preguntas a la cadena correcta mediante clasificadores
* Combinar *async* + *sync*
* Usar `RunnableBranch`, `RunnableLambda`, `RunnableMap`
* Encapsular un RAG como chain integrada

El archivo clave del proyecto es:

```
A5_chains_routers/
â”‚
â”œâ”€â”€ chains.py  â† â­ EXPLICADO A DETALLE EN ESTE README
â”œâ”€â”€ router.py
â”œâ”€â”€ prompts.py
â”œâ”€â”€ rag.py
â””â”€â”€ ...
```

---

# ğŸ“˜ **1. Overview del flujo completo**

El pipeline final aplica esta secuencia:

```
Pregunta
   â†“
ClassifierChain  (LCEL)
   â†“  intent: rag | code | summary | math | general
RouterChain (RunnableBranch)
   â†“
Cadena seleccionada (General/Code/Summary/Math/RAG)
   â†“
Respuesta final + chain_used
```

Diagrama:

```
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ classifier_chain â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚  intent
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 router_chain                     â”‚
â”‚ (RunnableBranch con condiciones dinÃ¡micas)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ "rag"     â†’ rag_chain                            â”‚
â”‚ "code"    â†’ code_chain                           â”‚
â”‚ "summary" â†’ summary_chain                        â”‚
â”‚ "math"    â†’ math_chain                           â”‚
â”‚ default   â†’ general_chain                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                   Respuesta final
```

---

# ğŸ§  **2. Â¿QuÃ© es LCEL y por quÃ© se usa aquÃ­?**

**LCEL (LangChain Expression Language)** permite componer chains usando operadores:

* `|` pipe operator: *encadena steps*
* Runnables como:

  * `RunnableLambda` â†’ transforma inputs/outputs con Python puro
  * `RunnableBranch` â†’ router inteligente
  * `RunnableMap` â†’ salida estructurada

Ventajas:

* Declarativo
* Menos boilerplate
* Funciona igual en sync + async
* Mejor rendimiento (streaming optimizado)

---

# ğŸ“‚ **3. ExplicaciÃ³n lÃ­nea por lÃ­nea de `chains.py`**

---

## ğŸ”¸ **Imports**

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import (
    RunnableBranch,
    RunnableLambda,
)
from app.services.llm_client import llm_chain
from .prompts import (
    classifier_prompt,
    general_prompt,
    code_prompt,
    summary_prompt,
    math_prompt,
)
from .rag import rag_chain
```

### ExplicaciÃ³n

* `StrOutputParser` â†’ Normaliza la salida del LLM como un string limpio.
* `RunnableBranch` â†’ Router condicional â€œsi X entonces usa esta chainâ€.
* `RunnableLambda` â†’ Funciones Python dentro de una cadena.
* `llm_chain()` â†’ Devuelve el cliente LLM (OpenAI, OpenRouter, etc.).
* `prompts.py` â†’ Cada chain tiene su prompt.
* `rag_chain` â†’ RAG declarado como runnable independiente.

---

# ğŸ§± **4. Inicializar LLM + Parser**

```python
llm = llm_chain()
parser = StrOutputParser()
```

### ExplicaciÃ³n

* `llm` es un runnable â€” cualquier chain puede recibirlo vÃ­a `|`.
* `parser` convierte la respuesta del LLM en texto sin formato.

---

# ğŸ—ï¸ **5. CreaciÃ³n de cada chain con LCEL**

Cada chain sigue este patrÃ³n:

```
prompt â†’ llm â†’ parser â†’ formateo final (RunnableLambda)
```

Ejemplo completo:

---

## ğŸ”¹ **General Chain**

```python
general_chain = (
    general_prompt
    | llm
    | parser
    | RunnableLambda(lambda x: {"answer": x, "chain_used": "general_chain"})
)
```

### ExplicaciÃ³n **lÃ­nea por lÃ­nea**

#### `general_prompt | llm`

EnvÃ­a el prompt al modelo y obtiene respuesta cruda.

#### `| parser`

Convierte la salida del modelo a un string limpio.

#### `| RunnableLambda(lambda x: {...})`

**AÃ±ade metadatos adicionales** a la salida.

### ğŸ” Â¿QuÃ© hace exactamente el `lambda`?

La firma es:

```python
lambda x: {"answer": x, "chain_used": "general_chain"}
```

Esto significa:

* Recibe la salida del paso anterior (`x = respuesta del LLM`)
* Produce un diccionario nuevo con:

  * `"answer"`     â†’ texto de la respuesta
  * `"chain_used"` â†’ nombre de la chain

AsÃ­ todas las chains devuelven el mismo esquema.

---

## ğŸ”¹ **Otras Chains (idÃ©ntico patrÃ³n)**

Todas siguen el mismo diseÃ±o:

```python
code_chain = (
    code_prompt
    | llm
    | parser
    | RunnableLambda(lambda x: {"answer": x, "chain_used": "code_chain"})
)

summary_chain = (
    summary_prompt
    | llm
    | parser
    | RunnableLambda(lambda x: {"answer": x, "chain_used": "summary_chain"})
)

math_chain = (
    math_prompt
    | llm
    | parser
    | RunnableLambda(lambda x: {"answer": x, "chain_used": "math_chain"})
)
```

---

## ğŸ”¹ **RAG Chain**

```python
rag_chain = (
    rag_chain
    | RunnableLambda(lambda x: {"answer": x, "chain_used": "rag_chain"})
)
```

---

# ğŸš¦ **6. ConstrucciÃ³n del Router LCEL (RunnableBranch)**

Este es el corazÃ³n del sistema.

```python
router_chain = RunnableBranch(
    (lambda x: "rag" in x["intent"], rag_chain),
    (lambda x: "code" in x["intent"], code_chain),
    (lambda x: "summary" in x["intent"], summary_chain),
    (lambda x: "math" in x["intent"], math_chain),
    general_chain,  # default
)
```

---

## ğŸ§© CÃ³mo funciona `RunnableBranch`

`RunnableBranch` evalÃºa cada condiciÃ³n en orden:

```
(condiciÃ³n1, cadena1)
(condiciÃ³n2, cadena2)
...
default_chain
```

El primer condicional `True` determina la chain seleccionada.

---

## ğŸ” ExplicaciÃ³n de cada `lambda`

Ejemplo:

```python
lambda x: "rag" in x["intent"]
```

Significa:

* Recibe un diccionario `x` con:

  ```json
  {"intent": "<intenciÃ³n>", "input": "<pregunta>"}
  ```
* EvalÃºa si la intenciÃ³n contiene `"rag"`.

Si es True â†’ se ejecuta `rag_chain`.

---

### ğŸ”¥ Diagrama del router

```
                 intent
                    â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                     â–¼
   (lambda cond1)        Â¿True? sÃ­ â†’ rag_chain
         â”‚ no
         â–¼
   (lambda cond2)        Â¿True? sÃ­ â†’ code_chain
         â”‚ no
         â–¼
   (lambda cond3)        Â¿True? sÃ­ â†’ summary_chain
         â”‚ no
         â–¼
   (lambda cond4)        Â¿True? sÃ­ â†’ math_chain
         â”‚ no
         â–¼
       default â†’ general_chain
```

---

# âš™ï¸ **7. FunciÃ³n principal: `run_router_chain()`**

```python
async def run_router_chain(question: str):

    # Paso 1: Intent
    intent = classifier_chain.invoke({"input": question}).strip().lower()

    # Paso 2: Router async
    block = await router_chain.ainvoke({"intent": intent, "input": question})

    # Paso 3: Resultado final
    return {
        "intent": intent,
        "chain_used": block["chain_used"],
        "answer": block["answer"].strip(),
    }
```

## ExplicaciÃ³n paso a paso

---

### **1) ClasificaciÃ³n (sync)**

```python
intent = classifier_chain.invoke({"input": question})
```

* `invoke()` es SÃNCRONO.
* Devuelve string.
* Se normaliza `.strip().lower()`.

---

### **2) Router (async)**

```python
block = await router_chain.ainvoke(...)
```

* `ainvoke()` es *asÃ­ncrono*.
* `router_chain` decide la chain que se ejecuta mediante `RunnableBranch`.
* `block` contiene:

  ```json
  {
    "answer": "...",
    "chain_used": "summary_chain"
  }
  ```

---

### **3) Respuesta estructurada**

```python
return {
    "intent": intent,
    "chain_used": block["chain_used"],
    "answer": block["answer"].strip(),
}
```

---

# âœ”ï¸ **8. Resultado final del pipeline**

Cuando llamas a:

```python
await run_router_chain("resume este texto...")
```

El sistema sigue este flujo:

```
input
 â†“
classifier_chain.invoke()
 â†“ intent="summary"
router_chain.ainvoke()
 â†“
summary_chain
 â†“
{ "answer": "...", "chain_used": "summary_chain" }
```

---

# ğŸ¯ **9. Ventajas de esta arquitectura**

| Elemento             | FunciÃ³n                                        |                                 |
| -------------------- | ---------------------------------------------- | ------------------------------- |
| **LCEL               | **                                             | ComposiciÃ³n clara y declarativa |
| **RunnableLambda**   | Adjuntar metadata & transformar outputs        |                                 |
| **RunnableBranch**   | Enrutamiento profesional                       |                                 |
| **Async + Sync**     | Compatible con FastAPI                         |                                 |
| **Formato uniforme** | Todas las chains devuelven la misma estructura |                                 |

---

# ğŸ **10. ConclusiÃ³n**

Este proyecto demuestra cÃ³mo construir un **router inteligente modular**, con una arquitectura clara, mantenible y extensible basada en LangChain LCEL.

Puedes aÃ±adir nuevas chains simplemente:

1. Crear prompt
2. Declarar chain con `| llm | parser | RunnableLambda`
3. AÃ±adir condiciÃ³n al router

Escalable y 100% profesional.

---
