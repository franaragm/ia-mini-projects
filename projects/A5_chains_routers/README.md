# ğŸš€ **A5 â€“ LangChain LCEL: Chains, Runnables y Router Inteligente**

Este mÃ³dulo implementa un pipeline profesional basado en **LangChain Expression Language (LCEL)** para:

* Crear *chains declarativas y componibles*
* Enrutar preguntas a la cadena correcta mediante clasificadores
* Combinar *async* + *sync*
* Usar `RunnableBranch`, `RunnableLambda`, `RunnablePassthrough`
* Integrar RAG como chain LCEL sin funciones externas

El archivo clave del proyecto es:

```
A5_chains_routers/
â”‚
â”œâ”€â”€ chains.py  â† â­ EXPLICADO A DETALLE EN ESTE README
â”œâ”€â”€ router.py
â”œâ”€â”€ prompts.py
â”œâ”€â”€ rag.py   â† solo contiene retrieve_context()
â””â”€â”€ ...
```

---

# ğŸ“˜ **1. Overview del flujo completo**

El pipeline final aplica esta secuencia:

```
Pregunta
   â†“
ClassifierChain  (LCEL)
   â†“  intent: rag | code | summary | math | general
RouterChain (RunnableBranch)
   â†“
Cadena seleccionada (General/Code/Summary/Math/RAG)
   â†“
Respuesta final + chain_used
```

Diagrama:

```
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ classifier_chain â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚  intent
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 router_chain                     â”‚
â”‚ (RunnableBranch con condiciones dinÃ¡micas)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ "rag"     â†’ rag_chain                            â”‚
â”‚ "code"    â†’ code_chain                           â”‚
â”‚ "summary" â†’ summary_chain                        â”‚
â”‚ "math"    â†’ math_chain                           â”‚
â”‚ default   â†’ general_chain                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                   Respuesta final
```

---

# ğŸ§  **2. Â¿QuÃ© es LCEL y por quÃ© se usa aquÃ­?**

**LCEL (LangChain Expression Language)** permite componer chains usando operadores:

* `|` pipe operator: *encadena steps*
* Runnables como:

  * `RunnableLambda` â†’ transforma inputs/outputs con Python puro
  * `RunnableBranch` â†’ router inteligente
  * `RunnablePassthrough` â†’ paso de datos sin modificar

Ventajas:

* Declarativo
* Menos boilerplate
* Funciona igual en sync + async
* Mejor rendimiento (streaming optimizado)

---

# ğŸ“‚ **3. ExplicaciÃ³n lÃ­nea por lÃ­nea de `chains.py`**

---

## ğŸ”¸ **Imports**

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableBranch, RunnableLambda, RunnablePassthrough
from app.services.llm_client import llm_chain
from .prompts import (
    classifier_prompt,
    general_prompt,
    code_prompt,
    summary_prompt,
    math_prompt,
    rag_prompt,
)
from .rag import retrieve_context
```

### ExplicaciÃ³n

* `StrOutputParser` â†’ Normaliza la salida del LLM como un string limpio.
* `RunnableBranch` â†’ Router condicional.
* `RunnableLambda` â†’ Funciones Python dentro de una cadena.
* `RunnablePassthrough` â†’ paso de datos sin transformaciÃ³n.
* `llm_chain()` â†’ Devuelve el cliente LLM (OpenRouter, OpenAI, etc.).
* `prompts.py` â†’ Cada chain tiene su prompt.
* `retrieve_context()` â†’ Solo recuperaciÃ³n de contexto para RAG.

---

# ğŸ§± **4. Inicializar LLM + Parser**

```python
llm = llm_chain()
parser = StrOutputParser()
```

---

# ğŸ—ï¸ **5. CreaciÃ³n de cada chain con LCEL**

Cada chain sigue el patrÃ³n:

```
prompt â†’ llm â†’ parser â†’ formateo final (RunnableLambda)
```

Ejemplo:

```python
general_chain = (
    general_prompt
    | llm
    | parser
    | RunnableLambda(lambda x: {"answer": x, "chain_used": "general_chain"})
)
```

---

## ğŸ”¹ **RAG Chain totalmente LCEL**

Ahora el RAG se construye como un **pipeline declarativo**:

```python
rag_chain = (
    {"input": RunnablePassthrough()}  # Paso la pregunta
    | RunnableLambda(lambda x: {
        "input": x["input"],
        "context": retrieve_context(x["input"])
    })  # Recupera contexto
    | RunnableLambda(lambda x: rag_prompt.format(
        context=x["context"],
        input=x["input"]
    ))  # Construye prompt
    | llm
    | parser
    | RunnableLambda(lambda x: {"answer": x, "chain_used": "rag_chain"})
)
```

**Ventajas**:

* No se necesita funciÃ³n async externa
* Encapsula todo: recuperaciÃ³n + prompt + LLM + parseo
* Siempre devuelve `{answer, chain_used}`

---

# ğŸš¦ **6. Router LCEL (RunnableBranch)**

```python
router_chain = RunnableBranch(
    (lambda x: "rag" in x["intent"], rag_chain),
    (lambda x: "code" in x["intent"], code_chain),
    (lambda x: "summary" in x["intent"], summary_chain),
    (lambda x: "math" in x["intent"], math_chain),
    general_chain,  # default
)
```

---

# âš™ï¸ **7. FunciÃ³n principal: `run_router_chain()`**

```python
async def run_router_chain(question: str):
    intent = classifier_chain.invoke({"input": question}).strip().lower()
    block = await router_chain.ainvoke({"intent": intent, "input": question})
    return {
        "intent": intent,
        "chain_used": block["chain_used"],
        "answer": block["answer"].strip(),
    }
```

---

# ğŸ **8. Resultados**

Cada llamada devuelve un diccionario uniforme:

```json
{
  "intent": "summary",
  "chain_used": "summary_chain",
  "answer": "Texto resumido..."
}
```

---

# ğŸ¯ **9. Cambios clave respecto a versiones previas**

* `rag_chain` ahora **LCEL**, no funciÃ³n async en rag.py
* `rag.py` solo conserva `retrieve_context()`
* Uso de `RunnablePassthrough` y `RunnableLambda` para un pipeline 100% declarativo
* Router con `RunnableBranch` profesional
* Formato de salida unificado en todas las chains

---

# âœ”ï¸ **10. CÃ³mo extender el sistema**

1. Crear prompt nuevo en `prompts.py`
2. Declarar la chain usando `| llm | parser | RunnableLambda`
3. AÃ±adir condiciÃ³n en `router_chain`


---
