[
    {
        "label": "AsyncOpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "projects.A1_chat_structured.router",
        "description": "projects.A1_chat_structured.router",
        "isExtraImport": true,
        "detail": "projects.A1_chat_structured.router",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "projects.A2_output_parser.router",
        "description": "projects.A2_output_parser.router",
        "isExtraImport": true,
        "detail": "projects.A2_output_parser.router",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "projects.A3_rag_basic.router",
        "description": "projects.A3_rag_basic.router",
        "isExtraImport": true,
        "detail": "projects.A3_rag_basic.router",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "projects.A3_rag_basic_v2.router",
        "description": "projects.A3_rag_basic_v2.router",
        "isExtraImport": true,
        "detail": "projects.A3_rag_basic_v2.router",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "llm",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "llm",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "llm",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "llm",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "date",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "chromadb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "chromadb",
        "description": "chromadb",
        "detail": "chromadb",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "OPENROUTER_API_KEY",
        "kind": 5,
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "peekOfCode": "OPENROUTER_API_KEY = get_env(\"OPENROUTER_API_KEY\")\nOPENROUTER_BASE_URL = get_env(\"OPENROUTER_BASE_URL\", \"https://openrouter.ai/api/v1\")\nOPENROUTER_DEFAULT_MODEL = get_env(\"DEFAULT_MODEL\", \"meta-llama/llama-3.3-8b-instruct:free\")\n# Cliente compatible con OpenAI, apuntado a OpenRouter\nclient = AsyncOpenAI(\n    api_key=OPENROUTER_API_KEY,\n    base_url=OPENROUTER_BASE_URL,\n)\nasync def llm(prompt: str):\n    response = await client.chat.completions.create(",
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "OPENROUTER_BASE_URL",
        "kind": 5,
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "peekOfCode": "OPENROUTER_BASE_URL = get_env(\"OPENROUTER_BASE_URL\", \"https://openrouter.ai/api/v1\")\nOPENROUTER_DEFAULT_MODEL = get_env(\"DEFAULT_MODEL\", \"meta-llama/llama-3.3-8b-instruct:free\")\n# Cliente compatible con OpenAI, apuntado a OpenRouter\nclient = AsyncOpenAI(\n    api_key=OPENROUTER_API_KEY,\n    base_url=OPENROUTER_BASE_URL,\n)\nasync def llm(prompt: str):\n    response = await client.chat.completions.create(\n        model=OPENROUTER_DEFAULT_MODEL,",
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "OPENROUTER_DEFAULT_MODEL",
        "kind": 5,
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "peekOfCode": "OPENROUTER_DEFAULT_MODEL = get_env(\"DEFAULT_MODEL\", \"meta-llama/llama-3.3-8b-instruct:free\")\n# Cliente compatible con OpenAI, apuntado a OpenRouter\nclient = AsyncOpenAI(\n    api_key=OPENROUTER_API_KEY,\n    base_url=OPENROUTER_BASE_URL,\n)\nasync def llm(prompt: str):\n    response = await client.chat.completions.create(\n        model=OPENROUTER_DEFAULT_MODEL,\n        messages=[{\"role\": \"user\", \"content\": prompt}],",
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "peekOfCode": "client = AsyncOpenAI(\n    api_key=OPENROUTER_API_KEY,\n    base_url=OPENROUTER_BASE_URL,\n)\nasync def llm(prompt: str):\n    response = await client.chat.completions.create(\n        model=OPENROUTER_DEFAULT_MODEL,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=400,\n        temperature=0.7,",
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "get_env",
        "kind": 2,
        "importPath": "app.services.utils",
        "description": "app.services.utils",
        "peekOfCode": "def get_env(name: str, default=None):\n    value = os.getenv(name, default)\n    if value is None:\n        raise ValueError(f\"❌ Variable de entorno no encontrada: {name}\")\n    return value",
        "detail": "app.services.utils",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "app = FastAPI(title=\"Mini Projects LangChain - Base Server\")\napp.include_router(router)",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "health",
        "kind": 2,
        "importPath": "app.routes",
        "description": "app.routes",
        "peekOfCode": "def health():\n    return {\"status\": \"ok\"}\n@router.get(\"/test-llm\")\nasync def test_llm():\n    answer = await llm(\"Dime una frase corta divertida como un astronauta para confirmar conexión.\")\n    return {\"response\": answer}\n# Rutas de los mini-proyectos\nrouter.include_router(a1_router)\nrouter.include_router(a2_router)\nrouter.include_router(a3_router)",
        "detail": "app.routes",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "app.routes",
        "description": "app.routes",
        "peekOfCode": "router = APIRouter()\n@router.get(\"/health\")\ndef health():\n    return {\"status\": \"ok\"}\n@router.get(\"/test-llm\")\nasync def test_llm():\n    answer = await llm(\"Dime una frase corta divertida como un astronauta para confirmar conexión.\")\n    return {\"response\": answer}\n# Rutas de los mini-proyectos\nrouter.include_router(a1_router)",
        "detail": "app.routes",
        "documentation": {}
    },
    {
        "label": "chat_template",
        "kind": 5,
        "importPath": "projects.A1_chat_structured.prompts",
        "description": "projects.A1_chat_structured.prompts",
        "peekOfCode": "chat_template = PromptTemplate(\n    input_variables=[\"user_message\"],\n    template=\"\"\"\nEres un asistente útil y preciso. Debes responder SIEMPRE en formato JSON válido.\nResponde al usuario manteniendo un tono educativo y claro.\nInstrucciones estrictas:\n- No agregues texto fuera del JSON.\n- No expliques ni describas el JSON, solo devuélvelo.\n- No incluyas comentarios.\nFormato esperado:",
        "detail": "projects.A1_chat_structured.prompts",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "projects.A1_chat_structured.router",
        "description": "projects.A1_chat_structured.router",
        "peekOfCode": "router = APIRouter(prefix=\"/a1\", tags=[\"A1 - Chat estructurado\"])\n@router.post(\n    \"/chat\",\n    summary=\"Chat con respuesta estructurada en JSON\",\n    description=\"\"\"\n    Recibe un mensaje de usuario, lo envía a un modelo de lenguaje y devuelve una respuesta en formato JSON válido.\n    El JSON incluye la respuesta del asistente, el tono y metadatos sobre el modelo utilizado.\n    \"\"\",\n    response_description=\"JSON estructurado con la respuesta del asistente\",\n    response_model=ChatResponse",
        "detail": "projects.A1_chat_structured.router",
        "documentation": {}
    },
    {
        "label": "ChatRequest",
        "kind": 6,
        "importPath": "projects.A1_chat_structured.schemas",
        "description": "projects.A1_chat_structured.schemas",
        "peekOfCode": "class ChatRequest(BaseModel):\n    message: str = Field(..., example=\"Define que es un chatbot estructurado\")\nclass ResponseMetadata(BaseModel):\n    model: str = Field(..., description=\"Nombre del modelo usado para generar la respuesta\")\nclass ChatResponse(BaseModel):\n    answer: str = Field(..., description=\"Texto final que entregamos al usuario\")\n    tone: str = Field(..., description=\"Tono de la respuesta (por ejemplo: educational, friendly, formal, etc.)\")\n    metadata: ResponseMetadata = Field(..., description=\"Información adicional sobre la generación\")",
        "detail": "projects.A1_chat_structured.schemas",
        "documentation": {}
    },
    {
        "label": "ResponseMetadata",
        "kind": 6,
        "importPath": "projects.A1_chat_structured.schemas",
        "description": "projects.A1_chat_structured.schemas",
        "peekOfCode": "class ResponseMetadata(BaseModel):\n    model: str = Field(..., description=\"Nombre del modelo usado para generar la respuesta\")\nclass ChatResponse(BaseModel):\n    answer: str = Field(..., description=\"Texto final que entregamos al usuario\")\n    tone: str = Field(..., description=\"Tono de la respuesta (por ejemplo: educational, friendly, formal, etc.)\")\n    metadata: ResponseMetadata = Field(..., description=\"Información adicional sobre la generación\")",
        "detail": "projects.A1_chat_structured.schemas",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "kind": 6,
        "importPath": "projects.A1_chat_structured.schemas",
        "description": "projects.A1_chat_structured.schemas",
        "peekOfCode": "class ChatResponse(BaseModel):\n    answer: str = Field(..., description=\"Texto final que entregamos al usuario\")\n    tone: str = Field(..., description=\"Tono de la respuesta (por ejemplo: educational, friendly, formal, etc.)\")\n    metadata: ResponseMetadata = Field(..., description=\"Información adicional sobre la generación\")",
        "detail": "projects.A1_chat_structured.schemas",
        "documentation": {}
    },
    {
        "label": "today",
        "kind": 5,
        "importPath": "projects.A2_output_parser.prompts",
        "description": "projects.A2_output_parser.prompts",
        "peekOfCode": "today = date.today().strftime(\"%Y-%m-%d\")\nintent_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente que analiza mensajes de usuario y devuelve un JSON con intención estructurada.\nLa fecha actual es: {today}\nFormato esperado del JSON (usa exactamente este formato, sin texto fuera del JSON):\n{{\n  \"action\": \"create_task | update_task | get_status | other\",\n  \"title\": \"texto o null\",\n  \"due_date\": \"YYYY-MM-DD o null\"\n}}",
        "detail": "projects.A2_output_parser.prompts",
        "documentation": {}
    },
    {
        "label": "intent_prompt",
        "kind": 5,
        "importPath": "projects.A2_output_parser.prompts",
        "description": "projects.A2_output_parser.prompts",
        "peekOfCode": "intent_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente que analiza mensajes de usuario y devuelve un JSON con intención estructurada.\nLa fecha actual es: {today}\nFormato esperado del JSON (usa exactamente este formato, sin texto fuera del JSON):\n{{\n  \"action\": \"create_task | update_task | get_status | other\",\n  \"title\": \"texto o null\",\n  \"due_date\": \"YYYY-MM-DD o null\"\n}}\nInstrucciones:",
        "detail": "projects.A2_output_parser.prompts",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "projects.A2_output_parser.router",
        "description": "projects.A2_output_parser.router",
        "peekOfCode": "router = APIRouter(prefix=\"/a2\", tags=[\"A2 - Output Parser & Validación\"])\n@router.post(\n    \"/parse-intent\",\n    summary=\"Analiza el mensaje del usuario y devuelve la intención en un JSON validado\",\n    description=\"\"\"\n    Recibe un mensaje de usuario, lo envía a un modelo de lenguaje para analizar la intención y devuelve un JSON estructurado y validado con Pydantic.\n    Se puede usar para crear tareas, actualizar tareas o consultar el estado de tareas.\n    Si se indica una fecha relativa, se calcula en base a la fecha actual.\n    \"\"\",\n    response_description=\"JSON estructurado con la respuesta del asistente\",",
        "detail": "projects.A2_output_parser.router",
        "documentation": {}
    },
    {
        "label": "IntentRequest",
        "kind": 6,
        "importPath": "projects.A2_output_parser.schemas",
        "description": "projects.A2_output_parser.schemas",
        "peekOfCode": "class IntentRequest(BaseModel):\n    message: str = Field(..., example=\"Crea una tarea llamada Preparar informe para mañana\")\nclass IntentResponse(BaseModel):\n    action: str = Field(..., description=\"Tipo de acción que el usuario quiere realizar\")\n    title: str | None = Field(None, description=\"Título si aplica (por ejemplo crear tarea)\")\n    due_date: str | None = Field(None, description=\"Fecha en formato YYYY-MM-DD si aplica\")",
        "detail": "projects.A2_output_parser.schemas",
        "documentation": {}
    },
    {
        "label": "IntentResponse",
        "kind": 6,
        "importPath": "projects.A2_output_parser.schemas",
        "description": "projects.A2_output_parser.schemas",
        "peekOfCode": "class IntentResponse(BaseModel):\n    action: str = Field(..., description=\"Tipo de acción que el usuario quiere realizar\")\n    title: str | None = Field(None, description=\"Título si aplica (por ejemplo crear tarea)\")\n    due_date: str | None = Field(None, description=\"Fecha en formato YYYY-MM-DD si aplica\")",
        "detail": "projects.A2_output_parser.schemas",
        "documentation": {}
    },
    {
        "label": "load_documents",
        "kind": 2,
        "importPath": "projects.A3_rag_basic.loader",
        "description": "projects.A3_rag_basic.loader",
        "peekOfCode": "def load_documents(data_path: str):\n    docs = []\n    for file in os.listdir(data_path):\n        full_path = os.path.join(data_path, file)\n        if file.endswith(\".txt\"):\n            with open(full_path, \"r\", encoding=\"utf-8\") as f:\n                docs.append(f.read())\n    return docs",
        "detail": "projects.A3_rag_basic.loader",
        "documentation": {}
    },
    {
        "label": "rag_prompt",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.prompts",
        "description": "projects.A3_rag_basic.prompts",
        "peekOfCode": "rag_prompt = PromptTemplate.from_template(\"\"\"\nUtiliza únicamente la información proporcionada para responder al usuario.\nSi no encuentras la respuesta en los documentos, responde:\n\"No encuentro esa información en los documentos.\"\nDocumentos relevantes:\n{context}\nPregunta del usuario:\n{question}\n\"\"\")",
        "detail": "projects.A3_rag_basic.prompts",
        "documentation": {}
    },
    {
        "label": "build_index",
        "kind": 2,
        "importPath": "projects.A3_rag_basic.rag",
        "description": "projects.A3_rag_basic.rag",
        "peekOfCode": "def build_index(documents):\n    vectors = model.encode(documents).tolist() # Convierte los documentos a vectores\n    ids = [f\"doc_{i}\" for i in range(len(documents))] # Genera IDs únicas para cada documento\n    collection.add(documents=documents, embeddings=vectors, ids=ids) # Añade los documentos y sus vectores a la colección\ndef retrieve(question: str):\n    query_vec = model.encode([question]).tolist()[0] # Convierte la pregunta a vector\n    results = collection.query(query_embeddings=[query_vec], n_results=3) # Recupera los 3 documentos más similares\n    return results[\"documents\"][0] # Devuelve los documentos recuperados",
        "detail": "projects.A3_rag_basic.rag",
        "documentation": {}
    },
    {
        "label": "retrieve",
        "kind": 2,
        "importPath": "projects.A3_rag_basic.rag",
        "description": "projects.A3_rag_basic.rag",
        "peekOfCode": "def retrieve(question: str):\n    query_vec = model.encode([question]).tolist()[0] # Convierte la pregunta a vector\n    results = collection.query(query_embeddings=[query_vec], n_results=3) # Recupera los 3 documentos más similares\n    return results[\"documents\"][0] # Devuelve los documentos recuperados",
        "detail": "projects.A3_rag_basic.rag",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.rag",
        "description": "projects.A3_rag_basic.rag",
        "peekOfCode": "model = SentenceTransformer(\"all-MiniLM-L6-v2\") # Modelo ligero para embeddings\nclient = chromadb.PersistentClient(path=\"./chroma_db\") # Cliente persistente para almacenar datos\ncollection = client.get_or_create_collection(\"a3_docs\") # Colección para documentos\ndef build_index(documents):\n    vectors = model.encode(documents).tolist() # Convierte los documentos a vectores\n    ids = [f\"doc_{i}\" for i in range(len(documents))] # Genera IDs únicas para cada documento\n    collection.add(documents=documents, embeddings=vectors, ids=ids) # Añade los documentos y sus vectores a la colección\ndef retrieve(question: str):\n    query_vec = model.encode([question]).tolist()[0] # Convierte la pregunta a vector\n    results = collection.query(query_embeddings=[query_vec], n_results=3) # Recupera los 3 documentos más similares",
        "detail": "projects.A3_rag_basic.rag",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.rag",
        "description": "projects.A3_rag_basic.rag",
        "peekOfCode": "client = chromadb.PersistentClient(path=\"./chroma_db\") # Cliente persistente para almacenar datos\ncollection = client.get_or_create_collection(\"a3_docs\") # Colección para documentos\ndef build_index(documents):\n    vectors = model.encode(documents).tolist() # Convierte los documentos a vectores\n    ids = [f\"doc_{i}\" for i in range(len(documents))] # Genera IDs únicas para cada documento\n    collection.add(documents=documents, embeddings=vectors, ids=ids) # Añade los documentos y sus vectores a la colección\ndef retrieve(question: str):\n    query_vec = model.encode([question]).tolist()[0] # Convierte la pregunta a vector\n    results = collection.query(query_embeddings=[query_vec], n_results=3) # Recupera los 3 documentos más similares\n    return results[\"documents\"][0] # Devuelve los documentos recuperados",
        "detail": "projects.A3_rag_basic.rag",
        "documentation": {}
    },
    {
        "label": "collection",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.rag",
        "description": "projects.A3_rag_basic.rag",
        "peekOfCode": "collection = client.get_or_create_collection(\"a3_docs\") # Colección para documentos\ndef build_index(documents):\n    vectors = model.encode(documents).tolist() # Convierte los documentos a vectores\n    ids = [f\"doc_{i}\" for i in range(len(documents))] # Genera IDs únicas para cada documento\n    collection.add(documents=documents, embeddings=vectors, ids=ids) # Añade los documentos y sus vectores a la colección\ndef retrieve(question: str):\n    query_vec = model.encode([question]).tolist()[0] # Convierte la pregunta a vector\n    results = collection.query(query_embeddings=[query_vec], n_results=3) # Recupera los 3 documentos más similares\n    return results[\"documents\"][0] # Devuelve los documentos recuperados",
        "detail": "projects.A3_rag_basic.rag",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.router",
        "description": "projects.A3_rag_basic.router",
        "peekOfCode": "router = APIRouter(prefix=\"/a3\", tags=[\"A3 - RAG básico\"])\nDATA_PATH = \"projects/A3_rag_basic/data\"\n# Construye el índice on startup:\ndocuments = load_documents(DATA_PATH)\nbuild_index(documents)\n@router.post(\n    \"/ask\",\n    summary=\"Realiza una pregunta utilizando RAG básico\",\n    description=\"\"\"\n    Realiza una pregunta utilizando un enfoque de Recuperación Augmentada por Generación (RAG) básico.",
        "detail": "projects.A3_rag_basic.router",
        "documentation": {}
    },
    {
        "label": "DATA_PATH",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.router",
        "description": "projects.A3_rag_basic.router",
        "peekOfCode": "DATA_PATH = \"projects/A3_rag_basic/data\"\n# Construye el índice on startup:\ndocuments = load_documents(DATA_PATH)\nbuild_index(documents)\n@router.post(\n    \"/ask\",\n    summary=\"Realiza una pregunta utilizando RAG básico\",\n    description=\"\"\"\n    Realiza una pregunta utilizando un enfoque de Recuperación Augmentada por Generación (RAG) básico.\n    1. Recupera documentos relevantes basados en la pregunta del usuario.",
        "detail": "projects.A3_rag_basic.router",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.router",
        "description": "projects.A3_rag_basic.router",
        "peekOfCode": "documents = load_documents(DATA_PATH)\nbuild_index(documents)\n@router.post(\n    \"/ask\",\n    summary=\"Realiza una pregunta utilizando RAG básico\",\n    description=\"\"\"\n    Realiza una pregunta utilizando un enfoque de Recuperación Augmentada por Generación (RAG) básico.\n    1. Recupera documentos relevantes basados en la pregunta del usuario.\n    2. Usa un modelo de lenguaje para generar una respuesta basada en esos documentos.\n    \"\"\",",
        "detail": "projects.A3_rag_basic.router",
        "documentation": {}
    },
    {
        "label": "QueryRequest",
        "kind": 6,
        "importPath": "projects.A3_rag_basic.schemas",
        "description": "projects.A3_rag_basic.schemas",
        "peekOfCode": "class QueryRequest(BaseModel):\n    question: str = Field(..., example=\"Qué es la inteligencia artificial?\")\nclass QueryResponse(BaseModel):\n    response: str = Field(..., description=\"Respuesta generada por el modelo LLM\")\n    sources: list[str] = Field(..., description=\"Documentos utilizados para generar la respuesta\")",
        "detail": "projects.A3_rag_basic.schemas",
        "documentation": {}
    },
    {
        "label": "QueryResponse",
        "kind": 6,
        "importPath": "projects.A3_rag_basic.schemas",
        "description": "projects.A3_rag_basic.schemas",
        "peekOfCode": "class QueryResponse(BaseModel):\n    response: str = Field(..., description=\"Respuesta generada por el modelo LLM\")\n    sources: list[str] = Field(..., description=\"Documentos utilizados para generar la respuesta\")",
        "detail": "projects.A3_rag_basic.schemas",
        "documentation": {}
    },
    {
        "label": "rag_prompt",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.prompts",
        "description": "projects.A3_rag_basic_v2.prompts",
        "peekOfCode": "rag_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente que responde usando la información de contexto proporcionada.\nUsa **solo** la información del contexto si es relevante.\nSi no hay suficiente información, indica que no puedes responder con precisión.\n### Contexto:\n{context}\n### Pregunta:\n{question}\nDevuelve un JSON con este formato:\n{{",
        "detail": "projects.A3_rag_basic_v2.prompts",
        "documentation": {}
    },
    {
        "label": "build_index_from_folder",
        "kind": 2,
        "importPath": "projects.A3_rag_basic_v2.rag",
        "description": "projects.A3_rag_basic_v2.rag",
        "peekOfCode": "def build_index_from_folder(folder_path: str):\n    docs = []\n    ids = []\n    for fname in os.listdir(folder_path):\n        if not fname.endswith((\".txt\", \".md\")):\n            continue\n        with open(os.path.join(folder_path, fname), \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n        # Fragmentar en chunks de 400 caracteres\n        chunks = [content[i:i+400] for i in range(0, len(content), 400)]",
        "detail": "projects.A3_rag_basic_v2.rag",
        "documentation": {}
    },
    {
        "label": "retrieve",
        "kind": 2,
        "importPath": "projects.A3_rag_basic_v2.rag",
        "description": "projects.A3_rag_basic_v2.rag",
        "peekOfCode": "def retrieve(question: str, top_k: int = 3):\n    query_vec = model.encode([question]).tolist()[0]\n    results = collection.query(query_embeddings=[query_vec], n_results=top_k)\n    documents = results.get(\"documents\", [[]])[0]\n    return documents",
        "detail": "projects.A3_rag_basic_v2.rag",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.rag",
        "description": "projects.A3_rag_basic_v2.rag",
        "peekOfCode": "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n# Crear cliente persistente de ChromaDB\nclient = chromadb.PersistentClient(path=\"./chroma_db\")\ncollection = client.get_or_create_collection(\"a3_docs_v2\")\n# Utilidad para hashear texto y generar IDs únicos\ndef _hash_text(text: str) -> str:\n    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n# Construye el índice desde una carpeta de documentos\ndef build_index_from_folder(folder_path: str):\n    docs = []",
        "detail": "projects.A3_rag_basic_v2.rag",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.rag",
        "description": "projects.A3_rag_basic_v2.rag",
        "peekOfCode": "client = chromadb.PersistentClient(path=\"./chroma_db\")\ncollection = client.get_or_create_collection(\"a3_docs_v2\")\n# Utilidad para hashear texto y generar IDs únicos\ndef _hash_text(text: str) -> str:\n    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n# Construye el índice desde una carpeta de documentos\ndef build_index_from_folder(folder_path: str):\n    docs = []\n    ids = []\n    for fname in os.listdir(folder_path):",
        "detail": "projects.A3_rag_basic_v2.rag",
        "documentation": {}
    },
    {
        "label": "collection",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.rag",
        "description": "projects.A3_rag_basic_v2.rag",
        "peekOfCode": "collection = client.get_or_create_collection(\"a3_docs_v2\")\n# Utilidad para hashear texto y generar IDs únicos\ndef _hash_text(text: str) -> str:\n    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n# Construye el índice desde una carpeta de documentos\ndef build_index_from_folder(folder_path: str):\n    docs = []\n    ids = []\n    for fname in os.listdir(folder_path):\n        if not fname.endswith((\".txt\", \".md\")):",
        "detail": "projects.A3_rag_basic_v2.rag",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.router",
        "description": "projects.A3_rag_basic_v2.router",
        "peekOfCode": "router = APIRouter(prefix=\"/a3v2\", tags=[\"A3 - RAG Básico v2\"])\nDATA_PATH = \"projects/A3_rag_basic_v2/data\"\n# Indexado automático en segundo plano al cargar el módulo \ndef _auto_build_index():\n    try:\n        build_index_from_folder(DATA_PATH)\n    except Exception as e:\n        print(f\"[RAG] Error durante el indexado automático: {e}\")\n# Lanzamos el indexado en un hilo para no bloquear FastAPI al arrancar\nthreading.Thread(target=_auto_build_index, daemon=True).start()",
        "detail": "projects.A3_rag_basic_v2.router",
        "documentation": {}
    },
    {
        "label": "DATA_PATH",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.router",
        "description": "projects.A3_rag_basic_v2.router",
        "peekOfCode": "DATA_PATH = \"projects/A3_rag_basic_v2/data\"\n# Indexado automático en segundo plano al cargar el módulo \ndef _auto_build_index():\n    try:\n        build_index_from_folder(DATA_PATH)\n    except Exception as e:\n        print(f\"[RAG] Error durante el indexado automático: {e}\")\n# Lanzamos el indexado en un hilo para no bloquear FastAPI al arrancar\nthreading.Thread(target=_auto_build_index, daemon=True).start()\n@router.post(",
        "detail": "projects.A3_rag_basic_v2.router",
        "documentation": {}
    },
    {
        "label": "QueryRequest",
        "kind": 6,
        "importPath": "projects.A3_rag_basic_v2.schemas",
        "description": "projects.A3_rag_basic_v2.schemas",
        "peekOfCode": "class QueryRequest(BaseModel):\n    question: str = Field(..., example=\"Qué es LangChain?\")\nclass QueryResponse(BaseModel):\n    answer: str = Field(..., description=\"Respuesta generada por el modelo LLM\")\n    sources: list[str] = Field(..., description=\"Documentos utilizados para generar la respuesta\")",
        "detail": "projects.A3_rag_basic_v2.schemas",
        "documentation": {}
    },
    {
        "label": "QueryResponse",
        "kind": 6,
        "importPath": "projects.A3_rag_basic_v2.schemas",
        "description": "projects.A3_rag_basic_v2.schemas",
        "peekOfCode": "class QueryResponse(BaseModel):\n    answer: str = Field(..., description=\"Respuesta generada por el modelo LLM\")\n    sources: list[str] = Field(..., description=\"Documentos utilizados para generar la respuesta\")",
        "detail": "projects.A3_rag_basic_v2.schemas",
        "documentation": {}
    },
    {
        "label": "safe_json_parse",
        "kind": 2,
        "importPath": "projects.A3_rag_basic_v2.utils",
        "description": "projects.A3_rag_basic_v2.utils",
        "peekOfCode": "def safe_json_parse(text: str):\n    try:\n        data = json.loads(text)\n    except json.JSONDecodeError:\n        return {\"answer\": text.strip(), \"sources\": []}\n    # Validamos estructura con Pydantic\n    try:\n        parsed = QueryResponse(**data)\n    except Exception as e:\n        return {",
        "detail": "projects.A3_rag_basic_v2.utils",
        "documentation": {}
    }
]