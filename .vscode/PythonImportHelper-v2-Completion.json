[
    {
        "label": "AsyncOpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "DEFAULT_LLM_MODEL",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "FALLBACK_LLM_MODEL",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EMBEDDING_MODEL",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EMBEDDING_MODEL",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EMBEDDING_MODEL",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EMBEDDING_MODEL",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EMBEDDING_MODEL",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "projects.A1_chat_structured.router",
        "description": "projects.A1_chat_structured.router",
        "isExtraImport": true,
        "detail": "projects.A1_chat_structured.router",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "projects.A2_output_parser.router",
        "description": "projects.A2_output_parser.router",
        "isExtraImport": true,
        "detail": "projects.A2_output_parser.router",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "projects.A3_rag_basic.router",
        "description": "projects.A3_rag_basic.router",
        "isExtraImport": true,
        "detail": "projects.A3_rag_basic.router",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "projects.A3_rag_basic_v2.router",
        "description": "projects.A3_rag_basic_v2.router",
        "isExtraImport": true,
        "detail": "projects.A3_rag_basic_v2.router",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "projects.A4_rag_advanced.router",
        "description": "projects.A4_rag_advanced.router",
        "isExtraImport": true,
        "detail": "projects.A4_rag_advanced.router",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "projects.A4_rag_advanced_v2.router",
        "description": "projects.A4_rag_advanced_v2.router",
        "isExtraImport": true,
        "detail": "projects.A4_rag_advanced_v2.router",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "projects.A5_chains_routers.router",
        "description": "projects.A5_chains_routers.router",
        "isExtraImport": true,
        "detail": "projects.A5_chains_routers.router",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "projects.A6_memory.router",
        "description": "projects.A6_memory.router",
        "isExtraImport": true,
        "detail": "projects.A6_memory.router",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "llm",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "llm",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "llm",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "llm",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "llm",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "llm",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "llm",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "llm",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "date",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "chromadb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "chromadb",
        "description": "chromadb",
        "detail": "chromadb",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "RunnableBranch",
        "importPath": "langchain_core.runnables",
        "description": "langchain_core.runnables",
        "isExtraImport": true,
        "detail": "langchain_core.runnables",
        "documentation": {}
    },
    {
        "label": "RunnableLambda",
        "importPath": "langchain_core.runnables",
        "description": "langchain_core.runnables",
        "isExtraImport": true,
        "detail": "langchain_core.runnables",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain_core.runnables",
        "description": "langchain_core.runnables",
        "isExtraImport": true,
        "detail": "langchain_core.runnables",
        "documentation": {}
    },
    {
        "label": "collection",
        "importPath": "projects.A4_rag_advanced_v2.chroma_client",
        "description": "projects.A4_rag_advanced_v2.chroma_client",
        "isExtraImport": true,
        "detail": "projects.A4_rag_advanced_v2.chroma_client",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "StateGraph",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "START",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "END",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "kind": 2,
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "peekOfCode": "def llm_chain(model: str | None = None, temperature: float = 0.0,) -> ChatOpenAI:\n    model_to_use = model or DEFAULT_LLM_MODEL\n    llm_params = {\n        \"api_key\": OPENROUTER_API_KEY,\n        \"base_url\": OPENROUTER_BASE_URL,\n        \"temperature\": temperature,\n    }\n    try:\n        return ChatOpenAI(model=model_to_use, **llm_params)\n    except Exception:",
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "OPENROUTER_API_KEY",
        "kind": 5,
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "peekOfCode": "OPENROUTER_API_KEY = get_env(\"OPENROUTER_API_KEY\")\nOPENROUTER_BASE_URL = get_env(\"OPENROUTER_BASE_URL\", \"https://openrouter.ai/api/v1\")\n# Cliente OpenRouter compatible con AsyncOpenAI\nclient = AsyncOpenAI(\n    api_key=OPENROUTER_API_KEY,\n    base_url=OPENROUTER_BASE_URL,\n)\n# ============================================================\n# 1) Cliente simple (para proyectos casuales o endpoints básicos)\n# ============================================================",
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "OPENROUTER_BASE_URL",
        "kind": 5,
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "peekOfCode": "OPENROUTER_BASE_URL = get_env(\"OPENROUTER_BASE_URL\", \"https://openrouter.ai/api/v1\")\n# Cliente OpenRouter compatible con AsyncOpenAI\nclient = AsyncOpenAI(\n    api_key=OPENROUTER_API_KEY,\n    base_url=OPENROUTER_BASE_URL,\n)\n# ============================================================\n# 1) Cliente simple (para proyectos casuales o endpoints básicos)\n# ============================================================\n# Cliente minimalista para prompts directos sin LangChain. Devuelve solo texto. Ideal para endpoints simples.",
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "peekOfCode": "client = AsyncOpenAI(\n    api_key=OPENROUTER_API_KEY,\n    base_url=OPENROUTER_BASE_URL,\n)\n# ============================================================\n# 1) Cliente simple (para proyectos casuales o endpoints básicos)\n# ============================================================\n# Cliente minimalista para prompts directos sin LangChain. Devuelve solo texto. Ideal para endpoints simples.\nasync def llm(prompt: str, model: str | None = None) -> str:\n    model_to_use = model or DEFAULT_LLM_MODEL",
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "get_env",
        "kind": 2,
        "importPath": "app.services.utils",
        "description": "app.services.utils",
        "peekOfCode": "def get_env(name: str, default=None):\n    value = os.getenv(name, default)\n    if value is None:\n        raise ValueError(f\"❌ Variable de entorno no encontrada: {name}\")\n    return value",
        "detail": "app.services.utils",
        "documentation": {}
    },
    {
        "label": "ENV",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "ENV = get_env(\"ENV\", \"dev\")  # dev | prod\ndocs_url = \"/docs\" if ENV == \"dev\" else None\nredoc_url = \"/redoc\" if ENV == \"dev\" else None\nopenapi_url = \"/openapi.json\" if ENV == \"dev\" else None\napp = FastAPI(\n    title=\"LangChain Lab - AI Server\",\n    description=\"\"\"\n    Servidor de experimentación con modelos de IA, RAG y agentes.\n    Este backend expone APIs para explorar:\n    - Recuperación aumentada con generación (RAG)",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "docs_url",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "docs_url = \"/docs\" if ENV == \"dev\" else None\nredoc_url = \"/redoc\" if ENV == \"dev\" else None\nopenapi_url = \"/openapi.json\" if ENV == \"dev\" else None\napp = FastAPI(\n    title=\"LangChain Lab - AI Server\",\n    description=\"\"\"\n    Servidor de experimentación con modelos de IA, RAG y agentes.\n    Este backend expone APIs para explorar:\n    - Recuperación aumentada con generación (RAG)\n    - Llamadas a modelos LLM",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "redoc_url",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "redoc_url = \"/redoc\" if ENV == \"dev\" else None\nopenapi_url = \"/openapi.json\" if ENV == \"dev\" else None\napp = FastAPI(\n    title=\"LangChain Lab - AI Server\",\n    description=\"\"\"\n    Servidor de experimentación con modelos de IA, RAG y agentes.\n    Este backend expone APIs para explorar:\n    - Recuperación aumentada con generación (RAG)\n    - Llamadas a modelos LLM\n    - Herramientas generativas",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "openapi_url",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "openapi_url = \"/openapi.json\" if ENV == \"dev\" else None\napp = FastAPI(\n    title=\"LangChain Lab - AI Server\",\n    description=\"\"\"\n    Servidor de experimentación con modelos de IA, RAG y agentes.\n    Este backend expone APIs para explorar:\n    - Recuperación aumentada con generación (RAG)\n    - Llamadas a modelos LLM\n    - Herramientas generativas\n    - Proyectos modulares de IA",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "app = FastAPI(\n    title=\"LangChain Lab - AI Server\",\n    description=\"\"\"\n    Servidor de experimentación con modelos de IA, RAG y agentes.\n    Este backend expone APIs para explorar:\n    - Recuperación aumentada con generación (RAG)\n    - Llamadas a modelos LLM\n    - Herramientas generativas\n    - Proyectos modulares de IA\n    \"\"\",",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "health",
        "kind": 2,
        "importPath": "app.routes",
        "description": "app.routes",
        "peekOfCode": "def health():\n    return {\"status\": \"ok\"}\n@router.get(\"/test-llm\")\nasync def test_llm():\n    answer = await llm(\"Dime una frase corta divertida como un astronauta para confirmar conexión.\")\n    return {\"response\": answer}\n# Rutas de los mini-proyectos\nrouter.include_router(a1_router)\nrouter.include_router(a2_router)\nrouter.include_router(a3_router)",
        "detail": "app.routes",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "app.routes",
        "description": "app.routes",
        "peekOfCode": "router = APIRouter()\n@router.get(\"/health\")\ndef health():\n    return {\"status\": \"ok\"}\n@router.get(\"/test-llm\")\nasync def test_llm():\n    answer = await llm(\"Dime una frase corta divertida como un astronauta para confirmar conexión.\")\n    return {\"response\": answer}\n# Rutas de los mini-proyectos\nrouter.include_router(a1_router)",
        "detail": "app.routes",
        "documentation": {}
    },
    {
        "label": "chat_template",
        "kind": 5,
        "importPath": "projects.A1_chat_structured.prompts",
        "description": "projects.A1_chat_structured.prompts",
        "peekOfCode": "chat_template = PromptTemplate(\n    input_variables=[\"user_message\"],\n    template=\"\"\"\nEres un asistente útil y preciso. Debes responder SIEMPRE en formato JSON válido.\nResponde al usuario manteniendo un tono educativo y claro.\nInstrucciones estrictas:\n- No agregues texto fuera del JSON.\n- No expliques ni describas el JSON, solo devuélvelo.\n- No incluyas comentarios.\nFormato esperado:",
        "detail": "projects.A1_chat_structured.prompts",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "projects.A1_chat_structured.router",
        "description": "projects.A1_chat_structured.router",
        "peekOfCode": "router = APIRouter(prefix=\"/a1\", tags=[\"A1 - Chat estructurado\"])\n@router.post(\n    \"/chat\",\n    summary=\"Chat con respuesta estructurada en JSON\",\n    description=\"\"\"\n    Recibe un mensaje de usuario, lo envía a un modelo de lenguaje y devuelve una respuesta en formato JSON válido.\n    El JSON incluye la respuesta del asistente, el tono y metadatos sobre el modelo utilizado.\n    \"\"\",\n    response_description=\"JSON estructurado con la respuesta del asistente\",\n    response_model=ChatResponse",
        "detail": "projects.A1_chat_structured.router",
        "documentation": {}
    },
    {
        "label": "ChatRequest",
        "kind": 6,
        "importPath": "projects.A1_chat_structured.schemas",
        "description": "projects.A1_chat_structured.schemas",
        "peekOfCode": "class ChatRequest(BaseModel):\n    message: str = Field(..., example=\"Define que es un chatbot estructurado\")\nclass ResponseMetadata(BaseModel):\n    model: str = Field(..., description=\"Nombre del modelo usado para generar la respuesta\")\nclass ChatResponse(BaseModel):\n    answer: str = Field(..., description=\"Texto final que entregamos al usuario\")\n    tone: str = Field(..., description=\"Tono de la respuesta (por ejemplo: educational, friendly, formal, etc.)\")\n    metadata: ResponseMetadata = Field(..., description=\"Información adicional sobre la generación\")",
        "detail": "projects.A1_chat_structured.schemas",
        "documentation": {}
    },
    {
        "label": "ResponseMetadata",
        "kind": 6,
        "importPath": "projects.A1_chat_structured.schemas",
        "description": "projects.A1_chat_structured.schemas",
        "peekOfCode": "class ResponseMetadata(BaseModel):\n    model: str = Field(..., description=\"Nombre del modelo usado para generar la respuesta\")\nclass ChatResponse(BaseModel):\n    answer: str = Field(..., description=\"Texto final que entregamos al usuario\")\n    tone: str = Field(..., description=\"Tono de la respuesta (por ejemplo: educational, friendly, formal, etc.)\")\n    metadata: ResponseMetadata = Field(..., description=\"Información adicional sobre la generación\")",
        "detail": "projects.A1_chat_structured.schemas",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "kind": 6,
        "importPath": "projects.A1_chat_structured.schemas",
        "description": "projects.A1_chat_structured.schemas",
        "peekOfCode": "class ChatResponse(BaseModel):\n    answer: str = Field(..., description=\"Texto final que entregamos al usuario\")\n    tone: str = Field(..., description=\"Tono de la respuesta (por ejemplo: educational, friendly, formal, etc.)\")\n    metadata: ResponseMetadata = Field(..., description=\"Información adicional sobre la generación\")",
        "detail": "projects.A1_chat_structured.schemas",
        "documentation": {}
    },
    {
        "label": "today",
        "kind": 5,
        "importPath": "projects.A2_output_parser.prompts",
        "description": "projects.A2_output_parser.prompts",
        "peekOfCode": "today = date.today().strftime(\"%Y-%m-%d\")\nintent_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente que analiza mensajes de usuario y devuelve un JSON con intención estructurada.\nLa fecha actual es: {today}\nFormato esperado del JSON (usa exactamente este formato, sin texto fuera del JSON):\n{{\n  \"action\": \"create_task | update_task | get_status | other\",\n  \"title\": \"texto o null\",\n  \"due_date\": \"YYYY-MM-DD o null\"\n}}",
        "detail": "projects.A2_output_parser.prompts",
        "documentation": {}
    },
    {
        "label": "intent_prompt",
        "kind": 5,
        "importPath": "projects.A2_output_parser.prompts",
        "description": "projects.A2_output_parser.prompts",
        "peekOfCode": "intent_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente que analiza mensajes de usuario y devuelve un JSON con intención estructurada.\nLa fecha actual es: {today}\nFormato esperado del JSON (usa exactamente este formato, sin texto fuera del JSON):\n{{\n  \"action\": \"create_task | update_task | get_status | other\",\n  \"title\": \"texto o null\",\n  \"due_date\": \"YYYY-MM-DD o null\"\n}}\nInstrucciones:",
        "detail": "projects.A2_output_parser.prompts",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "projects.A2_output_parser.router",
        "description": "projects.A2_output_parser.router",
        "peekOfCode": "router = APIRouter(prefix=\"/a2\", tags=[\"A2 - Output Parser & Validación\"])\n@router.post(\n    \"/parse-intent\",\n    summary=\"Analiza el mensaje del usuario y devuelve la intención en un JSON validado\",\n    description=\"\"\"\n    Recibe un mensaje de usuario, lo envía a un modelo de lenguaje para analizar la intención y devuelve un JSON estructurado y validado con Pydantic.\n    Se puede usar para crear tareas, actualizar tareas o consultar el estado de tareas.\n    Si se indica una fecha relativa, se calcula en base a la fecha actual.\n    \"\"\",\n    response_description=\"JSON estructurado con la respuesta del asistente\",",
        "detail": "projects.A2_output_parser.router",
        "documentation": {}
    },
    {
        "label": "IntentRequest",
        "kind": 6,
        "importPath": "projects.A2_output_parser.schemas",
        "description": "projects.A2_output_parser.schemas",
        "peekOfCode": "class IntentRequest(BaseModel):\n    message: str = Field(..., example=\"Crea una tarea llamada Preparar informe para mañana\")\nclass IntentResponse(BaseModel):\n    action: str = Field(..., description=\"Tipo de acción que el usuario quiere realizar\")\n    title: str | None = Field(None, description=\"Título si aplica (por ejemplo crear tarea)\")\n    due_date: str | None = Field(None, description=\"Fecha en formato YYYY-MM-DD si aplica\")",
        "detail": "projects.A2_output_parser.schemas",
        "documentation": {}
    },
    {
        "label": "IntentResponse",
        "kind": 6,
        "importPath": "projects.A2_output_parser.schemas",
        "description": "projects.A2_output_parser.schemas",
        "peekOfCode": "class IntentResponse(BaseModel):\n    action: str = Field(..., description=\"Tipo de acción que el usuario quiere realizar\")\n    title: str | None = Field(None, description=\"Título si aplica (por ejemplo crear tarea)\")\n    due_date: str | None = Field(None, description=\"Fecha en formato YYYY-MM-DD si aplica\")",
        "detail": "projects.A2_output_parser.schemas",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.chroma_client",
        "description": "projects.A3_rag_basic.chroma_client",
        "peekOfCode": "client = chromadb.PersistentClient(path=CHROMA_PATH)\ncollection = client.get_or_create_collection(COLLECTION_NAME)",
        "detail": "projects.A3_rag_basic.chroma_client",
        "documentation": {}
    },
    {
        "label": "collection",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.chroma_client",
        "description": "projects.A3_rag_basic.chroma_client",
        "peekOfCode": "collection = client.get_or_create_collection(COLLECTION_NAME)",
        "detail": "projects.A3_rag_basic.chroma_client",
        "documentation": {}
    },
    {
        "label": "COLLECTION_NAME",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.config",
        "description": "projects.A3_rag_basic.config",
        "peekOfCode": "COLLECTION_NAME = \"a3_docs\"\nEMBEDDING_MODEL = DEFAULT_EMBEDDING_MODEL\nCHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A3_rag_basic.config",
        "documentation": {}
    },
    {
        "label": "EMBEDDING_MODEL",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.config",
        "description": "projects.A3_rag_basic.config",
        "peekOfCode": "EMBEDDING_MODEL = DEFAULT_EMBEDDING_MODEL\nCHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A3_rag_basic.config",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.config",
        "description": "projects.A3_rag_basic.config",
        "peekOfCode": "CHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A3_rag_basic.config",
        "documentation": {}
    },
    {
        "label": "load_documents",
        "kind": 2,
        "importPath": "projects.A3_rag_basic.loader",
        "description": "projects.A3_rag_basic.loader",
        "peekOfCode": "def load_documents(data_path: str):\n    docs = []\n    for file in os.listdir(data_path):\n        full_path = os.path.join(data_path, file)\n        if file.endswith(\".txt\"):\n            with open(full_path, \"r\", encoding=\"utf-8\") as f:\n                docs.append(f.read())\n    return docs",
        "detail": "projects.A3_rag_basic.loader",
        "documentation": {}
    },
    {
        "label": "rag_prompt",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.prompts",
        "description": "projects.A3_rag_basic.prompts",
        "peekOfCode": "rag_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente especializado en RAG. Debes responder **únicamente** con la información que aparezca en el siguiente contexto. \nNo inventes datos, no agregues conocimiento externo y no uses información general que no esté contenida explícitamente en el contexto.\nSi el contexto no contiene información suficiente para responder la pregunta, responde exactamente:\n\"Sin suficiente información en la documentación para responder.\"\nProduce una respuesta:\n- Clara, concisa y directa.\n- Basada solo en detalles presentes en el contexto.\n- Sintetizando y explicando, no copiando el contexto textual sin procesarlo.\n- Sin añadir interpretaciones no justificadas por el contenido.",
        "detail": "projects.A3_rag_basic.prompts",
        "documentation": {}
    },
    {
        "label": "build_index",
        "kind": 2,
        "importPath": "projects.A3_rag_basic.rag",
        "description": "projects.A3_rag_basic.rag",
        "peekOfCode": "def build_index(documents):\n    vectors = model.encode(documents).tolist() # Convierte los documentos a vectores\n    ids = [f\"doc_{i}\" for i in range(len(documents))] # Genera IDs únicas para cada documento\n    collection.add(documents=documents, embeddings=vectors, ids=ids) # Añade los documentos y sus vectores a la colección\ndef retrieve(question: str):\n    query_vec = model.encode([question]).tolist()[0] # Convierte la pregunta a vector\n    results = collection.query(query_embeddings=[query_vec], n_results=3) # Recupera los 3 documentos más similares\n    return results[\"documents\"][0] # Devuelve los documentos recuperados",
        "detail": "projects.A3_rag_basic.rag",
        "documentation": {}
    },
    {
        "label": "retrieve",
        "kind": 2,
        "importPath": "projects.A3_rag_basic.rag",
        "description": "projects.A3_rag_basic.rag",
        "peekOfCode": "def retrieve(question: str):\n    query_vec = model.encode([question]).tolist()[0] # Convierte la pregunta a vector\n    results = collection.query(query_embeddings=[query_vec], n_results=3) # Recupera los 3 documentos más similares\n    return results[\"documents\"][0] # Devuelve los documentos recuperados",
        "detail": "projects.A3_rag_basic.rag",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.rag",
        "description": "projects.A3_rag_basic.rag",
        "peekOfCode": "model = SentenceTransformer(EMBEDDING_MODEL) # Modelo ligero para embeddings\ndef build_index(documents):\n    vectors = model.encode(documents).tolist() # Convierte los documentos a vectores\n    ids = [f\"doc_{i}\" for i in range(len(documents))] # Genera IDs únicas para cada documento\n    collection.add(documents=documents, embeddings=vectors, ids=ids) # Añade los documentos y sus vectores a la colección\ndef retrieve(question: str):\n    query_vec = model.encode([question]).tolist()[0] # Convierte la pregunta a vector\n    results = collection.query(query_embeddings=[query_vec], n_results=3) # Recupera los 3 documentos más similares\n    return results[\"documents\"][0] # Devuelve los documentos recuperados",
        "detail": "projects.A3_rag_basic.rag",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.router",
        "description": "projects.A3_rag_basic.router",
        "peekOfCode": "router = APIRouter(prefix=\"/a3\", tags=[\"A3 - RAG Básico\"])\nDATA_PATH = os.path.join(os.path.dirname(__file__), \"data\")\n# Construye el índice on startup:\ndocuments = load_documents(DATA_PATH)\nbuild_index(documents)\n@router.post(\n    \"/ask\",\n    summary=\"Realiza una pregunta utilizando RAG básico\",\n    description=\"\"\"\n    Realiza una pregunta utilizando un enfoque de Recuperación Augmentada por Generación (RAG) básico.",
        "detail": "projects.A3_rag_basic.router",
        "documentation": {}
    },
    {
        "label": "DATA_PATH",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.router",
        "description": "projects.A3_rag_basic.router",
        "peekOfCode": "DATA_PATH = os.path.join(os.path.dirname(__file__), \"data\")\n# Construye el índice on startup:\ndocuments = load_documents(DATA_PATH)\nbuild_index(documents)\n@router.post(\n    \"/ask\",\n    summary=\"Realiza una pregunta utilizando RAG básico\",\n    description=\"\"\"\n    Realiza una pregunta utilizando un enfoque de Recuperación Augmentada por Generación (RAG) básico.\n    1. Recupera documentos relevantes basados en la pregunta del usuario.",
        "detail": "projects.A3_rag_basic.router",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.router",
        "description": "projects.A3_rag_basic.router",
        "peekOfCode": "documents = load_documents(DATA_PATH)\nbuild_index(documents)\n@router.post(\n    \"/ask\",\n    summary=\"Realiza una pregunta utilizando RAG básico\",\n    description=\"\"\"\n    Realiza una pregunta utilizando un enfoque de Recuperación Augmentada por Generación (RAG) básico.\n    1. Recupera documentos relevantes basados en la pregunta del usuario.\n    2. Usa un modelo de lenguaje para generar una respuesta basada en esos documentos.\n    \"\"\",",
        "detail": "projects.A3_rag_basic.router",
        "documentation": {}
    },
    {
        "label": "QueryRequest",
        "kind": 6,
        "importPath": "projects.A3_rag_basic.schemas",
        "description": "projects.A3_rag_basic.schemas",
        "peekOfCode": "class QueryRequest(BaseModel):\n    question: str = Field(..., example=\"Qué es la inteligencia artificial?\")\nclass QueryResponse(BaseModel):\n    response: str = Field(..., description=\"Respuesta generada por el modelo LLM\")\n    sources: list[str] = Field(..., description=\"Documentos utilizados para generar la respuesta\")",
        "detail": "projects.A3_rag_basic.schemas",
        "documentation": {}
    },
    {
        "label": "QueryResponse",
        "kind": 6,
        "importPath": "projects.A3_rag_basic.schemas",
        "description": "projects.A3_rag_basic.schemas",
        "peekOfCode": "class QueryResponse(BaseModel):\n    response: str = Field(..., description=\"Respuesta generada por el modelo LLM\")\n    sources: list[str] = Field(..., description=\"Documentos utilizados para generar la respuesta\")",
        "detail": "projects.A3_rag_basic.schemas",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.chroma_client",
        "description": "projects.A3_rag_basic_v2.chroma_client",
        "peekOfCode": "client = chromadb.PersistentClient(path=CHROMA_PATH)\ncollection = client.get_or_create_collection(COLLECTION_NAME)",
        "detail": "projects.A3_rag_basic_v2.chroma_client",
        "documentation": {}
    },
    {
        "label": "collection",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.chroma_client",
        "description": "projects.A3_rag_basic_v2.chroma_client",
        "peekOfCode": "collection = client.get_or_create_collection(COLLECTION_NAME)",
        "detail": "projects.A3_rag_basic_v2.chroma_client",
        "documentation": {}
    },
    {
        "label": "COLLECTION_NAME",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.config",
        "description": "projects.A3_rag_basic_v2.config",
        "peekOfCode": "COLLECTION_NAME = \"a3_docs_v2\"\nEMBEDDING_MODEL = DEFAULT_EMBEDDING_MODEL\nCHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A3_rag_basic_v2.config",
        "documentation": {}
    },
    {
        "label": "EMBEDDING_MODEL",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.config",
        "description": "projects.A3_rag_basic_v2.config",
        "peekOfCode": "EMBEDDING_MODEL = DEFAULT_EMBEDDING_MODEL\nCHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A3_rag_basic_v2.config",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.config",
        "description": "projects.A3_rag_basic_v2.config",
        "peekOfCode": "CHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A3_rag_basic_v2.config",
        "documentation": {}
    },
    {
        "label": "rag_prompt",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.prompts",
        "description": "projects.A3_rag_basic_v2.prompts",
        "peekOfCode": "rag_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente especializado en RAG. Debes responder **únicamente** con la información que aparezca en el siguiente contexto. \nNo inventes datos, no agregues conocimiento externo y no uses información general que no esté contenida explícitamente en el contexto.\nSi el contexto no contiene información suficiente para responder la pregunta, responde exactamente:\n\"Sin suficiente información en la documentación para responder.\"\nProduce una respuesta:\n- Clara, concisa y directa.\n- Basada solo en detalles presentes en el contexto.\n- Sintetizando y explicando, no copiando el contexto textual sin procesarlo.\n- Sin añadir interpretaciones no justificadas por el contenido.",
        "detail": "projects.A3_rag_basic_v2.prompts",
        "documentation": {}
    },
    {
        "label": "build_index_from_folder",
        "kind": 2,
        "importPath": "projects.A3_rag_basic_v2.rag",
        "description": "projects.A3_rag_basic_v2.rag",
        "peekOfCode": "def build_index_from_folder(folder_path: str):\n    docs = []\n    ids = []\n    for fname in os.listdir(folder_path):\n        if not fname.endswith((\".txt\", \".md\")):\n            continue\n        with open(os.path.join(folder_path, fname), \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n        # Fragmentar en chunks de 400 caracteres\n        chunks = [content[i:i+400] for i in range(0, len(content), 400)]",
        "detail": "projects.A3_rag_basic_v2.rag",
        "documentation": {}
    },
    {
        "label": "retrieve",
        "kind": 2,
        "importPath": "projects.A3_rag_basic_v2.rag",
        "description": "projects.A3_rag_basic_v2.rag",
        "peekOfCode": "def retrieve(question: str, top_k: int = 3):\n    query_vec = model.encode([question]).tolist()[0]\n    results = collection.query(query_embeddings=[query_vec], n_results=top_k)\n    documents = results.get(\"documents\", [[]])[0]\n    return documents",
        "detail": "projects.A3_rag_basic_v2.rag",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.rag",
        "description": "projects.A3_rag_basic_v2.rag",
        "peekOfCode": "model = SentenceTransformer(EMBEDDING_MODEL)\n# Construye el índice desde una carpeta de documentos\ndef build_index_from_folder(folder_path: str):\n    docs = []\n    ids = []\n    for fname in os.listdir(folder_path):\n        if not fname.endswith((\".txt\", \".md\")):\n            continue\n        with open(os.path.join(folder_path, fname), \"r\", encoding=\"utf-8\") as f:\n            content = f.read()",
        "detail": "projects.A3_rag_basic_v2.rag",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.router",
        "description": "projects.A3_rag_basic_v2.router",
        "peekOfCode": "router = APIRouter(prefix=\"/a3v2\", tags=[\"A3 - RAG Básico v2\"])\nDATA_PATH = os.path.join(os.path.dirname(__file__), \"data\")\n# Indexado automático en segundo plano al cargar el módulo \ndef _auto_build_index():\n    try:\n        build_index_from_folder(DATA_PATH)\n    except Exception as e:\n        print(f\"[RAG] Error durante el indexado automático: {e}\")\n# Lanzamos el indexado en un hilo para no bloquear FastAPI al arrancar\nthreading.Thread(target=_auto_build_index, daemon=True).start()",
        "detail": "projects.A3_rag_basic_v2.router",
        "documentation": {}
    },
    {
        "label": "DATA_PATH",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.router",
        "description": "projects.A3_rag_basic_v2.router",
        "peekOfCode": "DATA_PATH = os.path.join(os.path.dirname(__file__), \"data\")\n# Indexado automático en segundo plano al cargar el módulo \ndef _auto_build_index():\n    try:\n        build_index_from_folder(DATA_PATH)\n    except Exception as e:\n        print(f\"[RAG] Error durante el indexado automático: {e}\")\n# Lanzamos el indexado en un hilo para no bloquear FastAPI al arrancar\nthreading.Thread(target=_auto_build_index, daemon=True).start()\n@router.post(",
        "detail": "projects.A3_rag_basic_v2.router",
        "documentation": {}
    },
    {
        "label": "QueryRequest",
        "kind": 6,
        "importPath": "projects.A3_rag_basic_v2.schemas",
        "description": "projects.A3_rag_basic_v2.schemas",
        "peekOfCode": "class QueryRequest(BaseModel):\n    question: str = Field(..., example=\"Qué es LangChain?\")\nclass QueryResponse(BaseModel):\n    answer: str = Field(..., description=\"Respuesta generada por el modelo LLM\")\n    sources: list[str] = Field(..., description=\"Documentos utilizados para generar la respuesta\")",
        "detail": "projects.A3_rag_basic_v2.schemas",
        "documentation": {}
    },
    {
        "label": "QueryResponse",
        "kind": 6,
        "importPath": "projects.A3_rag_basic_v2.schemas",
        "description": "projects.A3_rag_basic_v2.schemas",
        "peekOfCode": "class QueryResponse(BaseModel):\n    answer: str = Field(..., description=\"Respuesta generada por el modelo LLM\")\n    sources: list[str] = Field(..., description=\"Documentos utilizados para generar la respuesta\")",
        "detail": "projects.A3_rag_basic_v2.schemas",
        "documentation": {}
    },
    {
        "label": "hash_text",
        "kind": 2,
        "importPath": "projects.A3_rag_basic_v2.utils",
        "description": "projects.A3_rag_basic_v2.utils",
        "peekOfCode": "def hash_text(text: str) -> str:\n    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\ndef safe_json_parse(text: str):\n    try:\n        data = json.loads(text)\n    except json.JSONDecodeError:\n        return {\"answer\": text.strip(), \"sources\": []}\n    # Validamos estructura con Pydantic\n    try:\n        parsed = QueryResponse(**data)",
        "detail": "projects.A3_rag_basic_v2.utils",
        "documentation": {}
    },
    {
        "label": "safe_json_parse",
        "kind": 2,
        "importPath": "projects.A3_rag_basic_v2.utils",
        "description": "projects.A3_rag_basic_v2.utils",
        "peekOfCode": "def safe_json_parse(text: str):\n    try:\n        data = json.loads(text)\n    except json.JSONDecodeError:\n        return {\"answer\": text.strip(), \"sources\": []}\n    # Validamos estructura con Pydantic\n    try:\n        parsed = QueryResponse(**data)\n    except Exception as e:\n        return {",
        "detail": "projects.A3_rag_basic_v2.utils",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced.chroma_client",
        "description": "projects.A4_rag_advanced.chroma_client",
        "peekOfCode": "client = chromadb.PersistentClient(path=CHROMA_PATH)\ncollection = client.get_or_create_collection(COLLECTION_NAME)",
        "detail": "projects.A4_rag_advanced.chroma_client",
        "documentation": {}
    },
    {
        "label": "collection",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced.chroma_client",
        "description": "projects.A4_rag_advanced.chroma_client",
        "peekOfCode": "collection = client.get_or_create_collection(COLLECTION_NAME)",
        "detail": "projects.A4_rag_advanced.chroma_client",
        "documentation": {}
    },
    {
        "label": "COLLECTION_NAME",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced.config",
        "description": "projects.A4_rag_advanced.config",
        "peekOfCode": "COLLECTION_NAME = \"a4_docs\"\nEMBEDDING_MODEL = DEFAULT_EMBEDDING_MODEL\nCHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A4_rag_advanced.config",
        "documentation": {}
    },
    {
        "label": "EMBEDDING_MODEL",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced.config",
        "description": "projects.A4_rag_advanced.config",
        "peekOfCode": "EMBEDDING_MODEL = DEFAULT_EMBEDDING_MODEL\nCHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A4_rag_advanced.config",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced.config",
        "description": "projects.A4_rag_advanced.config",
        "peekOfCode": "CHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A4_rag_advanced.config",
        "documentation": {}
    },
    {
        "label": "load_documents",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced.loader",
        "description": "projects.A4_rag_advanced.loader",
        "peekOfCode": "def load_documents():\n    docs = []\n    for file in os.listdir(DATA_PATH):\n        if file.endswith((\".txt\", \".md\")):\n            loader = TextLoader(os.path.join(DATA_PATH, file), encoding=\"utf-8\") # Crear cargador de texto\n            docs.extend(loader.load()) # Cargar y agregar documentos\n    return docs\n# Divide documentos en chunks con solapamiento\ndef split_documents(documents, chunk_size=600, chunk_overlap=100):\n    splitter = RecursiveCharacterTextSplitter(",
        "detail": "projects.A4_rag_advanced.loader",
        "documentation": {}
    },
    {
        "label": "split_documents",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced.loader",
        "description": "projects.A4_rag_advanced.loader",
        "peekOfCode": "def split_documents(documents, chunk_size=600, chunk_overlap=100):\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \"]\n    )\n    return splitter.split_documents(documents)",
        "detail": "projects.A4_rag_advanced.loader",
        "documentation": {}
    },
    {
        "label": "DATA_PATH",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced.loader",
        "description": "projects.A4_rag_advanced.loader",
        "peekOfCode": "DATA_PATH = os.path.join(os.path.dirname(__file__), \"data\")\n# Carga todos los documentos TXT y MD desde /data/\ndef load_documents():\n    docs = []\n    for file in os.listdir(DATA_PATH):\n        if file.endswith((\".txt\", \".md\")):\n            loader = TextLoader(os.path.join(DATA_PATH, file), encoding=\"utf-8\") # Crear cargador de texto\n            docs.extend(loader.load()) # Cargar y agregar documentos\n    return docs\n# Divide documentos en chunks con solapamiento",
        "detail": "projects.A4_rag_advanced.loader",
        "documentation": {}
    },
    {
        "label": "rag_prompt",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced.prompts",
        "description": "projects.A4_rag_advanced.prompts",
        "peekOfCode": "rag_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente especializado en RAG. Debes responder **únicamente** con la información que aparezca en el siguiente contexto. \nNo inventes datos, no agregues conocimiento externo y no uses información general que no esté contenida explícitamente en el contexto.\nSi el contexto no contiene información suficiente para responder la pregunta, responde exactamente:\n\"Sin suficiente información en la documentación para responder.\"\nProduce una respuesta:\n- Clara, concisa y directa.\n- Basada solo en detalles presentes en el contexto.\n- Sintetizando y explicando, no copiando el contexto textual sin procesarlo.\n- Sin añadir interpretaciones no justificadas por el contenido.",
        "detail": "projects.A4_rag_advanced.prompts",
        "documentation": {}
    },
    {
        "label": "build_vectorstore",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced.rag",
        "description": "projects.A4_rag_advanced.rag",
        "peekOfCode": "def build_vectorstore():\n    print(f\"Construyendo colección persistente '{COLLECTION_NAME}'...\")\n    # Cargar y dividir documentos\n    raw_chunks = split_documents(load_documents())\n    if not raw_chunks:\n        print(\"No se encontraron documentos para procesar.\")\n        return collection\n    # Lista de nuevos chunks a indexar\n    new_chunks = []\n    # Recorrer raw_chunks que es un list[Document] con documentos fragmentados y verificar duplicados, ",
        "detail": "projects.A4_rag_advanced.rag",
        "documentation": {}
    },
    {
        "label": "retrieve_context",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced.rag",
        "description": "projects.A4_rag_advanced.rag",
        "peekOfCode": "def retrieve_context(question: str, n_results: int = 3):\n    # Convertir pregunta → embedding\n    query_vec = model.encode([question]).tolist()[0]\n    # Consultar la colección en ChromaDB\n    results = collection.query(\n        query_embeddings=[query_vec],\n        n_results=n_results\n    )\n    # Extraer documentos y metadatos (si no existen, usar listas vacías), distancias para posibles futuros usos\n    retrieved_docs = results.get(\"documents\", [[]])[0]",
        "detail": "projects.A4_rag_advanced.rag",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced.rag",
        "description": "projects.A4_rag_advanced.rag",
        "peekOfCode": "model = SentenceTransformer(EMBEDDING_MODEL)\n# ==========================================================\n# Construcción del índice (siempre se reconstruye si hay nuevos documentos)\n# ==========================================================\n# Crea embeddings y guarda documentos nuevos en la colección persistente.\ndef build_vectorstore():\n    print(f\"Construyendo colección persistente '{COLLECTION_NAME}'...\")\n    # Cargar y dividir documentos\n    raw_chunks = split_documents(load_documents())\n    if not raw_chunks:",
        "detail": "projects.A4_rag_advanced.rag",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced.router",
        "description": "projects.A4_rag_advanced.router",
        "peekOfCode": "router = APIRouter(prefix=\"/a4\", tags=[\"A4 - RAG Avanzado\"])\n# Lanzamos indexado en segundo plano al importar el router\ndef _auto_build_index():\n    try:\n        print(\"Construyendo índice RAG avanzado en background...\")\n        build_vectorstore()\n        print(\"Índice RAG avanzado listo.\")\n    except Exception as e:\n        print(f\"[RAG] Error durante el indexado automático: {e}\")\nthreading.Thread(target=_auto_build_index, daemon=True).start()",
        "detail": "projects.A4_rag_advanced.router",
        "documentation": {}
    },
    {
        "label": "QueryRequest",
        "kind": 6,
        "importPath": "projects.A4_rag_advanced.schemas",
        "description": "projects.A4_rag_advanced.schemas",
        "peekOfCode": "class QueryRequest(BaseModel):\n    question: str = Field(..., example=\"Qué son las Redes Neuronales Convolucionales?\")\nclass SourceDocument(BaseModel):\n    source: str = Field(..., description=\"Ruta o nombre del documento de origen\")\n    score: float = Field(..., description=\"Similitud o relevancia del documento recuperado\")\nclass QueryResponse(BaseModel):\n    answer: str = Field(..., description=\"Respuesta generada por el modelo\")\n    sources: list[SourceDocument] = Field(..., description=\"Documentos usados para generar la respuesta\")",
        "detail": "projects.A4_rag_advanced.schemas",
        "documentation": {}
    },
    {
        "label": "SourceDocument",
        "kind": 6,
        "importPath": "projects.A4_rag_advanced.schemas",
        "description": "projects.A4_rag_advanced.schemas",
        "peekOfCode": "class SourceDocument(BaseModel):\n    source: str = Field(..., description=\"Ruta o nombre del documento de origen\")\n    score: float = Field(..., description=\"Similitud o relevancia del documento recuperado\")\nclass QueryResponse(BaseModel):\n    answer: str = Field(..., description=\"Respuesta generada por el modelo\")\n    sources: list[SourceDocument] = Field(..., description=\"Documentos usados para generar la respuesta\")",
        "detail": "projects.A4_rag_advanced.schemas",
        "documentation": {}
    },
    {
        "label": "QueryResponse",
        "kind": 6,
        "importPath": "projects.A4_rag_advanced.schemas",
        "description": "projects.A4_rag_advanced.schemas",
        "peekOfCode": "class QueryResponse(BaseModel):\n    answer: str = Field(..., description=\"Respuesta generada por el modelo\")\n    sources: list[SourceDocument] = Field(..., description=\"Documentos usados para generar la respuesta\")",
        "detail": "projects.A4_rag_advanced.schemas",
        "documentation": {}
    },
    {
        "label": "hash_text",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced.utils",
        "description": "projects.A4_rag_advanced.utils",
        "peekOfCode": "def hash_text(text: str) -> str:\n    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n# Comprueba si un chunk ya está almacenado en la colección.\ndef is_chunk_indexed(chunk_id: str) -> bool:\n    try:\n        existing = collection.get(ids=[chunk_id])\n        return bool(existing and existing[\"ids\"])\n    except Exception:\n        return False\n# Convierte metadatos y distancias en una lista de fuentes formateadas.",
        "detail": "projects.A4_rag_advanced.utils",
        "documentation": {}
    },
    {
        "label": "is_chunk_indexed",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced.utils",
        "description": "projects.A4_rag_advanced.utils",
        "peekOfCode": "def is_chunk_indexed(chunk_id: str) -> bool:\n    try:\n        existing = collection.get(ids=[chunk_id])\n        return bool(existing and existing[\"ids\"])\n    except Exception:\n        return False\n# Convierte metadatos y distancias en una lista de fuentes formateadas.\ndef format_sources(metadatas: list, distances: list) -> list:\n    formatted = []\n    for meta, dist in zip(metadatas, distances):",
        "detail": "projects.A4_rag_advanced.utils",
        "documentation": {}
    },
    {
        "label": "format_sources",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced.utils",
        "description": "projects.A4_rag_advanced.utils",
        "peekOfCode": "def format_sources(metadatas: list, distances: list) -> list:\n    formatted = []\n    for meta, dist in zip(metadatas, distances):\n        # Convertir distancia a similitud (asumiendo distancia >= 0)\n        similarity = 1 / (1 + dist)\n        formatted.append({\n            \"source\": meta.get(\"source\", \"desconocido\"),\n            \"score\": round(similarity, 4) # Redondear a 4 decimales\n        })\n    return formatted",
        "detail": "projects.A4_rag_advanced.utils",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.chroma_client",
        "description": "projects.A4_rag_advanced_v2.chroma_client",
        "peekOfCode": "client = chromadb.PersistentClient(path=CHROMA_PATH)\ncollection = client.get_or_create_collection(COLLECTION_NAME)",
        "detail": "projects.A4_rag_advanced_v2.chroma_client",
        "documentation": {}
    },
    {
        "label": "collection",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.chroma_client",
        "description": "projects.A4_rag_advanced_v2.chroma_client",
        "peekOfCode": "collection = client.get_or_create_collection(COLLECTION_NAME)",
        "detail": "projects.A4_rag_advanced_v2.chroma_client",
        "documentation": {}
    },
    {
        "label": "COLLECTION_NAME",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.config",
        "description": "projects.A4_rag_advanced_v2.config",
        "peekOfCode": "COLLECTION_NAME = \"a4_docs_v2\"\nEMBEDDING_MODEL = DEFAULT_EMBEDDING_MODEL\nCHROMA_PATH = CHROMA_PATH\nURLS_TO_SCRAPE = [\n    \"https://es.wikipedia.org/wiki/Web_scraping\",\n    \"https://es.wikipedia.org/wiki/Base_de_datos_de_vectores\",\n    # Añade más URLs según sea necesario\n]",
        "detail": "projects.A4_rag_advanced_v2.config",
        "documentation": {}
    },
    {
        "label": "EMBEDDING_MODEL",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.config",
        "description": "projects.A4_rag_advanced_v2.config",
        "peekOfCode": "EMBEDDING_MODEL = DEFAULT_EMBEDDING_MODEL\nCHROMA_PATH = CHROMA_PATH\nURLS_TO_SCRAPE = [\n    \"https://es.wikipedia.org/wiki/Web_scraping\",\n    \"https://es.wikipedia.org/wiki/Base_de_datos_de_vectores\",\n    # Añade más URLs según sea necesario\n]",
        "detail": "projects.A4_rag_advanced_v2.config",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.config",
        "description": "projects.A4_rag_advanced_v2.config",
        "peekOfCode": "CHROMA_PATH = CHROMA_PATH\nURLS_TO_SCRAPE = [\n    \"https://es.wikipedia.org/wiki/Web_scraping\",\n    \"https://es.wikipedia.org/wiki/Base_de_datos_de_vectores\",\n    # Añade más URLs según sea necesario\n]",
        "detail": "projects.A4_rag_advanced_v2.config",
        "documentation": {}
    },
    {
        "label": "URLS_TO_SCRAPE",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.config",
        "description": "projects.A4_rag_advanced_v2.config",
        "peekOfCode": "URLS_TO_SCRAPE = [\n    \"https://es.wikipedia.org/wiki/Web_scraping\",\n    \"https://es.wikipedia.org/wiki/Base_de_datos_de_vectores\",\n    # Añade más URLs según sea necesario\n]",
        "detail": "projects.A4_rag_advanced_v2.config",
        "documentation": {}
    },
    {
        "label": "load_documents",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced_v2.loader",
        "description": "projects.A4_rag_advanced_v2.loader",
        "peekOfCode": "def load_documents():\n    docs = []\n    for file in os.listdir(DATA_PATH):\n        if file.endswith((\".txt\", \".md\")):\n            loader = TextLoader(os.path.join(DATA_PATH, file), encoding=\"utf-8\") # Crear cargador de texto\n            docs.extend(loader.load()) # Cargar y agregar documentos\n    return docs\n# Divide documentos en chunks con solapamiento\ndef split_documents(documents, chunk_size=600, chunk_overlap=100):\n    splitter = RecursiveCharacterTextSplitter(",
        "detail": "projects.A4_rag_advanced_v2.loader",
        "documentation": {}
    },
    {
        "label": "split_documents",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced_v2.loader",
        "description": "projects.A4_rag_advanced_v2.loader",
        "peekOfCode": "def split_documents(documents, chunk_size=600, chunk_overlap=100):\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \"]\n    )\n    return splitter.split_documents(documents)",
        "detail": "projects.A4_rag_advanced_v2.loader",
        "documentation": {}
    },
    {
        "label": "DATA_PATH",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.loader",
        "description": "projects.A4_rag_advanced_v2.loader",
        "peekOfCode": "DATA_PATH = os.path.join(os.path.dirname(__file__), \"data\")\n# Carga todos los documentos TXT y MD desde /data/\ndef load_documents():\n    docs = []\n    for file in os.listdir(DATA_PATH):\n        if file.endswith((\".txt\", \".md\")):\n            loader = TextLoader(os.path.join(DATA_PATH, file), encoding=\"utf-8\") # Crear cargador de texto\n            docs.extend(loader.load()) # Cargar y agregar documentos\n    return docs\n# Divide documentos en chunks con solapamiento",
        "detail": "projects.A4_rag_advanced_v2.loader",
        "documentation": {}
    },
    {
        "label": "rag_prompt",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.prompts",
        "description": "projects.A4_rag_advanced_v2.prompts",
        "peekOfCode": "rag_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente especializado en RAG. Debes responder **únicamente** con la información que aparezca en el siguiente contexto. \nNo inventes datos, no agregues conocimiento externo y no uses información general que no esté contenida explícitamente en el contexto.\nSi el contexto no contiene información suficiente para responder la pregunta, responde exactamente:\n\"Sin suficiente información en la documentación para responder.\"\nProduce una respuesta:\n- Clara, concisa y directa.\n- Basada solo en detalles presentes en el contexto.\n- Sintetizando y explicando, no copiando el contexto textual sin procesarlo.\n- Sin añadir interpretaciones no justificadas por el contenido.",
        "detail": "projects.A4_rag_advanced_v2.prompts",
        "documentation": {}
    },
    {
        "label": "build_vectorstore",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced_v2.rag",
        "description": "projects.A4_rag_advanced_v2.rag",
        "peekOfCode": "def build_vectorstore(urls: list[str] = None):\n    print(\"📌 Iniciando indexado...\")\n    # list[Document] con todos los chunks sin procesar\n    raw_chunks = []\n    # Documentos locales desde /data/\n    raw_chunks.extend(split_documents(load_documents()))\n    # Documentos scrapeados desde URLs (si se proporcionan)\n    if urls:\n        for url in urls:\n            print(f\"Scrapeando y procesando: {url}\")",
        "detail": "projects.A4_rag_advanced_v2.rag",
        "documentation": {}
    },
    {
        "label": "retrieve_context",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced_v2.rag",
        "description": "projects.A4_rag_advanced_v2.rag",
        "peekOfCode": "def retrieve_context(question: str, n_results: int = 3) -> (tuple[list[str], list[dict]]):\n    # Convertir pregunta → embedding\n    query_vec = model.encode([question]).tolist()[0]\n    # Consultar la colección en ChromaDB\n    results = collection.query(\n        query_embeddings=[query_vec],\n        n_results=n_results\n    )\n    # Extraer documentos y metadatos (si no existen, usar listas vacías), distancias para posibles futuros usos\n    retrieved_docs = results.get(\"documents\", [[]])[0]",
        "detail": "projects.A4_rag_advanced_v2.rag",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.rag",
        "description": "projects.A4_rag_advanced_v2.rag",
        "peekOfCode": "model = SentenceTransformer(EMBEDDING_MODEL)\n# ==========================================================\n# Construcción del índice (local + web) (siempre se reconstruye si hay nuevos documentos)\n# ==========================================================\n# Crea embeddings y guarda documentos nuevos en la colección persistente.\ndef build_vectorstore(urls: list[str] = None):\n    print(\"📌 Iniciando indexado...\")\n    # list[Document] con todos los chunks sin procesar\n    raw_chunks = []\n    # Documentos locales desde /data/",
        "detail": "projects.A4_rag_advanced_v2.rag",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.router",
        "description": "projects.A4_rag_advanced_v2.router",
        "peekOfCode": "router = APIRouter(prefix=\"/a4v2\", tags=[\"A4 - RAG Avanzado con web scraping, compresión contextual y fuentes puntuadas\"])\n# Lanzamos indexado en segundo plano al importar el router\ndef _auto_build_index():\n    try:\n        print(\"Construyendo índice RAG avanzado en background...\")\n        build_vectorstore(URLS_TO_SCRAPE)\n        print(\"Índice RAG avanzado listo.\")\n    except Exception as e:\n        print(f\"[RAG] Error durante el indexado automático: {e}\")\nthreading.Thread(target=_auto_build_index, daemon=True).start()",
        "detail": "projects.A4_rag_advanced_v2.router",
        "documentation": {}
    },
    {
        "label": "QueryRequest",
        "kind": 6,
        "importPath": "projects.A4_rag_advanced_v2.schemas",
        "description": "projects.A4_rag_advanced_v2.schemas",
        "peekOfCode": "class QueryRequest(BaseModel):\n    question: str = Field(..., example=\"Qué significa escrapear una página web?\")\nclass SourceDocument(BaseModel):\n    source: str = Field(..., description=\"Ruta o nombre del documento de origen\")\n    score: float = Field(..., description=\"Similitud o relevancia del documento recuperado\")\nclass QueryResponse(BaseModel):\n    answer: str = Field(..., description=\"Respuesta generada por el modelo\")\n    sources: list[SourceDocument] = Field(..., description=\"Documentos usados para generar la respuesta\")",
        "detail": "projects.A4_rag_advanced_v2.schemas",
        "documentation": {}
    },
    {
        "label": "SourceDocument",
        "kind": 6,
        "importPath": "projects.A4_rag_advanced_v2.schemas",
        "description": "projects.A4_rag_advanced_v2.schemas",
        "peekOfCode": "class SourceDocument(BaseModel):\n    source: str = Field(..., description=\"Ruta o nombre del documento de origen\")\n    score: float = Field(..., description=\"Similitud o relevancia del documento recuperado\")\nclass QueryResponse(BaseModel):\n    answer: str = Field(..., description=\"Respuesta generada por el modelo\")\n    sources: list[SourceDocument] = Field(..., description=\"Documentos usados para generar la respuesta\")",
        "detail": "projects.A4_rag_advanced_v2.schemas",
        "documentation": {}
    },
    {
        "label": "QueryResponse",
        "kind": 6,
        "importPath": "projects.A4_rag_advanced_v2.schemas",
        "description": "projects.A4_rag_advanced_v2.schemas",
        "peekOfCode": "class QueryResponse(BaseModel):\n    answer: str = Field(..., description=\"Respuesta generada por el modelo\")\n    sources: list[SourceDocument] = Field(..., description=\"Documentos usados para generar la respuesta\")",
        "detail": "projects.A4_rag_advanced_v2.schemas",
        "documentation": {}
    },
    {
        "label": "scrape_webpage",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced_v2.scraper",
        "description": "projects.A4_rag_advanced_v2.scraper",
        "peekOfCode": "def scrape_webpage(url: str) -> list[Document]:\n    headers = {\n        \"User-Agent\": (\n            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n            \"Chrome/123.0.0.0 Safari/537.36\"\n        )\n    }\n    resp = requests.get(url, headers=headers, timeout=10)\n    resp.raise_for_status()",
        "detail": "projects.A4_rag_advanced_v2.scraper",
        "documentation": {}
    },
    {
        "label": "hash_text",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced_v2.utils",
        "description": "projects.A4_rag_advanced_v2.utils",
        "peekOfCode": "def hash_text(text: str) -> str:\n    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n# Comprueba si un chunk ya está almacenado en la colección.\ndef is_chunk_indexed(chunk_id: str) -> bool:\n    try:\n        existing = collection.get(ids=[chunk_id])\n        return bool(existing and existing[\"ids\"])\n    except Exception:\n        return False\n# Convierte metadatos y distancias en una lista de fuentes formateadas.",
        "detail": "projects.A4_rag_advanced_v2.utils",
        "documentation": {}
    },
    {
        "label": "is_chunk_indexed",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced_v2.utils",
        "description": "projects.A4_rag_advanced_v2.utils",
        "peekOfCode": "def is_chunk_indexed(chunk_id: str) -> bool:\n    try:\n        existing = collection.get(ids=[chunk_id])\n        return bool(existing and existing[\"ids\"])\n    except Exception:\n        return False\n# Convierte metadatos y distancias en una lista de fuentes formateadas.\ndef format_sources(metadatas: list, distances: list) -> list:\n    formatted = []\n    for meta, dist in zip(metadatas, distances):",
        "detail": "projects.A4_rag_advanced_v2.utils",
        "documentation": {}
    },
    {
        "label": "format_sources",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced_v2.utils",
        "description": "projects.A4_rag_advanced_v2.utils",
        "peekOfCode": "def format_sources(metadatas: list, distances: list) -> list:\n    formatted = []\n    for meta, dist in zip(metadatas, distances):\n        # Convertir distancia a similitud (asumiendo distancia >= 0)\n        similarity = 1 / (1 + dist)\n        formatted.append({\n            \"source\": meta.get(\"source\", \"desconocido\"),\n            \"score\": round(similarity, 4) # Redondear a 4 decimales\n        })\n    return formatted",
        "detail": "projects.A4_rag_advanced_v2.utils",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "projects.A5_chains_routers.chains",
        "description": "projects.A5_chains_routers.chains",
        "peekOfCode": "llm = llm_chain()\nparser = StrOutputParser()\n# ======================================================\n# Definición de Chains LCEL\n# Cada Chain:\n# - Prompt → LLM → Parser\n# - Se encapsula la salida en {\"answer\": ..., \"chain_used\": ...}\n# ======================================================\nclassifier_chain = classifier_prompt | llm | parser\ngeneral_chain = (",
        "detail": "projects.A5_chains_routers.chains",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "projects.A5_chains_routers.chains",
        "description": "projects.A5_chains_routers.chains",
        "peekOfCode": "parser = StrOutputParser()\n# ======================================================\n# Definición de Chains LCEL\n# Cada Chain:\n# - Prompt → LLM → Parser\n# - Se encapsula la salida en {\"answer\": ..., \"chain_used\": ...}\n# ======================================================\nclassifier_chain = classifier_prompt | llm | parser\ngeneral_chain = (\n    general_prompt",
        "detail": "projects.A5_chains_routers.chains",
        "documentation": {}
    },
    {
        "label": "classifier_chain",
        "kind": 5,
        "importPath": "projects.A5_chains_routers.chains",
        "description": "projects.A5_chains_routers.chains",
        "peekOfCode": "classifier_chain = classifier_prompt | llm | parser\ngeneral_chain = (\n    general_prompt\n    | llm\n    | parser\n    | RunnableLambda(lambda x: {\"answer\": x, \"chain_used\": \"general_chain\"})\n)\ncode_chain = (\n    code_prompt\n    | llm",
        "detail": "projects.A5_chains_routers.chains",
        "documentation": {}
    },
    {
        "label": "general_chain",
        "kind": 5,
        "importPath": "projects.A5_chains_routers.chains",
        "description": "projects.A5_chains_routers.chains",
        "peekOfCode": "general_chain = (\n    general_prompt\n    | llm\n    | parser\n    | RunnableLambda(lambda x: {\"answer\": x, \"chain_used\": \"general_chain\"})\n)\ncode_chain = (\n    code_prompt\n    | llm\n    | parser",
        "detail": "projects.A5_chains_routers.chains",
        "documentation": {}
    },
    {
        "label": "code_chain",
        "kind": 5,
        "importPath": "projects.A5_chains_routers.chains",
        "description": "projects.A5_chains_routers.chains",
        "peekOfCode": "code_chain = (\n    code_prompt\n    | llm\n    | parser\n    | RunnableLambda(lambda x: {\"answer\": x, \"chain_used\": \"code_chain\"})\n)\nsummary_chain = (\n    summary_prompt\n    | llm\n    | parser",
        "detail": "projects.A5_chains_routers.chains",
        "documentation": {}
    },
    {
        "label": "summary_chain",
        "kind": 5,
        "importPath": "projects.A5_chains_routers.chains",
        "description": "projects.A5_chains_routers.chains",
        "peekOfCode": "summary_chain = (\n    summary_prompt\n    | llm\n    | parser\n    | RunnableLambda(lambda x: {\"answer\": x, \"chain_used\": \"summary_chain\"})\n)\nmath_chain = (\n    math_prompt\n    | llm\n    | parser",
        "detail": "projects.A5_chains_routers.chains",
        "documentation": {}
    },
    {
        "label": "math_chain",
        "kind": 5,
        "importPath": "projects.A5_chains_routers.chains",
        "description": "projects.A5_chains_routers.chains",
        "peekOfCode": "math_chain = (\n    math_prompt\n    | llm\n    | parser\n    | RunnableLambda(lambda x: {\"answer\": x, \"chain_used\": \"math_chain\"})\n)\n# rag_chain = (\n#     rag_chain\n#     | RunnableLambda(lambda x: {\"answer\": x, \"chain_used\": \"rag_chain\"})\n# )",
        "detail": "projects.A5_chains_routers.chains",
        "documentation": {}
    },
    {
        "label": "rag_chain",
        "kind": 5,
        "importPath": "projects.A5_chains_routers.chains",
        "description": "projects.A5_chains_routers.chains",
        "peekOfCode": "rag_chain = (\n    # 1) Recibir la pregunta\n    {\"input\": RunnablePassthrough()} # filtro que solo pasa el input\n    # 2) Recuperar contexto\n    | RunnableLambda(\n        lambda x: {\n            \"input\": x[\"input\"],\n            \"context\": retrieve_context(x[\"input\"])\n        }\n    )",
        "detail": "projects.A5_chains_routers.chains",
        "documentation": {}
    },
    {
        "label": "router_chain",
        "kind": 5,
        "importPath": "projects.A5_chains_routers.chains",
        "description": "projects.A5_chains_routers.chains",
        "peekOfCode": "router_chain = RunnableBranch(\n    (lambda x: \"rag\" in x[\"intent\"], rag_chain),\n    (lambda x: \"code\" in x[\"intent\"], code_chain),\n    (lambda x: \"summary\" in x[\"intent\"], summary_chain),\n    (lambda x: \"math\" in x[\"intent\"], math_chain),\n    general_chain,  # Default\n)\n# ======================================================\n# Función principal async\n# ======================================================",
        "detail": "projects.A5_chains_routers.chains",
        "documentation": {}
    },
    {
        "label": "classifier_prompt",
        "kind": 5,
        "importPath": "projects.A5_chains_routers.prompts",
        "description": "projects.A5_chains_routers.prompts",
        "peekOfCode": "classifier_prompt = PromptTemplate.from_template(\"\"\"\nClasifica la siguiente pregunta en una de las siguientes categorías:\n- general → preguntas abiertas, explicaciones, creatividad, conversación normal.\n- rag → preguntas que requieren usar información REAL proveniente de documentos, base de conocimiento o datos externos. Palabras clave: \"basado en documentación\", \"según el textos almacenados\", \"extrae de información almacenada\", \"basado en el material\", \"qué indican los datos almacenados\".\n- summary → cuando se pide RESUMIR un texto proporcionado por el usuario. Solo se clasifica como summary si el usuario claramente proporciona texto para ser resumido.\n- code → preguntas relacionadas con programación, errores, generación de funciones, fragmentos de código, debugging.\n- math → problemas matemáticos, cálculos, expresiones numéricas, ecuaciones o razonamiento matemático.\nReglas importantes:\n1. Si la pregunta pide resumir contenido proporcionado por el usuario → summary.\n2. Si la pregunta pide extraer, buscar o consultar información de un documento, PDF, manual o contexto → rag.",
        "detail": "projects.A5_chains_routers.prompts",
        "documentation": {}
    },
    {
        "label": "general_prompt",
        "kind": 5,
        "importPath": "projects.A5_chains_routers.prompts",
        "description": "projects.A5_chains_routers.prompts",
        "peekOfCode": "general_prompt = PromptTemplate.from_template(\"\"\"\nResponde de forma clara y precisa, evitando información inventada o no verificada, si no tienes la respuesta di \"No lo sé\":\n{input}\n\"\"\")\ncode_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente experto en Python. Ayuda al usuario con su código:\n{input}\n\"\"\")\nsummary_prompt = PromptTemplate.from_template(\"\"\"\nResume el siguiente contenido de manera clara:",
        "detail": "projects.A5_chains_routers.prompts",
        "documentation": {}
    },
    {
        "label": "code_prompt",
        "kind": 5,
        "importPath": "projects.A5_chains_routers.prompts",
        "description": "projects.A5_chains_routers.prompts",
        "peekOfCode": "code_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente experto en Python. Ayuda al usuario con su código:\n{input}\n\"\"\")\nsummary_prompt = PromptTemplate.from_template(\"\"\"\nResume el siguiente contenido de manera clara:\n{input}\n\"\"\")\nmath_prompt = PromptTemplate.from_template(\"\"\"\nResuelve el siguiente ejercicio paso a paso, pero devuelve solo el resultado final:",
        "detail": "projects.A5_chains_routers.prompts",
        "documentation": {}
    },
    {
        "label": "summary_prompt",
        "kind": 5,
        "importPath": "projects.A5_chains_routers.prompts",
        "description": "projects.A5_chains_routers.prompts",
        "peekOfCode": "summary_prompt = PromptTemplate.from_template(\"\"\"\nResume el siguiente contenido de manera clara:\n{input}\n\"\"\")\nmath_prompt = PromptTemplate.from_template(\"\"\"\nResuelve el siguiente ejercicio paso a paso, pero devuelve solo el resultado final:\n{input}\n\"\"\")\nrag_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente especializado en RAG. Debes responder **únicamente** con la información que aparezca en el siguiente contexto. ",
        "detail": "projects.A5_chains_routers.prompts",
        "documentation": {}
    },
    {
        "label": "math_prompt",
        "kind": 5,
        "importPath": "projects.A5_chains_routers.prompts",
        "description": "projects.A5_chains_routers.prompts",
        "peekOfCode": "math_prompt = PromptTemplate.from_template(\"\"\"\nResuelve el siguiente ejercicio paso a paso, pero devuelve solo el resultado final:\n{input}\n\"\"\")\nrag_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente especializado en RAG. Debes responder **únicamente** con la información que aparezca en el siguiente contexto. \nNo inventes datos, no agregues conocimiento externo y no uses información general que no esté contenida explícitamente en el contexto.\nSi el contexto no contiene información suficiente para responder la pregunta, responde exactamente:\n\"Sin suficiente información en la documentación para responder.\"\nProduce una respuesta:",
        "detail": "projects.A5_chains_routers.prompts",
        "documentation": {}
    },
    {
        "label": "rag_prompt",
        "kind": 5,
        "importPath": "projects.A5_chains_routers.prompts",
        "description": "projects.A5_chains_routers.prompts",
        "peekOfCode": "rag_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente especializado en RAG. Debes responder **únicamente** con la información que aparezca en el siguiente contexto. \nNo inventes datos, no agregues conocimiento externo y no uses información general que no esté contenida explícitamente en el contexto.\nSi el contexto no contiene información suficiente para responder la pregunta, responde exactamente:\n\"Sin suficiente información en la documentación para responder.\"\nProduce una respuesta:\n- Clara, concisa y directa.\n- Basada solo en detalles presentes en el contexto.\n- Sintetizando y explicando, no copiando el contexto textual sin procesarlo.\n- Sin añadir interpretaciones no justificadas por el contenido.",
        "detail": "projects.A5_chains_routers.prompts",
        "documentation": {}
    },
    {
        "label": "retrieve_context",
        "kind": 2,
        "importPath": "projects.A5_chains_routers.rag",
        "description": "projects.A5_chains_routers.rag",
        "peekOfCode": "def retrieve_context(question: str, n_results: int = 3) -> str:\n    # Convertir pregunta → embedding\n    query_vec = model.encode([question]).tolist()[0]\n    # Consultar la colección en ChromaDB del proyecto A4_rag_advanced_v2\n    results = collection.query(\n        query_embeddings=[query_vec],\n        n_results=n_results\n    )\n    # Extraer documentos (si no existen, usar listas vacías)\n    retrieved_docs = results.get(\"documents\", [[]])[0]",
        "detail": "projects.A5_chains_routers.rag",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "projects.A5_chains_routers.rag",
        "description": "projects.A5_chains_routers.rag",
        "peekOfCode": "model = SentenceTransformer(DEFAULT_EMBEDDING_MODEL)\n# Recupera contexto relevante desde la colección Chroma del proyecto A4_rag_advanced_v2\ndef retrieve_context(question: str, n_results: int = 3) -> str:\n    # Convertir pregunta → embedding\n    query_vec = model.encode([question]).tolist()[0]\n    # Consultar la colección en ChromaDB del proyecto A4_rag_advanced_v2\n    results = collection.query(\n        query_embeddings=[query_vec],\n        n_results=n_results\n    )",
        "detail": "projects.A5_chains_routers.rag",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "projects.A5_chains_routers.router",
        "description": "projects.A5_chains_routers.router",
        "peekOfCode": "router = APIRouter(\n    prefix=\"/a5\",\n    tags=[\"A5 - Chains y Routers (LangChain avanzado)\"]\n)\n@router.post(\n    \"/query\",\n    summary=\"Pipeline con clasificación, enrutado dinámico y cadenas secuenciales\",\n    description=\"\"\"\n    Implementar un pipeline avanzado usando **LangChain** que incluya:\n    - Clasificación de la intención del usuario.",
        "detail": "projects.A5_chains_routers.router",
        "documentation": {}
    },
    {
        "label": "A5Request",
        "kind": 6,
        "importPath": "projects.A5_chains_routers.schemas",
        "description": "projects.A5_chains_routers.schemas",
        "peekOfCode": "class A5Request(BaseModel):\n    question: str = Field(..., example=\"¿Qué es un embedding?\")\nclass A5Response(BaseModel):\n    intent: str\n    chain_used: str\n    answer: str",
        "detail": "projects.A5_chains_routers.schemas",
        "documentation": {}
    },
    {
        "label": "A5Response",
        "kind": 6,
        "importPath": "projects.A5_chains_routers.schemas",
        "description": "projects.A5_chains_routers.schemas",
        "peekOfCode": "class A5Response(BaseModel):\n    intent: str\n    chain_used: str\n    answer: str",
        "detail": "projects.A5_chains_routers.schemas",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "projects.A6_memory.chroma_client",
        "description": "projects.A6_memory.chroma_client",
        "peekOfCode": "client = chromadb.PersistentClient(path=CHROMA_PATH)\ncollection = client.get_or_create_collection(COLLECTION_NAME)",
        "detail": "projects.A6_memory.chroma_client",
        "documentation": {}
    },
    {
        "label": "collection",
        "kind": 5,
        "importPath": "projects.A6_memory.chroma_client",
        "description": "projects.A6_memory.chroma_client",
        "peekOfCode": "collection = client.get_or_create_collection(COLLECTION_NAME)",
        "detail": "projects.A6_memory.chroma_client",
        "documentation": {}
    },
    {
        "label": "COLLECTION_NAME",
        "kind": 5,
        "importPath": "projects.A6_memory.config",
        "description": "projects.A6_memory.config",
        "peekOfCode": "COLLECTION_NAME = \"a6_memory\"\nCHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A6_memory.config",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "kind": 5,
        "importPath": "projects.A6_memory.config",
        "description": "projects.A6_memory.config",
        "peekOfCode": "CHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A6_memory.config",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "projects.A6_memory.llm_node",
        "description": "projects.A6_memory.llm_node",
        "peekOfCode": "logger = logging.getLogger(__name__)\nasync def call_llm_node(state: Union[ChatState, Dict[str, Any]]) -> Dict[str, Any]:\n    \"\"\"\n    Nodo principal del grafo encargado de:\n    - Normalizar el estado entrante (dict o Pydantic)\n    - Preparar memoria nueva con ayuda de un LLM\n    - Guardar memoria del usuario en ChromaDB\n    - Consultar memoria previa relevante\n    - Generar respuesta usando dicha memoria\n    - Devolver un estado consistente para el siguiente nodo",
        "detail": "projects.A6_memory.llm_node",
        "documentation": {}
    },
    {
        "label": "build_chat_graph",
        "kind": 2,
        "importPath": "projects.A6_memory.memory_graph",
        "description": "projects.A6_memory.memory_graph",
        "peekOfCode": "def build_chat_graph():\n    # El estado base del grafo es un dict\n    graph = StateGraph(dict)\n    # Añadimos los nodos del flujo\n    graph.add_node(\"add_user_message\", add_user_message_node)\n    graph.add_node(\"maybe_summarize\", maybe_summarize_node)\n    graph.add_node(\"call_llm\", call_llm_node)\n    # Definimos transiciones entre nodos\n    graph.add_edge(START, \"add_user_message\")\n    graph.add_edge(\"add_user_message\", \"maybe_summarize\")",
        "detail": "projects.A6_memory.memory_graph",
        "documentation": {}
    },
    {
        "label": "get_chat_graph",
        "kind": 2,
        "importPath": "projects.A6_memory.memory_graph",
        "description": "projects.A6_memory.memory_graph",
        "peekOfCode": "def get_chat_graph():\n    global _COMPILED_CHAT_GRAPH\n    # Si aún no existe, se construye\n    if _COMPILED_CHAT_GRAPH is None:\n        _COMPILED_CHAT_GRAPH = build_chat_graph()\n    # Siempre devuelve el grafo ya compilado\n    return _COMPILED_CHAT_GRAPH",
        "detail": "projects.A6_memory.memory_graph",
        "documentation": {}
    },
    {
        "label": "_COMPILED_CHAT_GRAPH",
        "kind": 5,
        "importPath": "projects.A6_memory.memory_graph",
        "description": "projects.A6_memory.memory_graph",
        "peekOfCode": "_COMPILED_CHAT_GRAPH = None\n# ======================================================\n# Función: build_chat_graph\n# ------------------------------------------------------\n# Crea el grafo conversacional completo de LangGraph.\n# La estructura del grafo es:\n#\n# START → add_user_message\n#         → maybe_summarize\n#         → call_llm",
        "detail": "projects.A6_memory.memory_graph",
        "documentation": {}
    },
    {
        "label": "memory_preparation_prompt",
        "kind": 5,
        "importPath": "projects.A6_memory.prompts",
        "description": "projects.A6_memory.prompts",
        "peekOfCode": "memory_preparation_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente encargado de convertir en memoria la información relevante que el usuario quiera que recuerdes.\nUsuario dice:\n{input}\nInstrucciones:\n- Extrae únicamente información que sea útil recordar (como datos personales, preferencias, números, bancos, etc.).\n- No te limites a guardar solo un numero o dato, guardalo junto con su contexto para que tenga sentido.\n- No guardes afirmaciones sin contenido útil.\n- No generes espacios en blanco, saltos de línea ni caracteres invisibles.\n- Si no hay nada que recordar, devuelve un guion medio (-) para indicar que no se debe guardar nada.",
        "detail": "projects.A6_memory.prompts",
        "documentation": {}
    },
    {
        "label": "memory_prompt",
        "kind": 5,
        "importPath": "projects.A6_memory.prompts",
        "description": "projects.A6_memory.prompts",
        "peekOfCode": "memory_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente que recuerda información del usuario.\nMemoria relevante del usuario:\n{memory}\nPregunta del usuario:\n{input}\nInstrucciones estrictas:\n1. Si el mensaje del usuario es una AFIRMACIÓN (no contiene pregunta y comunica un dato), responde únicamente:\n   ok\n2. Si es una PREGUNTA aunque no tenga signos de interrogación, responde solo a esa pregunta.",
        "detail": "projects.A6_memory.prompts",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "projects.A6_memory.router",
        "description": "projects.A6_memory.router",
        "peekOfCode": "router = APIRouter(prefix=\"/a6memory\", tags=[\"A6 - Memory (LangGraph)\"])\n# ======================================================\n# POST /a6memory/query\n# ------------------------------------------------------\n# Punto principal para que el usuario interactúe con el\n# sistema de memoria + LangGraph.\n#\n# Flujo interno:\n#   1. Crea un estado inicial con el mensaje del usuario.\n#   2. Ejecuta el grafo conversacional (LangGraph).",
        "detail": "projects.A6_memory.router",
        "documentation": {}
    },
    {
        "label": "Message",
        "kind": 6,
        "importPath": "projects.A6_memory.schemas",
        "description": "projects.A6_memory.schemas",
        "peekOfCode": "class Message(BaseModel):\n    role: str            # \"user\" o \"assistant\"\n    content: str         # Texto del mensaje\n# Estado completo utilizado por el grafo de LangGraph.\n# Contiene la información necesaria para ejecutar cada nodo\n# y pasar datos entre ellos.\nclass ChatState(BaseModel):\n    user_id: Optional[str] = None            # Identificador del usuario (para memoria en ChromaDB)\n    messages: Optional[List[Message]] = None # Conversación hasta el momento\n    summary: Optional[str] = None            # Resumen de los últimos mensajes (generado por el grafo)",
        "detail": "projects.A6_memory.schemas",
        "documentation": {}
    },
    {
        "label": "ChatState",
        "kind": 6,
        "importPath": "projects.A6_memory.schemas",
        "description": "projects.A6_memory.schemas",
        "peekOfCode": "class ChatState(BaseModel):\n    user_id: Optional[str] = None            # Identificador del usuario (para memoria en ChromaDB)\n    messages: Optional[List[Message]] = None # Conversación hasta el momento\n    summary: Optional[str] = None            # Resumen de los últimos mensajes (generado por el grafo)\n    meta: Optional[Dict[str, Any]] = None    # Información adicional para seguimiento del flujo (última pregunta, etc.)\n# Esquema del body recibido por el endpoint POST /query.\n# Representa la pregunta del usuario junto con su user_id.\nclass MemoryQuery(BaseModel):\n    user_id: str = Field(..., example=\"fran.aragon\")  \n    question: str = Field(..., example=\"Mi número de cliente era 9843, recuérdalo.\")",
        "detail": "projects.A6_memory.schemas",
        "documentation": {}
    },
    {
        "label": "MemoryQuery",
        "kind": 6,
        "importPath": "projects.A6_memory.schemas",
        "description": "projects.A6_memory.schemas",
        "peekOfCode": "class MemoryQuery(BaseModel):\n    user_id: str = Field(..., example=\"fran.aragon\")  \n    question: str = Field(..., example=\"Mi número de cliente era 9843, recuérdalo.\")\n# Respuesta del endpoint /query.\n# Contiene la respuesta generada por el asistente\n# y qué tipo de memoria fue utilizada (summary, buffer, etc.).\nclass MemoryResponse(BaseModel):\n    answer: str              # Respuesta final del asistente\n    memory_used: List[str]   # Lista de memorias consultadas o generadas durante el proceso\n# Nuevo esquema para el endpoint GET /memory_state/{user_id}",
        "detail": "projects.A6_memory.schemas",
        "documentation": {}
    },
    {
        "label": "MemoryResponse",
        "kind": 6,
        "importPath": "projects.A6_memory.schemas",
        "description": "projects.A6_memory.schemas",
        "peekOfCode": "class MemoryResponse(BaseModel):\n    answer: str              # Respuesta final del asistente\n    memory_used: List[str]   # Lista de memorias consultadas o generadas durante el proceso\n# Nuevo esquema para el endpoint GET /memory_state/{user_id}\nclass MemoryStateResponse(BaseModel):\n    user_id: str               # Identificador del usuario\n    memory: List[str]          # Lista de textos de memoria asociados al usuario\n# Respuesta estándar para endpoints que solo deben indicar éxito,\n# como el borrado de memoria.\nclass EmptyResponse(BaseModel):",
        "detail": "projects.A6_memory.schemas",
        "documentation": {}
    },
    {
        "label": "MemoryStateResponse",
        "kind": 6,
        "importPath": "projects.A6_memory.schemas",
        "description": "projects.A6_memory.schemas",
        "peekOfCode": "class MemoryStateResponse(BaseModel):\n    user_id: str               # Identificador del usuario\n    memory: List[str]          # Lista de textos de memoria asociados al usuario\n# Respuesta estándar para endpoints que solo deben indicar éxito,\n# como el borrado de memoria.\nclass EmptyResponse(BaseModel):\n    ok: bool                 # True si la operación fue exitosa",
        "detail": "projects.A6_memory.schemas",
        "documentation": {}
    },
    {
        "label": "EmptyResponse",
        "kind": 6,
        "importPath": "projects.A6_memory.schemas",
        "description": "projects.A6_memory.schemas",
        "peekOfCode": "class EmptyResponse(BaseModel):\n    ok: bool                 # True si la operación fue exitosa",
        "detail": "projects.A6_memory.schemas",
        "documentation": {}
    },
    {
        "label": "get_field",
        "kind": 2,
        "importPath": "projects.A6_memory.utils",
        "description": "projects.A6_memory.utils",
        "peekOfCode": "def get_field(obj, key):\n    # Si es un diccionario, usar .get() evita excepciones\n    if isinstance(obj, dict):\n        return obj.get(key)\n    # Para objetos (como Pydantic), getattr devuelve None si no existe\n    return getattr(obj, key, None)\n# ======================================================\n# clean_memory_text\n# ------------------------------------------------------\n# Función encargada de filtrar el texto generado por el",
        "detail": "projects.A6_memory.utils",
        "documentation": {}
    },
    {
        "label": "clean_memory_text",
        "kind": 2,
        "importPath": "projects.A6_memory.utils",
        "description": "projects.A6_memory.utils",
        "peekOfCode": "def clean_memory_text(text: str) -> str:\n    \"\"\"\n    Limpia texto devuelto por el LLM para evitar almacenar:\n    - caracteres invisibles (zero-width, BOM, etc.)\n    - cadenas vacías\n    - marcador \"-\" usado para indicar \"no guardar\"\n    - respuestas no informativas (ok, sí, vale…)\n    \"\"\"\n    # Si viene None o vacío desde el LLM → no guardar\n    if not text:",
        "detail": "projects.A6_memory.utils",
        "documentation": {}
    },
    {
        "label": "ROOT_DIR",
        "kind": 5,
        "importPath": "config_base",
        "description": "config_base",
        "peekOfCode": "ROOT_DIR = os.path.abspath(os.path.dirname(__file__))\n# Carpeta global compartida de bases vectoriales persistentes (ChromaDB)\nCHROMA_PATH = os.path.join(ROOT_DIR, \"chroma_db\")\n# Carpeta de proyectos\nPROJECTS_PATH = os.path.join(ROOT_DIR, \"projects\")\n# Carpeta de aplicación común (FastAPI, servicios, utilidades)\nAPP_PATH = os.path.join(ROOT_DIR, \"app\")\n# === Configuración técnica compartida ===\n# Modelo de embeddings por defecto (SentenceTransformers)\nDEFAULT_EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"",
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "kind": 5,
        "importPath": "config_base",
        "description": "config_base",
        "peekOfCode": "CHROMA_PATH = os.path.join(ROOT_DIR, \"chroma_db\")\n# Carpeta de proyectos\nPROJECTS_PATH = os.path.join(ROOT_DIR, \"projects\")\n# Carpeta de aplicación común (FastAPI, servicios, utilidades)\nAPP_PATH = os.path.join(ROOT_DIR, \"app\")\n# === Configuración técnica compartida ===\n# Modelo de embeddings por defecto (SentenceTransformers)\nDEFAULT_EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n# Modelo LLM default y modelo LLM fallback (para OpenRouter / OpenAI compatible)\nDEFAULT_LLM_MODEL = \"openai/gpt-oss-20b:free\"",
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "PROJECTS_PATH",
        "kind": 5,
        "importPath": "config_base",
        "description": "config_base",
        "peekOfCode": "PROJECTS_PATH = os.path.join(ROOT_DIR, \"projects\")\n# Carpeta de aplicación común (FastAPI, servicios, utilidades)\nAPP_PATH = os.path.join(ROOT_DIR, \"app\")\n# === Configuración técnica compartida ===\n# Modelo de embeddings por defecto (SentenceTransformers)\nDEFAULT_EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n# Modelo LLM default y modelo LLM fallback (para OpenRouter / OpenAI compatible)\nDEFAULT_LLM_MODEL = \"openai/gpt-oss-20b:free\"\nFALLBACK_LLM_MODEL = \"nvidia/nemotron-nano-12b-v2-vl:free\"",
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "APP_PATH",
        "kind": 5,
        "importPath": "config_base",
        "description": "config_base",
        "peekOfCode": "APP_PATH = os.path.join(ROOT_DIR, \"app\")\n# === Configuración técnica compartida ===\n# Modelo de embeddings por defecto (SentenceTransformers)\nDEFAULT_EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n# Modelo LLM default y modelo LLM fallback (para OpenRouter / OpenAI compatible)\nDEFAULT_LLM_MODEL = \"openai/gpt-oss-20b:free\"\nFALLBACK_LLM_MODEL = \"nvidia/nemotron-nano-12b-v2-vl:free\"",
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EMBEDDING_MODEL",
        "kind": 5,
        "importPath": "config_base",
        "description": "config_base",
        "peekOfCode": "DEFAULT_EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n# Modelo LLM default y modelo LLM fallback (para OpenRouter / OpenAI compatible)\nDEFAULT_LLM_MODEL = \"openai/gpt-oss-20b:free\"\nFALLBACK_LLM_MODEL = \"nvidia/nemotron-nano-12b-v2-vl:free\"",
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_LLM_MODEL",
        "kind": 5,
        "importPath": "config_base",
        "description": "config_base",
        "peekOfCode": "DEFAULT_LLM_MODEL = \"openai/gpt-oss-20b:free\"\nFALLBACK_LLM_MODEL = \"nvidia/nemotron-nano-12b-v2-vl:free\"",
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "FALLBACK_LLM_MODEL",
        "kind": 5,
        "importPath": "config_base",
        "description": "config_base",
        "peekOfCode": "FALLBACK_LLM_MODEL = \"nvidia/nemotron-nano-12b-v2-vl:free\"",
        "detail": "config_base",
        "documentation": {}
    }
]