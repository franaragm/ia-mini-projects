[
    {
        "label": "AsyncOpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "DEFAULT_LLM_MODEL",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EMBEDDING_MODEL",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EMBEDDING_MODEL",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EMBEDDING_MODEL",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EMBEDDING_MODEL",
        "importPath": "config_base",
        "description": "config_base",
        "isExtraImport": true,
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "projects.A1_chat_structured.router",
        "description": "projects.A1_chat_structured.router",
        "isExtraImport": true,
        "detail": "projects.A1_chat_structured.router",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "projects.A2_output_parser.router",
        "description": "projects.A2_output_parser.router",
        "isExtraImport": true,
        "detail": "projects.A2_output_parser.router",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "projects.A3_rag_basic.router",
        "description": "projects.A3_rag_basic.router",
        "isExtraImport": true,
        "detail": "projects.A3_rag_basic.router",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "projects.A3_rag_basic_v2.router",
        "description": "projects.A3_rag_basic_v2.router",
        "isExtraImport": true,
        "detail": "projects.A3_rag_basic_v2.router",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "projects.A4_rag_advanced.router",
        "description": "projects.A4_rag_advanced.router",
        "isExtraImport": true,
        "detail": "projects.A4_rag_advanced.router",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "llm",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "llm",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "llm",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "llm",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "llm",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "llm",
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "isExtraImport": true,
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "date",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "chromadb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "chromadb",
        "description": "chromadb",
        "detail": "chromadb",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "OPENROUTER_API_KEY",
        "kind": 5,
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "peekOfCode": "OPENROUTER_API_KEY = get_env(\"OPENROUTER_API_KEY\")\nOPENROUTER_BASE_URL = get_env(\"OPENROUTER_BASE_URL\", \"https://openrouter.ai/api/v1\")\n# Cliente compatible con OpenAI, apuntado a OpenRouter\nclient = AsyncOpenAI(\n    api_key=OPENROUTER_API_KEY,\n    base_url=OPENROUTER_BASE_URL,\n)\nasync def llm(prompt: str):\n    response = await client.chat.completions.create(\n        model=DEFAULT_LLM_MODEL,",
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "OPENROUTER_BASE_URL",
        "kind": 5,
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "peekOfCode": "OPENROUTER_BASE_URL = get_env(\"OPENROUTER_BASE_URL\", \"https://openrouter.ai/api/v1\")\n# Cliente compatible con OpenAI, apuntado a OpenRouter\nclient = AsyncOpenAI(\n    api_key=OPENROUTER_API_KEY,\n    base_url=OPENROUTER_BASE_URL,\n)\nasync def llm(prompt: str):\n    response = await client.chat.completions.create(\n        model=DEFAULT_LLM_MODEL,\n        messages=[{\"role\": \"user\", \"content\": prompt}],",
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "app.services.llm_client",
        "description": "app.services.llm_client",
        "peekOfCode": "client = AsyncOpenAI(\n    api_key=OPENROUTER_API_KEY,\n    base_url=OPENROUTER_BASE_URL,\n)\nasync def llm(prompt: str):\n    response = await client.chat.completions.create(\n        model=DEFAULT_LLM_MODEL,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=400,\n        temperature=0.7,",
        "detail": "app.services.llm_client",
        "documentation": {}
    },
    {
        "label": "get_env",
        "kind": 2,
        "importPath": "app.services.utils",
        "description": "app.services.utils",
        "peekOfCode": "def get_env(name: str, default=None):\n    value = os.getenv(name, default)\n    if value is None:\n        raise ValueError(f\"❌ Variable de entorno no encontrada: {name}\")\n    return value",
        "detail": "app.services.utils",
        "documentation": {}
    },
    {
        "label": "ENV",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "ENV = get_env(\"ENV\", \"dev\")  # dev | prod\ndocs_url = \"/docs\" if ENV == \"dev\" else None\nredoc_url = \"/redoc\" if ENV == \"dev\" else None\nopenapi_url = \"/openapi.json\" if ENV == \"dev\" else None\napp = FastAPI(\n    title=\"LangChain Lab - AI Server\",\n    description=\"\"\"\n    Servidor de experimentación con modelos de IA, RAG y agentes.\n    Este backend expone APIs para explorar:\n    - Recuperación aumentada con generación (RAG)",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "docs_url",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "docs_url = \"/docs\" if ENV == \"dev\" else None\nredoc_url = \"/redoc\" if ENV == \"dev\" else None\nopenapi_url = \"/openapi.json\" if ENV == \"dev\" else None\napp = FastAPI(\n    title=\"LangChain Lab - AI Server\",\n    description=\"\"\"\n    Servidor de experimentación con modelos de IA, RAG y agentes.\n    Este backend expone APIs para explorar:\n    - Recuperación aumentada con generación (RAG)\n    - Llamadas a modelos LLM",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "redoc_url",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "redoc_url = \"/redoc\" if ENV == \"dev\" else None\nopenapi_url = \"/openapi.json\" if ENV == \"dev\" else None\napp = FastAPI(\n    title=\"LangChain Lab - AI Server\",\n    description=\"\"\"\n    Servidor de experimentación con modelos de IA, RAG y agentes.\n    Este backend expone APIs para explorar:\n    - Recuperación aumentada con generación (RAG)\n    - Llamadas a modelos LLM\n    - Herramientas generativas",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "openapi_url",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "openapi_url = \"/openapi.json\" if ENV == \"dev\" else None\napp = FastAPI(\n    title=\"LangChain Lab - AI Server\",\n    description=\"\"\"\n    Servidor de experimentación con modelos de IA, RAG y agentes.\n    Este backend expone APIs para explorar:\n    - Recuperación aumentada con generación (RAG)\n    - Llamadas a modelos LLM\n    - Herramientas generativas\n    - Proyectos modulares de IA",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "app = FastAPI(\n    title=\"LangChain Lab - AI Server\",\n    description=\"\"\"\n    Servidor de experimentación con modelos de IA, RAG y agentes.\n    Este backend expone APIs para explorar:\n    - Recuperación aumentada con generación (RAG)\n    - Llamadas a modelos LLM\n    - Herramientas generativas\n    - Proyectos modulares de IA\n    \"\"\",",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "health",
        "kind": 2,
        "importPath": "app.routes",
        "description": "app.routes",
        "peekOfCode": "def health():\n    return {\"status\": \"ok\"}\n@router.get(\"/test-llm\")\nasync def test_llm():\n    answer = await llm(\"Dime una frase corta divertida como un astronauta para confirmar conexión.\")\n    return {\"response\": answer}\n# Rutas de los mini-proyectos\nrouter.include_router(a1_router)\nrouter.include_router(a2_router)\nrouter.include_router(a3_router)",
        "detail": "app.routes",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "app.routes",
        "description": "app.routes",
        "peekOfCode": "router = APIRouter()\n@router.get(\"/health\")\ndef health():\n    return {\"status\": \"ok\"}\n@router.get(\"/test-llm\")\nasync def test_llm():\n    answer = await llm(\"Dime una frase corta divertida como un astronauta para confirmar conexión.\")\n    return {\"response\": answer}\n# Rutas de los mini-proyectos\nrouter.include_router(a1_router)",
        "detail": "app.routes",
        "documentation": {}
    },
    {
        "label": "chat_template",
        "kind": 5,
        "importPath": "projects.A1_chat_structured.prompts",
        "description": "projects.A1_chat_structured.prompts",
        "peekOfCode": "chat_template = PromptTemplate(\n    input_variables=[\"user_message\"],\n    template=\"\"\"\nEres un asistente útil y preciso. Debes responder SIEMPRE en formato JSON válido.\nResponde al usuario manteniendo un tono educativo y claro.\nInstrucciones estrictas:\n- No agregues texto fuera del JSON.\n- No expliques ni describas el JSON, solo devuélvelo.\n- No incluyas comentarios.\nFormato esperado:",
        "detail": "projects.A1_chat_structured.prompts",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "projects.A1_chat_structured.router",
        "description": "projects.A1_chat_structured.router",
        "peekOfCode": "router = APIRouter(prefix=\"/a1\", tags=[\"A1 - Chat estructurado\"])\n@router.post(\n    \"/chat\",\n    summary=\"Chat con respuesta estructurada en JSON\",\n    description=\"\"\"\n    Recibe un mensaje de usuario, lo envía a un modelo de lenguaje y devuelve una respuesta en formato JSON válido.\n    El JSON incluye la respuesta del asistente, el tono y metadatos sobre el modelo utilizado.\n    \"\"\",\n    response_description=\"JSON estructurado con la respuesta del asistente\",\n    response_model=ChatResponse",
        "detail": "projects.A1_chat_structured.router",
        "documentation": {}
    },
    {
        "label": "ChatRequest",
        "kind": 6,
        "importPath": "projects.A1_chat_structured.schemas",
        "description": "projects.A1_chat_structured.schemas",
        "peekOfCode": "class ChatRequest(BaseModel):\n    message: str = Field(..., example=\"Define que es un chatbot estructurado\")\nclass ResponseMetadata(BaseModel):\n    model: str = Field(..., description=\"Nombre del modelo usado para generar la respuesta\")\nclass ChatResponse(BaseModel):\n    answer: str = Field(..., description=\"Texto final que entregamos al usuario\")\n    tone: str = Field(..., description=\"Tono de la respuesta (por ejemplo: educational, friendly, formal, etc.)\")\n    metadata: ResponseMetadata = Field(..., description=\"Información adicional sobre la generación\")",
        "detail": "projects.A1_chat_structured.schemas",
        "documentation": {}
    },
    {
        "label": "ResponseMetadata",
        "kind": 6,
        "importPath": "projects.A1_chat_structured.schemas",
        "description": "projects.A1_chat_structured.schemas",
        "peekOfCode": "class ResponseMetadata(BaseModel):\n    model: str = Field(..., description=\"Nombre del modelo usado para generar la respuesta\")\nclass ChatResponse(BaseModel):\n    answer: str = Field(..., description=\"Texto final que entregamos al usuario\")\n    tone: str = Field(..., description=\"Tono de la respuesta (por ejemplo: educational, friendly, formal, etc.)\")\n    metadata: ResponseMetadata = Field(..., description=\"Información adicional sobre la generación\")",
        "detail": "projects.A1_chat_structured.schemas",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "kind": 6,
        "importPath": "projects.A1_chat_structured.schemas",
        "description": "projects.A1_chat_structured.schemas",
        "peekOfCode": "class ChatResponse(BaseModel):\n    answer: str = Field(..., description=\"Texto final que entregamos al usuario\")\n    tone: str = Field(..., description=\"Tono de la respuesta (por ejemplo: educational, friendly, formal, etc.)\")\n    metadata: ResponseMetadata = Field(..., description=\"Información adicional sobre la generación\")",
        "detail": "projects.A1_chat_structured.schemas",
        "documentation": {}
    },
    {
        "label": "today",
        "kind": 5,
        "importPath": "projects.A2_output_parser.prompts",
        "description": "projects.A2_output_parser.prompts",
        "peekOfCode": "today = date.today().strftime(\"%Y-%m-%d\")\nintent_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente que analiza mensajes de usuario y devuelve un JSON con intención estructurada.\nLa fecha actual es: {today}\nFormato esperado del JSON (usa exactamente este formato, sin texto fuera del JSON):\n{{\n  \"action\": \"create_task | update_task | get_status | other\",\n  \"title\": \"texto o null\",\n  \"due_date\": \"YYYY-MM-DD o null\"\n}}",
        "detail": "projects.A2_output_parser.prompts",
        "documentation": {}
    },
    {
        "label": "intent_prompt",
        "kind": 5,
        "importPath": "projects.A2_output_parser.prompts",
        "description": "projects.A2_output_parser.prompts",
        "peekOfCode": "intent_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente que analiza mensajes de usuario y devuelve un JSON con intención estructurada.\nLa fecha actual es: {today}\nFormato esperado del JSON (usa exactamente este formato, sin texto fuera del JSON):\n{{\n  \"action\": \"create_task | update_task | get_status | other\",\n  \"title\": \"texto o null\",\n  \"due_date\": \"YYYY-MM-DD o null\"\n}}\nInstrucciones:",
        "detail": "projects.A2_output_parser.prompts",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "projects.A2_output_parser.router",
        "description": "projects.A2_output_parser.router",
        "peekOfCode": "router = APIRouter(prefix=\"/a2\", tags=[\"A2 - Output Parser & Validación\"])\n@router.post(\n    \"/parse-intent\",\n    summary=\"Analiza el mensaje del usuario y devuelve la intención en un JSON validado\",\n    description=\"\"\"\n    Recibe un mensaje de usuario, lo envía a un modelo de lenguaje para analizar la intención y devuelve un JSON estructurado y validado con Pydantic.\n    Se puede usar para crear tareas, actualizar tareas o consultar el estado de tareas.\n    Si se indica una fecha relativa, se calcula en base a la fecha actual.\n    \"\"\",\n    response_description=\"JSON estructurado con la respuesta del asistente\",",
        "detail": "projects.A2_output_parser.router",
        "documentation": {}
    },
    {
        "label": "IntentRequest",
        "kind": 6,
        "importPath": "projects.A2_output_parser.schemas",
        "description": "projects.A2_output_parser.schemas",
        "peekOfCode": "class IntentRequest(BaseModel):\n    message: str = Field(..., example=\"Crea una tarea llamada Preparar informe para mañana\")\nclass IntentResponse(BaseModel):\n    action: str = Field(..., description=\"Tipo de acción que el usuario quiere realizar\")\n    title: str | None = Field(None, description=\"Título si aplica (por ejemplo crear tarea)\")\n    due_date: str | None = Field(None, description=\"Fecha en formato YYYY-MM-DD si aplica\")",
        "detail": "projects.A2_output_parser.schemas",
        "documentation": {}
    },
    {
        "label": "IntentResponse",
        "kind": 6,
        "importPath": "projects.A2_output_parser.schemas",
        "description": "projects.A2_output_parser.schemas",
        "peekOfCode": "class IntentResponse(BaseModel):\n    action: str = Field(..., description=\"Tipo de acción que el usuario quiere realizar\")\n    title: str | None = Field(None, description=\"Título si aplica (por ejemplo crear tarea)\")\n    due_date: str | None = Field(None, description=\"Fecha en formato YYYY-MM-DD si aplica\")",
        "detail": "projects.A2_output_parser.schemas",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.chroma_client",
        "description": "projects.A3_rag_basic.chroma_client",
        "peekOfCode": "client = chromadb.PersistentClient(path=CHROMA_PATH)\ncollection = client.get_or_create_collection(COLLECTION_NAME)",
        "detail": "projects.A3_rag_basic.chroma_client",
        "documentation": {}
    },
    {
        "label": "collection",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.chroma_client",
        "description": "projects.A3_rag_basic.chroma_client",
        "peekOfCode": "collection = client.get_or_create_collection(COLLECTION_NAME)",
        "detail": "projects.A3_rag_basic.chroma_client",
        "documentation": {}
    },
    {
        "label": "COLLECTION_NAME",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.config",
        "description": "projects.A3_rag_basic.config",
        "peekOfCode": "COLLECTION_NAME = \"a3_docs\"\nEMBEDDING_MODEL = DEFAULT_EMBEDDING_MODEL\nCHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A3_rag_basic.config",
        "documentation": {}
    },
    {
        "label": "EMBEDDING_MODEL",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.config",
        "description": "projects.A3_rag_basic.config",
        "peekOfCode": "EMBEDDING_MODEL = DEFAULT_EMBEDDING_MODEL\nCHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A3_rag_basic.config",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.config",
        "description": "projects.A3_rag_basic.config",
        "peekOfCode": "CHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A3_rag_basic.config",
        "documentation": {}
    },
    {
        "label": "load_documents",
        "kind": 2,
        "importPath": "projects.A3_rag_basic.loader",
        "description": "projects.A3_rag_basic.loader",
        "peekOfCode": "def load_documents(data_path: str):\n    docs = []\n    for file in os.listdir(data_path):\n        full_path = os.path.join(data_path, file)\n        if file.endswith(\".txt\"):\n            with open(full_path, \"r\", encoding=\"utf-8\") as f:\n                docs.append(f.read())\n    return docs",
        "detail": "projects.A3_rag_basic.loader",
        "documentation": {}
    },
    {
        "label": "rag_prompt",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.prompts",
        "description": "projects.A3_rag_basic.prompts",
        "peekOfCode": "rag_prompt = PromptTemplate.from_template(\"\"\"\nUtiliza únicamente la información proporcionada para responder al usuario.\nSi no encuentras la respuesta en los documentos, responde:\n\"No encuentro esa información en los documentos.\"\nDocumentos relevantes:\n{context}\nPregunta del usuario:\n{question}\n\"\"\")",
        "detail": "projects.A3_rag_basic.prompts",
        "documentation": {}
    },
    {
        "label": "build_index",
        "kind": 2,
        "importPath": "projects.A3_rag_basic.rag",
        "description": "projects.A3_rag_basic.rag",
        "peekOfCode": "def build_index(documents):\n    vectors = model.encode(documents).tolist() # Convierte los documentos a vectores\n    ids = [f\"doc_{i}\" for i in range(len(documents))] # Genera IDs únicas para cada documento\n    collection.add(documents=documents, embeddings=vectors, ids=ids) # Añade los documentos y sus vectores a la colección\ndef retrieve(question: str):\n    query_vec = model.encode([question]).tolist()[0] # Convierte la pregunta a vector\n    results = collection.query(query_embeddings=[query_vec], n_results=3) # Recupera los 3 documentos más similares\n    return results[\"documents\"][0] # Devuelve los documentos recuperados",
        "detail": "projects.A3_rag_basic.rag",
        "documentation": {}
    },
    {
        "label": "retrieve",
        "kind": 2,
        "importPath": "projects.A3_rag_basic.rag",
        "description": "projects.A3_rag_basic.rag",
        "peekOfCode": "def retrieve(question: str):\n    query_vec = model.encode([question]).tolist()[0] # Convierte la pregunta a vector\n    results = collection.query(query_embeddings=[query_vec], n_results=3) # Recupera los 3 documentos más similares\n    return results[\"documents\"][0] # Devuelve los documentos recuperados",
        "detail": "projects.A3_rag_basic.rag",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.rag",
        "description": "projects.A3_rag_basic.rag",
        "peekOfCode": "model = SentenceTransformer(EMBEDDING_MODEL) # Modelo ligero para embeddings\ndef build_index(documents):\n    vectors = model.encode(documents).tolist() # Convierte los documentos a vectores\n    ids = [f\"doc_{i}\" for i in range(len(documents))] # Genera IDs únicas para cada documento\n    collection.add(documents=documents, embeddings=vectors, ids=ids) # Añade los documentos y sus vectores a la colección\ndef retrieve(question: str):\n    query_vec = model.encode([question]).tolist()[0] # Convierte la pregunta a vector\n    results = collection.query(query_embeddings=[query_vec], n_results=3) # Recupera los 3 documentos más similares\n    return results[\"documents\"][0] # Devuelve los documentos recuperados",
        "detail": "projects.A3_rag_basic.rag",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.router",
        "description": "projects.A3_rag_basic.router",
        "peekOfCode": "router = APIRouter(prefix=\"/a3\", tags=[\"A3 - RAG Básico\"])\nDATA_PATH = os.path.join(os.path.dirname(__file__), \"data\")\n# Construye el índice on startup:\ndocuments = load_documents(DATA_PATH)\nbuild_index(documents)\n@router.post(\n    \"/ask\",\n    summary=\"Realiza una pregunta utilizando RAG básico\",\n    description=\"\"\"\n    Realiza una pregunta utilizando un enfoque de Recuperación Augmentada por Generación (RAG) básico.",
        "detail": "projects.A3_rag_basic.router",
        "documentation": {}
    },
    {
        "label": "DATA_PATH",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.router",
        "description": "projects.A3_rag_basic.router",
        "peekOfCode": "DATA_PATH = os.path.join(os.path.dirname(__file__), \"data\")\n# Construye el índice on startup:\ndocuments = load_documents(DATA_PATH)\nbuild_index(documents)\n@router.post(\n    \"/ask\",\n    summary=\"Realiza una pregunta utilizando RAG básico\",\n    description=\"\"\"\n    Realiza una pregunta utilizando un enfoque de Recuperación Augmentada por Generación (RAG) básico.\n    1. Recupera documentos relevantes basados en la pregunta del usuario.",
        "detail": "projects.A3_rag_basic.router",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 5,
        "importPath": "projects.A3_rag_basic.router",
        "description": "projects.A3_rag_basic.router",
        "peekOfCode": "documents = load_documents(DATA_PATH)\nbuild_index(documents)\n@router.post(\n    \"/ask\",\n    summary=\"Realiza una pregunta utilizando RAG básico\",\n    description=\"\"\"\n    Realiza una pregunta utilizando un enfoque de Recuperación Augmentada por Generación (RAG) básico.\n    1. Recupera documentos relevantes basados en la pregunta del usuario.\n    2. Usa un modelo de lenguaje para generar una respuesta basada en esos documentos.\n    \"\"\",",
        "detail": "projects.A3_rag_basic.router",
        "documentation": {}
    },
    {
        "label": "QueryRequest",
        "kind": 6,
        "importPath": "projects.A3_rag_basic.schemas",
        "description": "projects.A3_rag_basic.schemas",
        "peekOfCode": "class QueryRequest(BaseModel):\n    question: str = Field(..., example=\"Qué es la inteligencia artificial?\")\nclass QueryResponse(BaseModel):\n    response: str = Field(..., description=\"Respuesta generada por el modelo LLM\")\n    sources: list[str] = Field(..., description=\"Documentos utilizados para generar la respuesta\")",
        "detail": "projects.A3_rag_basic.schemas",
        "documentation": {}
    },
    {
        "label": "QueryResponse",
        "kind": 6,
        "importPath": "projects.A3_rag_basic.schemas",
        "description": "projects.A3_rag_basic.schemas",
        "peekOfCode": "class QueryResponse(BaseModel):\n    response: str = Field(..., description=\"Respuesta generada por el modelo LLM\")\n    sources: list[str] = Field(..., description=\"Documentos utilizados para generar la respuesta\")",
        "detail": "projects.A3_rag_basic.schemas",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.chroma_client",
        "description": "projects.A3_rag_basic_v2.chroma_client",
        "peekOfCode": "client = chromadb.PersistentClient(path=CHROMA_PATH)\ncollection = client.get_or_create_collection(COLLECTION_NAME)",
        "detail": "projects.A3_rag_basic_v2.chroma_client",
        "documentation": {}
    },
    {
        "label": "collection",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.chroma_client",
        "description": "projects.A3_rag_basic_v2.chroma_client",
        "peekOfCode": "collection = client.get_or_create_collection(COLLECTION_NAME)",
        "detail": "projects.A3_rag_basic_v2.chroma_client",
        "documentation": {}
    },
    {
        "label": "COLLECTION_NAME",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.config",
        "description": "projects.A3_rag_basic_v2.config",
        "peekOfCode": "COLLECTION_NAME = \"a3_docs_v2\"\nEMBEDDING_MODEL = DEFAULT_EMBEDDING_MODEL\nCHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A3_rag_basic_v2.config",
        "documentation": {}
    },
    {
        "label": "EMBEDDING_MODEL",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.config",
        "description": "projects.A3_rag_basic_v2.config",
        "peekOfCode": "EMBEDDING_MODEL = DEFAULT_EMBEDDING_MODEL\nCHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A3_rag_basic_v2.config",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.config",
        "description": "projects.A3_rag_basic_v2.config",
        "peekOfCode": "CHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A3_rag_basic_v2.config",
        "documentation": {}
    },
    {
        "label": "rag_prompt",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.prompts",
        "description": "projects.A3_rag_basic_v2.prompts",
        "peekOfCode": "rag_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente que responde usando la información de contexto proporcionada.\nUsa **solo** la información del contexto si es relevante.\nSi no hay suficiente información, indica que no puedes responder con precisión.\n### Contexto:\n{context}\n### Pregunta:\n{question}\nDevuelve un JSON con este formato:\n{{",
        "detail": "projects.A3_rag_basic_v2.prompts",
        "documentation": {}
    },
    {
        "label": "build_index_from_folder",
        "kind": 2,
        "importPath": "projects.A3_rag_basic_v2.rag",
        "description": "projects.A3_rag_basic_v2.rag",
        "peekOfCode": "def build_index_from_folder(folder_path: str):\n    docs = []\n    ids = []\n    for fname in os.listdir(folder_path):\n        if not fname.endswith((\".txt\", \".md\")):\n            continue\n        with open(os.path.join(folder_path, fname), \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n        # Fragmentar en chunks de 400 caracteres\n        chunks = [content[i:i+400] for i in range(0, len(content), 400)]",
        "detail": "projects.A3_rag_basic_v2.rag",
        "documentation": {}
    },
    {
        "label": "retrieve",
        "kind": 2,
        "importPath": "projects.A3_rag_basic_v2.rag",
        "description": "projects.A3_rag_basic_v2.rag",
        "peekOfCode": "def retrieve(question: str, top_k: int = 3):\n    query_vec = model.encode([question]).tolist()[0]\n    results = collection.query(query_embeddings=[query_vec], n_results=top_k)\n    documents = results.get(\"documents\", [[]])[0]\n    return documents",
        "detail": "projects.A3_rag_basic_v2.rag",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.rag",
        "description": "projects.A3_rag_basic_v2.rag",
        "peekOfCode": "model = SentenceTransformer(EMBEDDING_MODEL)\n# Construye el índice desde una carpeta de documentos\ndef build_index_from_folder(folder_path: str):\n    docs = []\n    ids = []\n    for fname in os.listdir(folder_path):\n        if not fname.endswith((\".txt\", \".md\")):\n            continue\n        with open(os.path.join(folder_path, fname), \"r\", encoding=\"utf-8\") as f:\n            content = f.read()",
        "detail": "projects.A3_rag_basic_v2.rag",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.router",
        "description": "projects.A3_rag_basic_v2.router",
        "peekOfCode": "router = APIRouter(prefix=\"/a3v2\", tags=[\"A3 - RAG Básico v2\"])\nDATA_PATH = os.path.join(os.path.dirname(__file__), \"data\")\n# Indexado automático en segundo plano al cargar el módulo \ndef _auto_build_index():\n    try:\n        build_index_from_folder(DATA_PATH)\n    except Exception as e:\n        print(f\"[RAG] Error durante el indexado automático: {e}\")\n# Lanzamos el indexado en un hilo para no bloquear FastAPI al arrancar\nthreading.Thread(target=_auto_build_index, daemon=True).start()",
        "detail": "projects.A3_rag_basic_v2.router",
        "documentation": {}
    },
    {
        "label": "DATA_PATH",
        "kind": 5,
        "importPath": "projects.A3_rag_basic_v2.router",
        "description": "projects.A3_rag_basic_v2.router",
        "peekOfCode": "DATA_PATH = os.path.join(os.path.dirname(__file__), \"data\")\n# Indexado automático en segundo plano al cargar el módulo \ndef _auto_build_index():\n    try:\n        build_index_from_folder(DATA_PATH)\n    except Exception as e:\n        print(f\"[RAG] Error durante el indexado automático: {e}\")\n# Lanzamos el indexado en un hilo para no bloquear FastAPI al arrancar\nthreading.Thread(target=_auto_build_index, daemon=True).start()\n@router.post(",
        "detail": "projects.A3_rag_basic_v2.router",
        "documentation": {}
    },
    {
        "label": "QueryRequest",
        "kind": 6,
        "importPath": "projects.A3_rag_basic_v2.schemas",
        "description": "projects.A3_rag_basic_v2.schemas",
        "peekOfCode": "class QueryRequest(BaseModel):\n    question: str = Field(..., example=\"Qué es LangChain?\")\nclass QueryResponse(BaseModel):\n    answer: str = Field(..., description=\"Respuesta generada por el modelo LLM\")\n    sources: list[str] = Field(..., description=\"Documentos utilizados para generar la respuesta\")",
        "detail": "projects.A3_rag_basic_v2.schemas",
        "documentation": {}
    },
    {
        "label": "QueryResponse",
        "kind": 6,
        "importPath": "projects.A3_rag_basic_v2.schemas",
        "description": "projects.A3_rag_basic_v2.schemas",
        "peekOfCode": "class QueryResponse(BaseModel):\n    answer: str = Field(..., description=\"Respuesta generada por el modelo LLM\")\n    sources: list[str] = Field(..., description=\"Documentos utilizados para generar la respuesta\")",
        "detail": "projects.A3_rag_basic_v2.schemas",
        "documentation": {}
    },
    {
        "label": "hash_text",
        "kind": 2,
        "importPath": "projects.A3_rag_basic_v2.utils",
        "description": "projects.A3_rag_basic_v2.utils",
        "peekOfCode": "def hash_text(text: str) -> str:\n    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\ndef safe_json_parse(text: str):\n    try:\n        data = json.loads(text)\n    except json.JSONDecodeError:\n        return {\"answer\": text.strip(), \"sources\": []}\n    # Validamos estructura con Pydantic\n    try:\n        parsed = QueryResponse(**data)",
        "detail": "projects.A3_rag_basic_v2.utils",
        "documentation": {}
    },
    {
        "label": "safe_json_parse",
        "kind": 2,
        "importPath": "projects.A3_rag_basic_v2.utils",
        "description": "projects.A3_rag_basic_v2.utils",
        "peekOfCode": "def safe_json_parse(text: str):\n    try:\n        data = json.loads(text)\n    except json.JSONDecodeError:\n        return {\"answer\": text.strip(), \"sources\": []}\n    # Validamos estructura con Pydantic\n    try:\n        parsed = QueryResponse(**data)\n    except Exception as e:\n        return {",
        "detail": "projects.A3_rag_basic_v2.utils",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced.chroma_client",
        "description": "projects.A4_rag_advanced.chroma_client",
        "peekOfCode": "client = chromadb.PersistentClient(path=CHROMA_PATH)\ncollection = client.get_or_create_collection(COLLECTION_NAME)",
        "detail": "projects.A4_rag_advanced.chroma_client",
        "documentation": {}
    },
    {
        "label": "collection",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced.chroma_client",
        "description": "projects.A4_rag_advanced.chroma_client",
        "peekOfCode": "collection = client.get_or_create_collection(COLLECTION_NAME)",
        "detail": "projects.A4_rag_advanced.chroma_client",
        "documentation": {}
    },
    {
        "label": "COLLECTION_NAME",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced.config",
        "description": "projects.A4_rag_advanced.config",
        "peekOfCode": "COLLECTION_NAME = \"a4_docs\"\nEMBEDDING_MODEL = DEFAULT_EMBEDDING_MODEL\nCHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A4_rag_advanced.config",
        "documentation": {}
    },
    {
        "label": "EMBEDDING_MODEL",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced.config",
        "description": "projects.A4_rag_advanced.config",
        "peekOfCode": "EMBEDDING_MODEL = DEFAULT_EMBEDDING_MODEL\nCHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A4_rag_advanced.config",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced.config",
        "description": "projects.A4_rag_advanced.config",
        "peekOfCode": "CHROMA_PATH = CHROMA_PATH",
        "detail": "projects.A4_rag_advanced.config",
        "documentation": {}
    },
    {
        "label": "load_documents",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced.loader",
        "description": "projects.A4_rag_advanced.loader",
        "peekOfCode": "def load_documents():\n    docs = []\n    for file in os.listdir(DATA_PATH):\n        if file.endswith((\".txt\", \".md\")):\n            loader = TextLoader(os.path.join(DATA_PATH, file), encoding=\"utf-8\") # Crear cargador de texto\n            docs.extend(loader.load()) # Cargar y agregar documentos\n    return docs\n# Divide documentos en chunks con solapamiento\ndef split_documents(documents, chunk_size=600, chunk_overlap=100):\n    splitter = RecursiveCharacterTextSplitter(",
        "detail": "projects.A4_rag_advanced.loader",
        "documentation": {}
    },
    {
        "label": "split_documents",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced.loader",
        "description": "projects.A4_rag_advanced.loader",
        "peekOfCode": "def split_documents(documents, chunk_size=600, chunk_overlap=100):\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \"]\n    )\n    return splitter.split_documents(documents)",
        "detail": "projects.A4_rag_advanced.loader",
        "documentation": {}
    },
    {
        "label": "DATA_PATH",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced.loader",
        "description": "projects.A4_rag_advanced.loader",
        "peekOfCode": "DATA_PATH = os.path.join(os.path.dirname(__file__), \"data\")\n# Carga todos los documentos TXT y MD desde /data/\ndef load_documents():\n    docs = []\n    for file in os.listdir(DATA_PATH):\n        if file.endswith((\".txt\", \".md\")):\n            loader = TextLoader(os.path.join(DATA_PATH, file), encoding=\"utf-8\") # Crear cargador de texto\n            docs.extend(loader.load()) # Cargar y agregar documentos\n    return docs\n# Divide documentos en chunks con solapamiento",
        "detail": "projects.A4_rag_advanced.loader",
        "documentation": {}
    },
    {
        "label": "rag_prompt",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced.prompts",
        "description": "projects.A4_rag_advanced.prompts",
        "peekOfCode": "rag_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente experto en IA. Usa el siguiente contexto recuperado de los documentos\npara responder a la pregunta del usuario de manera precisa, concisa y verificada.\nNo inventes información. Si no sabes algo, di \"No tengo información suficiente en los documentos\".\n=== CONTEXTO ===\n{context}\n=== PREGUNTA ===\n{question}\n\"\"\")",
        "detail": "projects.A4_rag_advanced.prompts",
        "documentation": {}
    },
    {
        "label": "build_vectorstore",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced.rag",
        "description": "projects.A4_rag_advanced.rag",
        "peekOfCode": "def build_vectorstore():\n    print(f\"Construyendo colección persistente '{COLLECTION_NAME}'...\")\n    # Cargar y dividir documentos\n    raw_chunks = split_documents(load_documents())\n    if not raw_chunks:\n        print(\"No se encontraron documentos para procesar.\")\n        return collection\n    # Lista de nuevos chunks a indexar\n    new_chunks = []\n    # Recorrer raw_chunks que es un list[Document] con documentos fragmentados y verificar duplicados, ",
        "detail": "projects.A4_rag_advanced.rag",
        "documentation": {}
    },
    {
        "label": "retrieve_context",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced.rag",
        "description": "projects.A4_rag_advanced.rag",
        "peekOfCode": "def retrieve_context(question: str, n_results: int = 3):\n    # Convertir pregunta → embedding\n    query_vec = model.encode([question]).tolist()[0]\n    # Consultar la colección en ChromaDB\n    results = collection.query(\n        query_embeddings=[query_vec],\n        n_results=n_results\n    )\n    # Extraer documentos y metadatos (si no existen, usar listas vacías), distancias para posibles futuros usos\n    retrieved_docs = results.get(\"documents\", [[]])[0]",
        "detail": "projects.A4_rag_advanced.rag",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced.rag",
        "description": "projects.A4_rag_advanced.rag",
        "peekOfCode": "model = SentenceTransformer(EMBEDDING_MODEL)\n# ==========================================================\n# Construcción del índice (siempre se reconstruye si hay nuevos documentos)\n# ==========================================================\n# Crea embeddings y guarda documentos nuevos en la colección persistente.\ndef build_vectorstore():\n    print(f\"Construyendo colección persistente '{COLLECTION_NAME}'...\")\n    # Cargar y dividir documentos\n    raw_chunks = split_documents(load_documents())\n    if not raw_chunks:",
        "detail": "projects.A4_rag_advanced.rag",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced.router",
        "description": "projects.A4_rag_advanced.router",
        "peekOfCode": "router = APIRouter(prefix=\"/a4\", tags=[\"A4 - RAG Avanzado\"])\n# Lanzamos indexado en segundo plano al importar el router\ndef _auto_build_index():\n    try:\n        print(\"Construyendo índice RAG avanzado en background...\")\n        build_vectorstore()\n        print(\"Índice RAG avanzado listo.\")\n    except Exception as e:\n        print(f\"[RAG] Error durante el indexado automático: {e}\")\nthreading.Thread(target=_auto_build_index, daemon=True).start()",
        "detail": "projects.A4_rag_advanced.router",
        "documentation": {}
    },
    {
        "label": "QueryRequest",
        "kind": 6,
        "importPath": "projects.A4_rag_advanced.schemas",
        "description": "projects.A4_rag_advanced.schemas",
        "peekOfCode": "class QueryRequest(BaseModel):\n    question: str = Field(..., example=\"Qué son las Redes Neuronales Convolucionales?\")\nclass SourceDocument(BaseModel):\n    source: str = Field(..., description=\"Ruta o nombre del documento de origen\")\n    score: float = Field(..., description=\"Similitud o relevancia del documento recuperado\")\nclass QueryResponse(BaseModel):\n    answer: str = Field(..., description=\"Respuesta generada por el modelo\")\n    sources: list[SourceDocument] = Field(..., description=\"Documentos usados para generar la respuesta\")",
        "detail": "projects.A4_rag_advanced.schemas",
        "documentation": {}
    },
    {
        "label": "SourceDocument",
        "kind": 6,
        "importPath": "projects.A4_rag_advanced.schemas",
        "description": "projects.A4_rag_advanced.schemas",
        "peekOfCode": "class SourceDocument(BaseModel):\n    source: str = Field(..., description=\"Ruta o nombre del documento de origen\")\n    score: float = Field(..., description=\"Similitud o relevancia del documento recuperado\")\nclass QueryResponse(BaseModel):\n    answer: str = Field(..., description=\"Respuesta generada por el modelo\")\n    sources: list[SourceDocument] = Field(..., description=\"Documentos usados para generar la respuesta\")",
        "detail": "projects.A4_rag_advanced.schemas",
        "documentation": {}
    },
    {
        "label": "QueryResponse",
        "kind": 6,
        "importPath": "projects.A4_rag_advanced.schemas",
        "description": "projects.A4_rag_advanced.schemas",
        "peekOfCode": "class QueryResponse(BaseModel):\n    answer: str = Field(..., description=\"Respuesta generada por el modelo\")\n    sources: list[SourceDocument] = Field(..., description=\"Documentos usados para generar la respuesta\")",
        "detail": "projects.A4_rag_advanced.schemas",
        "documentation": {}
    },
    {
        "label": "hash_text",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced.utils",
        "description": "projects.A4_rag_advanced.utils",
        "peekOfCode": "def hash_text(text: str) -> str:\n    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n# Comprueba si un chunk ya está almacenado en la colección.\ndef is_chunk_indexed(chunk_id: str) -> bool:\n    try:\n        existing = collection.get(ids=[chunk_id])\n        return bool(existing and existing[\"ids\"])\n    except Exception:\n        return False\n# Convierte metadatos y distancias en una lista de fuentes formateadas.",
        "detail": "projects.A4_rag_advanced.utils",
        "documentation": {}
    },
    {
        "label": "is_chunk_indexed",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced.utils",
        "description": "projects.A4_rag_advanced.utils",
        "peekOfCode": "def is_chunk_indexed(chunk_id: str) -> bool:\n    try:\n        existing = collection.get(ids=[chunk_id])\n        return bool(existing and existing[\"ids\"])\n    except Exception:\n        return False\n# Convierte metadatos y distancias en una lista de fuentes formateadas.\ndef format_sources(metadatas: list, distances: list) -> list:\n    formatted = []\n    for meta, dist in zip(metadatas, distances):",
        "detail": "projects.A4_rag_advanced.utils",
        "documentation": {}
    },
    {
        "label": "format_sources",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced.utils",
        "description": "projects.A4_rag_advanced.utils",
        "peekOfCode": "def format_sources(metadatas: list, distances: list) -> list:\n    formatted = []\n    for meta, dist in zip(metadatas, distances):\n        # Convertir distancia a similitud (asumiendo distancia >= 0)\n        similarity = 1 / (1 + dist)\n        formatted.append({\n            \"source\": meta.get(\"source\", \"desconocido\"),\n            \"score\": round(similarity, 4) # Redondear a 4 decimales\n        })\n    return formatted",
        "detail": "projects.A4_rag_advanced.utils",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.chroma_client",
        "description": "projects.A4_rag_advanced_v2.chroma_client",
        "peekOfCode": "client = chromadb.PersistentClient(path=CHROMA_PATH)\ncollection = client.get_or_create_collection(COLLECTION_NAME)",
        "detail": "projects.A4_rag_advanced_v2.chroma_client",
        "documentation": {}
    },
    {
        "label": "collection",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.chroma_client",
        "description": "projects.A4_rag_advanced_v2.chroma_client",
        "peekOfCode": "collection = client.get_or_create_collection(COLLECTION_NAME)",
        "detail": "projects.A4_rag_advanced_v2.chroma_client",
        "documentation": {}
    },
    {
        "label": "COLLECTION_NAME",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.config",
        "description": "projects.A4_rag_advanced_v2.config",
        "peekOfCode": "COLLECTION_NAME = \"a4_docs_v2\"\nEMBEDDING_MODEL = DEFAULT_EMBEDDING_MODEL\nCHROMA_PATH = CHROMA_PATH\nURLS_TO_SCRAPE = [\n    \"https://es.wikipedia.org/wiki/Web_scraping\",\n    \"https://es.wikipedia.org/wiki/Base_de_datos_de_vectores\",\n    # Añade más URLs según sea necesario\n]",
        "detail": "projects.A4_rag_advanced_v2.config",
        "documentation": {}
    },
    {
        "label": "EMBEDDING_MODEL",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.config",
        "description": "projects.A4_rag_advanced_v2.config",
        "peekOfCode": "EMBEDDING_MODEL = DEFAULT_EMBEDDING_MODEL\nCHROMA_PATH = CHROMA_PATH\nURLS_TO_SCRAPE = [\n    \"https://es.wikipedia.org/wiki/Web_scraping\",\n    \"https://es.wikipedia.org/wiki/Base_de_datos_de_vectores\",\n    # Añade más URLs según sea necesario\n]",
        "detail": "projects.A4_rag_advanced_v2.config",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.config",
        "description": "projects.A4_rag_advanced_v2.config",
        "peekOfCode": "CHROMA_PATH = CHROMA_PATH\nURLS_TO_SCRAPE = [\n    \"https://es.wikipedia.org/wiki/Web_scraping\",\n    \"https://es.wikipedia.org/wiki/Base_de_datos_de_vectores\",\n    # Añade más URLs según sea necesario\n]",
        "detail": "projects.A4_rag_advanced_v2.config",
        "documentation": {}
    },
    {
        "label": "URLS_TO_SCRAPE",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.config",
        "description": "projects.A4_rag_advanced_v2.config",
        "peekOfCode": "URLS_TO_SCRAPE = [\n    \"https://es.wikipedia.org/wiki/Web_scraping\",\n    \"https://es.wikipedia.org/wiki/Base_de_datos_de_vectores\",\n    # Añade más URLs según sea necesario\n]",
        "detail": "projects.A4_rag_advanced_v2.config",
        "documentation": {}
    },
    {
        "label": "load_documents",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced_v2.loader",
        "description": "projects.A4_rag_advanced_v2.loader",
        "peekOfCode": "def load_documents():\n    docs = []\n    for file in os.listdir(DATA_PATH):\n        if file.endswith((\".txt\", \".md\")):\n            loader = TextLoader(os.path.join(DATA_PATH, file), encoding=\"utf-8\") # Crear cargador de texto\n            docs.extend(loader.load()) # Cargar y agregar documentos\n    return docs\n# Divide documentos en chunks con solapamiento\ndef split_documents(documents, chunk_size=600, chunk_overlap=100):\n    splitter = RecursiveCharacterTextSplitter(",
        "detail": "projects.A4_rag_advanced_v2.loader",
        "documentation": {}
    },
    {
        "label": "split_documents",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced_v2.loader",
        "description": "projects.A4_rag_advanced_v2.loader",
        "peekOfCode": "def split_documents(documents, chunk_size=600, chunk_overlap=100):\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \"]\n    )\n    return splitter.split_documents(documents)",
        "detail": "projects.A4_rag_advanced_v2.loader",
        "documentation": {}
    },
    {
        "label": "DATA_PATH",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.loader",
        "description": "projects.A4_rag_advanced_v2.loader",
        "peekOfCode": "DATA_PATH = os.path.join(os.path.dirname(__file__), \"data\")\n# Carga todos los documentos TXT y MD desde /data/\ndef load_documents():\n    docs = []\n    for file in os.listdir(DATA_PATH):\n        if file.endswith((\".txt\", \".md\")):\n            loader = TextLoader(os.path.join(DATA_PATH, file), encoding=\"utf-8\") # Crear cargador de texto\n            docs.extend(loader.load()) # Cargar y agregar documentos\n    return docs\n# Divide documentos en chunks con solapamiento",
        "detail": "projects.A4_rag_advanced_v2.loader",
        "documentation": {}
    },
    {
        "label": "rag_prompt",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.prompts",
        "description": "projects.A4_rag_advanced_v2.prompts",
        "peekOfCode": "rag_prompt = PromptTemplate.from_template(\"\"\"\nEres un asistente experto en IA. Usa el siguiente contexto recuperado de los documentos\npara responder a la pregunta del usuario de manera precisa, concisa y verificada.\nNo inventes información. Si no sabes algo, di \"No tengo información suficiente en los documentos\".\n=== CONTEXTO ===\n{context}\n=== PREGUNTA ===\n{question}\n\"\"\")",
        "detail": "projects.A4_rag_advanced_v2.prompts",
        "documentation": {}
    },
    {
        "label": "build_vectorstore",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced_v2.rag",
        "description": "projects.A4_rag_advanced_v2.rag",
        "peekOfCode": "def build_vectorstore(urls: list[str] = None):\n    print(\"📌 Iniciando indexado...\")\n    # list[Document] con todos los chunks sin procesar\n    raw_chunks = []\n    # Documentos locales desde /data/\n    raw_chunks.extend(split_documents(load_documents()))\n    # Documentos scrapeados desde URLs (si se proporcionan)\n    if urls:\n        for url in urls:\n            print(f\"Scrapeando y procesando: {url}\")",
        "detail": "projects.A4_rag_advanced_v2.rag",
        "documentation": {}
    },
    {
        "label": "retrieve_context",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced_v2.rag",
        "description": "projects.A4_rag_advanced_v2.rag",
        "peekOfCode": "def retrieve_context(question: str, n_results: int = 3):\n    # Convertir pregunta → embedding\n    query_vec = model.encode([question]).tolist()[0]\n    # Consultar la colección en ChromaDB\n    results = collection.query(\n        query_embeddings=[query_vec],\n        n_results=n_results\n    )\n    # Extraer documentos y metadatos (si no existen, usar listas vacías), distancias para posibles futuros usos\n    retrieved_docs = results.get(\"documents\", [[]])[0]",
        "detail": "projects.A4_rag_advanced_v2.rag",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.rag",
        "description": "projects.A4_rag_advanced_v2.rag",
        "peekOfCode": "model = SentenceTransformer(EMBEDDING_MODEL)\n# ==========================================================\n# Construcción del índice (siempre se reconstruye si hay nuevos documentos)\n# ==========================================================\n# Crea embeddings y guarda documentos nuevos en la colección persistente.\ndef build_vectorstore(urls: list[str] = None):\n    print(\"📌 Iniciando indexado...\")\n    # list[Document] con todos los chunks sin procesar\n    raw_chunks = []\n    # Documentos locales desde /data/",
        "detail": "projects.A4_rag_advanced_v2.rag",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "projects.A4_rag_advanced_v2.router",
        "description": "projects.A4_rag_advanced_v2.router",
        "peekOfCode": "router = APIRouter(prefix=\"/a4\", tags=[\"A4 - RAG Avanzado\"])\n# Lanzamos indexado en segundo plano al importar el router\ndef _auto_build_index():\n    try:\n        print(\"Construyendo índice RAG avanzado en background...\")\n        build_vectorstore(URLS_TO_SCRAPE)\n        print(\"Índice RAG avanzado listo.\")\n    except Exception as e:\n        print(f\"[RAG] Error durante el indexado automático: {e}\")\nthreading.Thread(target=_auto_build_index, daemon=True).start()",
        "detail": "projects.A4_rag_advanced_v2.router",
        "documentation": {}
    },
    {
        "label": "QueryRequest",
        "kind": 6,
        "importPath": "projects.A4_rag_advanced_v2.schemas",
        "description": "projects.A4_rag_advanced_v2.schemas",
        "peekOfCode": "class QueryRequest(BaseModel):\n    question: str = Field(..., example=\"Qué son las Redes Neuronales Convolucionales?\")\nclass SourceDocument(BaseModel):\n    source: str = Field(..., description=\"Ruta o nombre del documento de origen\")\n    score: float = Field(..., description=\"Similitud o relevancia del documento recuperado\")\nclass QueryResponse(BaseModel):\n    answer: str = Field(..., description=\"Respuesta generada por el modelo\")\n    sources: list[SourceDocument] = Field(..., description=\"Documentos usados para generar la respuesta\")",
        "detail": "projects.A4_rag_advanced_v2.schemas",
        "documentation": {}
    },
    {
        "label": "SourceDocument",
        "kind": 6,
        "importPath": "projects.A4_rag_advanced_v2.schemas",
        "description": "projects.A4_rag_advanced_v2.schemas",
        "peekOfCode": "class SourceDocument(BaseModel):\n    source: str = Field(..., description=\"Ruta o nombre del documento de origen\")\n    score: float = Field(..., description=\"Similitud o relevancia del documento recuperado\")\nclass QueryResponse(BaseModel):\n    answer: str = Field(..., description=\"Respuesta generada por el modelo\")\n    sources: list[SourceDocument] = Field(..., description=\"Documentos usados para generar la respuesta\")",
        "detail": "projects.A4_rag_advanced_v2.schemas",
        "documentation": {}
    },
    {
        "label": "QueryResponse",
        "kind": 6,
        "importPath": "projects.A4_rag_advanced_v2.schemas",
        "description": "projects.A4_rag_advanced_v2.schemas",
        "peekOfCode": "class QueryResponse(BaseModel):\n    answer: str = Field(..., description=\"Respuesta generada por el modelo\")\n    sources: list[SourceDocument] = Field(..., description=\"Documentos usados para generar la respuesta\")",
        "detail": "projects.A4_rag_advanced_v2.schemas",
        "documentation": {}
    },
    {
        "label": "scrape_webpage",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced_v2.scraper",
        "description": "projects.A4_rag_advanced_v2.scraper",
        "peekOfCode": "def scrape_webpage(url: str) -> list[Document]:\n    resp = requests.get(url, timeout=10)\n    resp.raise_for_status()\n    # Parseamos el contenido HTML, soup contiene el árbol DOM completo\n    soup = BeautifulSoup(resp.text, \"html.parser\")\n    # Extraemos texto visible, obtenemos los tags no deseados y los eliminamos de soup\n    # tag representa cada etiqueta no deseada encontrada y es un apuntador a la misma en el árbol DOM que contiene soup\n    # tag no es un objeto independiente, por lo que al eliminarlo de soup, se elimina del árbol DOM original\n    for tag in soup([\"script\", \"style\", \"noscript\"]):\n        tag.decompose() # Elimina etiquetas no deseadas",
        "detail": "projects.A4_rag_advanced_v2.scraper",
        "documentation": {}
    },
    {
        "label": "hash_text",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced_v2.utils",
        "description": "projects.A4_rag_advanced_v2.utils",
        "peekOfCode": "def hash_text(text: str) -> str:\n    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n# Comprueba si un chunk ya está almacenado en la colección.\ndef is_chunk_indexed(chunk_id: str) -> bool:\n    try:\n        existing = collection.get(ids=[chunk_id])\n        return bool(existing and existing[\"ids\"])\n    except Exception:\n        return False\n# Convierte metadatos y distancias en una lista de fuentes formateadas.",
        "detail": "projects.A4_rag_advanced_v2.utils",
        "documentation": {}
    },
    {
        "label": "is_chunk_indexed",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced_v2.utils",
        "description": "projects.A4_rag_advanced_v2.utils",
        "peekOfCode": "def is_chunk_indexed(chunk_id: str) -> bool:\n    try:\n        existing = collection.get(ids=[chunk_id])\n        return bool(existing and existing[\"ids\"])\n    except Exception:\n        return False\n# Convierte metadatos y distancias en una lista de fuentes formateadas.\ndef format_sources(metadatas: list, distances: list) -> list:\n    formatted = []\n    for meta, dist in zip(metadatas, distances):",
        "detail": "projects.A4_rag_advanced_v2.utils",
        "documentation": {}
    },
    {
        "label": "format_sources",
        "kind": 2,
        "importPath": "projects.A4_rag_advanced_v2.utils",
        "description": "projects.A4_rag_advanced_v2.utils",
        "peekOfCode": "def format_sources(metadatas: list, distances: list) -> list:\n    formatted = []\n    for meta, dist in zip(metadatas, distances):\n        # Convertir distancia a similitud (asumiendo distancia >= 0)\n        similarity = 1 / (1 + dist)\n        formatted.append({\n            \"source\": meta.get(\"source\", \"desconocido\"),\n            \"score\": round(similarity, 4) # Redondear a 4 decimales\n        })\n    return formatted",
        "detail": "projects.A4_rag_advanced_v2.utils",
        "documentation": {}
    },
    {
        "label": "ROOT_DIR",
        "kind": 5,
        "importPath": "config_base",
        "description": "config_base",
        "peekOfCode": "ROOT_DIR = os.path.abspath(os.path.dirname(__file__))\n# Carpeta global compartida de bases vectoriales persistentes (ChromaDB)\nCHROMA_PATH = os.path.join(ROOT_DIR, \"chroma_db\")\n# Carpeta de proyectos\nPROJECTS_PATH = os.path.join(ROOT_DIR, \"projects\")\n# Carpeta de aplicación común (FastAPI, servicios, utilidades)\nAPP_PATH = os.path.join(ROOT_DIR, \"app\")\n# === Configuración técnica compartida ===\n# Modelo de embeddings por defecto (SentenceTransformers)\nDEFAULT_EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"",
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "CHROMA_PATH",
        "kind": 5,
        "importPath": "config_base",
        "description": "config_base",
        "peekOfCode": "CHROMA_PATH = os.path.join(ROOT_DIR, \"chroma_db\")\n# Carpeta de proyectos\nPROJECTS_PATH = os.path.join(ROOT_DIR, \"projects\")\n# Carpeta de aplicación común (FastAPI, servicios, utilidades)\nAPP_PATH = os.path.join(ROOT_DIR, \"app\")\n# === Configuración técnica compartida ===\n# Modelo de embeddings por defecto (SentenceTransformers)\nDEFAULT_EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n# Modelo LLM por defecto (para OpenRouter / OpenAI compatible)\nDEFAULT_LLM_MODEL = \"meta-llama/llama-3.3-8b-instruct:free\"",
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "PROJECTS_PATH",
        "kind": 5,
        "importPath": "config_base",
        "description": "config_base",
        "peekOfCode": "PROJECTS_PATH = os.path.join(ROOT_DIR, \"projects\")\n# Carpeta de aplicación común (FastAPI, servicios, utilidades)\nAPP_PATH = os.path.join(ROOT_DIR, \"app\")\n# === Configuración técnica compartida ===\n# Modelo de embeddings por defecto (SentenceTransformers)\nDEFAULT_EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n# Modelo LLM por defecto (para OpenRouter / OpenAI compatible)\nDEFAULT_LLM_MODEL = \"meta-llama/llama-3.3-8b-instruct:free\"",
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "APP_PATH",
        "kind": 5,
        "importPath": "config_base",
        "description": "config_base",
        "peekOfCode": "APP_PATH = os.path.join(ROOT_DIR, \"app\")\n# === Configuración técnica compartida ===\n# Modelo de embeddings por defecto (SentenceTransformers)\nDEFAULT_EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n# Modelo LLM por defecto (para OpenRouter / OpenAI compatible)\nDEFAULT_LLM_MODEL = \"meta-llama/llama-3.3-8b-instruct:free\"",
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EMBEDDING_MODEL",
        "kind": 5,
        "importPath": "config_base",
        "description": "config_base",
        "peekOfCode": "DEFAULT_EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n# Modelo LLM por defecto (para OpenRouter / OpenAI compatible)\nDEFAULT_LLM_MODEL = \"meta-llama/llama-3.3-8b-instruct:free\"",
        "detail": "config_base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_LLM_MODEL",
        "kind": 5,
        "importPath": "config_base",
        "description": "config_base",
        "peekOfCode": "DEFAULT_LLM_MODEL = \"meta-llama/llama-3.3-8b-instruct:free\"",
        "detail": "config_base",
        "documentation": {}
    }
]